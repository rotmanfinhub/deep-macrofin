{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep-MacroFin","text":"<p>Deep-MacroFin is a comprehensive deep-learning framework designed to solve equilibrium economic models in continuous time. The library leverages deep learning to alleviate curse of dimensionality.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<p>The stable version of the package can be installed from PyPI.</p> <pre><code>pip install deep-macrofin\n</code></pre>"},{"location":"#build-from-source","title":"Build from Source","text":"<p>For developers, you should clone the folder to your local machine and install from the local folder.</p> <ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/rotmanfinhub/deep-macrofin.git\n</code></pre></p> </li> <li> <p>Create a virtual environment (Optional, but recommended) <pre><code>python -m venv venv\nsource venv/bin/activate # venv/Scripts/activate using Windows powershell\n</code></pre></p> </li> <li> <p>Install dependencies <pre><code>pip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-doc.txt\n</code></pre></p> </li> <li> <p>Install the package <pre><code>pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"#docs","title":"Docs","text":"<p>The documentation site is based on mkdocs and mkdocs-mateiral.</p> <p>Layouts <pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre></p> <p>To see the site locally, run the following command: <pre><code>mkdocs serve\n</code></pre></p>"},{"location":"#cite-deep-macrofin","title":"Cite Deep-MacroFin","text":"<p>If you use Deep-MacroFin for academic research, you are encouraged to cite the following paper:</p> <pre><code>@misc{wu2024deepmacrofin,\n      title={Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models}, \n      author={Yuntao Wu and Jiayuan Guo and Goutham Gopalakrishna and Zisis Poulos},\n      year={2024},\n      eprint={2408.10368},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2408.10368}, \n}\n</code></pre>"},{"location":"faq/","title":"Tips and Additional Considerations","text":"<ul> <li>Watch out for constraints - adding too many can cause issues with learning.</li> <li>Try to simplify the constraints, making them in a linear form. Reducing the number of divisions, especially divisions with derivatives, help. This is probably because of numerical stability issues.</li> <li>Parametrizing too many variables with neural networks may not always help learning.</li> <li>Using ReLU as activation might be better for functions with discontinuity in first order derivatives. </li> </ul>"},{"location":"usage/","title":"Basic Usage","text":"<p>This document shows how to configure and train a model using <code>deep_macrofin</code>.</p>"},{"location":"usage/#define-pde-system","title":"Define PDE System","text":"<p>Firstly, we import <code>PDEModel</code> (API) from the package to define the problem to be solved. The following code snippet defines a PDE model with default training settings and name <code>example</code>:</p> <pre><code>from deep_macrofin import PDEModel\npde_model = PDEModel(\"model_name\")\n</code></pre> <p><code>PDEModel</code> takes three parameters: <code>name: str, config: Dict[str, Any] = DEFAULT_CONFIG, latex_var_mapping: Dict[str, str] = {}</code>. Only <code>name</code> is required. The trained models will be saved with filename <code>name</code> by default.</p>"},{"location":"usage/#training-configs","title":"Training Configs","text":"<p>The default training configs set batch size = 100, epochs = 1000, learning rate = \\(10^{-3}\\), and AdamW optimizer. In this setting, loss will be logged to a csv file every 100 epochs during training (<code>loss_log_interval</code>), and data points are sampled randomly.  <pre><code>DEFAULT_CONFIG = {\n    \"batch_size\": 100,\n    \"num_epochs\": 1000,\n    \"lr\": 1e-3,\n    \"loss_log_interval\": 100,\n    \"optimizer_type\": OptimizerType.AdamW,\n    \"sampling_method\": SamplingMethod.UniformRandom,\n    \"refinement_sample_interval\": int(0.2 * num_epochs),\n    \"loss_balancing\": False,\n    \"bernoulli_prob\": 0.9999,\n    \"loss_balancing_temp\": 0.1,\n    \"loss_balancing_alpha\": 0.999,\n    \"soft_adapt_interval\": -1,\n    \"loss_soft_attention\": False,\n}\n</code></pre></p> <p>This dictionary can be imported: <pre><code>from deep_macrofin import DEFAULT_CONFIG\n</code></pre></p> <p>We can override any of the settings by passing a dictionary to the <code>config</code> parameter. The dictionary does not need to be complete. Any missing values are automatically replaced with default values. The following initialization will only change the number of training epoches to 10000, and keep everything else the same as default. <pre><code>from deep_macrofin import PDEModel\npde_model = PDEModel(\"model_name\", {\"num_epochs\": 10000})\n</code></pre></p>"},{"location":"usage/#optimizers","title":"Optimizers","text":"<p>OptimizerType is a <code>Enum</code> object that can be imported from the package. Currently, we support the following optimizer types: <pre><code>from deep_macrofin import OptimizerType\n# OptimizerType.Adam = \"Adam\"\n# OptimizerType.AdamW = \"AdamW\"\n# OptimizerType.LBFGS = \"LBFGS\"\n</code></pre></p> <p>Note: when KAN is used as one of the learnable variables, only LBFGS is supported.</p>"},{"location":"usage/#samplingmethod","title":"SamplingMethod","text":"<p>SamplingMethod is a <code>Enum</code> object that can be imported from the package. Currently, we support the following sampling methods: <pre><code>from deep_macrofin import SamplingMethod\n# SamplingMethod.UniformRandom = \"UniformRandom\"\n# SamplingMethod.FixedGrid = \"FixedGrid\"\n# SamplingMethod.ActiveLearning = \"ActiveLearning\"\n# SamplingMethod.RARG = \"RAR-G\"\n# SamplingMethod.RARD = \"RAR-D\"\n</code></pre></p> <p>When sampling method is <code>ActiveLearning</code>, <code>RARG</code>, or <code>RARD</code>, additional points to help learning are sampled every <code>refinement_sample_interval</code> epochs. It is default to 200 epochs, which is 20% of total epochs.</p> <p>Note: For FixedGrid sampling, the batch size is applied to each dimension, and the final sample is of shape \\((B^n, n)\\), where \\(B\\) is batch size, \\(n\\) is number of state variables. Batch size should be set to a lower value than in uniform sampling.</p> <p>Note: RAR-G and RAR-D are implemented based on Wu et al. 2022<sup>1</sup>, the underlying sampling method for additional residual points is UniformRandom, and the base sampling method for training is FixedGrid. The total number of additional residual points sampled over the entire training period is \\(B^n\\). In each addition, \\(\\frac{B^n}{\\text{refinement rounds}}\\) points are added.</p> <p>Note: ActiveLearning is not yet implemented specifically, and currently uses the logic of RAR-G.</p>"},{"location":"usage/#dynamic-loss-weighting","title":"Dynamic Loss Weighting","text":"<p>Loss Balancing implements the Relative Loss Balancing with Random Lookback (ReLoBRaLo) algorithm in Bischof and Kraus 2021<sup>2</sup>. The update follows the equations:</p> \\[\\lambda_i^{bal}(t,t') = m \\frac{\\exp\\left( \\frac{\\mathcal{L}_i(t)}{\\mathcal{T}\\mathcal{L}_i(t')}\\right)}{\\sum_{j=1}^m \\exp\\left( \\frac{\\mathcal{L}_j(t)}{\\mathcal{T}\\mathcal{L}_j(t')}\\right)}\\] \\[\\lambda_i^{hist}(t)=\\rho \\lambda_i (t-1) + (1-\\rho) \\lambda_i^{bal}(t,0)\\] \\[\\lambda_i(t) = \\alpha \\lambda_i^{hist} + (1-\\alpha) \\lambda_i^{bal}(t,t-1)\\] <p>\\(m\\) is the number of loss functions. \\(i\\in \\{1,...,m\\}\\) are indices for loss functions. \\(\\mathcal{T}\\) (<code>loss_balancing_temp</code>) is softmax temperature. \\(\\rho\\) is a Bernoulli random variable with \\(\\mathbb{E}(\\rho)\\approx 1\\) (<code>bernoulli_prob</code>). \\(\\alpha\\) is the exponential decay rate (<code>loss_balancing_alpha</code>).</p> <p>Soft Adapt implements the algorithm in Heydari et al. 2019<sup>3</sup>. It merges the loss Weighted and normalized approach. It also uses <code>loss_balancing_temp</code> parameter as the softmax temperature.</p> \\[ns_i = \\frac{s_i}{\\sum_{j=1}^m |s_j|}\\] \\[\\alpha_i = \\frac{\\exp(\\beta(ns_i - \\max(ns_i)))}{\\sum_{j=1}^m \\exp(\\beta(ns_j - \\max(ns_j)))}\\] \\[\\alpha_i = \\frac{f_i \\alpha_i}{\\sum_{j=1}^m f_j \\alpha_j}\\] <p>Soft Attention implements the algorithm in Song et al. 2024<sup>4</sup>. Specifically, loss weights are linear neural networks applied to individual grid points in the training data.</p>"},{"location":"usage/#latex-variable-map","title":"Latex Variable Map","text":"<p>Economic models may involve a large amount of variables and equations. Each variable can have super-/subscripts. To properly distinguish super-/subscripts from powers/derivatives and parse equations when LaTex formula are provided, we require a mapping from LaTex variables to Python strings. The keys are LaTex strings in raw format <code>r\"\"</code>, and the values are the corresponding python string. The following dictionary maps LaTex string \\(\\xi_t^h\\) to Python string <code>\"xih\"</code>, and LaTex string \\(q_t^a\\) to Python string <code>\"qa\"</code>.</p> <pre><code>latex_var_mapping = {\n    r\"\\xi_t^h\": \"xih\",\n    r\"q_t^a\": \"qa\"\n} # define mapping\n\nfrom deep_macrofin import PDEModel\npde_model = PDEModel(\"model_name\", latex_var_mapping=latex_var_mapping) # enforce the mapping for formula parsing in the model\n</code></pre>"},{"location":"usage/#define-problem-domain","title":"Define Problem Domain","text":"<p>The problem dimension and domain are defined using state variables through <code>PDEModel.set_state</code> method (API).</p> <p>The following defines a 1D problem, with state variable \\(x\\), domain \\(x\\in [0.01,0.99]\\).  <pre><code>pde_model.set_state([\"x\"], {\"x\": [0.01, 0.99]})\n</code></pre></p> <p>The following defines a 2D problem, with state variable \\((x,y)\\), domain \\((x,y)\\in [-1,1]\\times[-1,1]\\) (default domain).  <pre><code>pde_model.set_state([\"x\", \"y\"])\n</code></pre></p> <p>The following defines a 2D problem, with state variable \\((x,y)\\), domain \\((x,y)\\in [0,1]\\times[0,\\pi]\\).  <pre><code>pde_model.set_state([\"x\", \"y\"], {\"x\": [0,1], \"y\": [0, np.pi]}) # torch.pi is also acceptable (They are the same constant)\n</code></pre></p>"},{"location":"usage/#define-agentendogvar","title":"Define Agent/EndogVar","text":"<p>Agents and EndogVar (LearnableVar) can only be added to a PDEModel after the states are defined, so that the functions to compute derivatives can be constructed properly.</p> <p>To add a single agent or endogenous variable to a model with default settings, we can use <code>add_agent</code> (API) and <code>add_endog</code> (API): <pre><code>pde_model.add_agent(\"xih\") # this adds an agent with name: xih\npde_model.add_endog(\"qa\") # this adds an agent with name: qa\n</code></pre> If the latex_variable_mapping is properly setup in the previous step, <code>xih</code> is associated with \\(\\xi_t^h\\) and <code>qa</code> is associated with \\(q_t^a\\) in formula parsing and evaluations.</p> <p>Suppose there are two state variables \\(x,y\\), then the derivatives <code>xih_x</code> (\\(\\frac{\\partial \\xi_t^h}{\\partial x}\\)), <code>xih_y</code> (\\(\\frac{\\partial \\xi_t^h}{\\partial y}\\)), <code>xih_xx</code> (\\(\\frac{\\partial^2 \\xi_t^h}{\\partial x^2}\\)), <code>xih_yy</code> (\\(\\frac{\\partial^2 \\xi_t^h}{\\partial y^2}\\)), <code>xih_xy</code> (\\(\\frac{\\partial^2 \\xi_t^h}{\\partial y\\partial x}\\)), and <code>xih_yx</code> (\\(\\frac{\\partial^2 \\xi_t^h}{\\partial x\\partial y}\\)) are computed for \\(\\xi_t^h\\). Similar for \\(q_t^a\\).</p>"},{"location":"usage/#model-configs","title":"Model Configs","text":"<p>The default learnable variable configs sets training device to <code>\"cuda\"</code> if it is available. Otherwise the models will be trained on CPU. However, we recommend using CPU for training, because the models are not usually computationally expensive, while the synchronization (evaluating string formula on CPU and transferring computation to GPU) can significantly bottleneck the computation speed of GPUs.  The models are default to 4-layer MLPs. Each layer contains 30 hidden units, with \\(\\tanh(x)\\) activation.  By default, <code>positive=False</code>, each model can output any values \\((-\\infty,\\infty)\\). Derivatives will be taken up to second order (<code>derivative_order=2</code>). <pre><code>DEFAULT_LEARNABLE_VAR_CONFIG = {\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"hidden_units\": [30, 30, 30, 30],\n    \"layer_type\": LayerType.MLP,\n    \"activation_type\": ActivationType.Tanh,\n    \"positive\": False,\n    \"derivative_order\": 2,\n}\n</code></pre></p> <p>This dictionary can be imported: <pre><code>from deep_macrofin import DEFAULT_LEARNABLE_VAR_CONFIG\n</code></pre></p> <p>LayerType and ActivationType are <code>Enum</code> objects that can be imported from the package. Currently, we support the following layers and activation types: <pre><code>from deep_macrofin import ActivationType, LayerType\n# ActivationType.ReLU = \"relu\" (nn.ReLU)\n# ActivationType.SiLU = \"silu\" (nn.SiLU)\n# ActivationType.Sigmoid = \"sigmoid\" (nn.Sigmoid)\n# ActivationType.Tanh = \"tanh\" (nn.Tanh)\n# ActivationType.Wavelet=\"wavelet\" (w1*sin(x)+w2*cos(x), where w1 and w2 are learnable)\n\n# LayerType.MLP = \"MLP\"\n# LayerType.KAN = \"KAN\"\n# LayerType.MultKAN=\"MultKAN\"\n</code></pre></p> <p>Similar to PDEModel defintion, we can override any of the settings by passing a dictionary to the <code>config</code> parameter.</p> <p>In the following setup, <code>xih</code> is defined to be a 2-layer MLP. Each layer contains 40 hidden units and is activated by SiLU. Its output is restricted to be positive only by SoftPlus. Suppose the state variable is \\(x\\), then four derivatives are computed: <code>xih_x</code>, <code>xih_xx</code>, <code>xih_xxx</code>, <code>xih_xxxx</code>.  <code>qa</code> is defined as a 2-layer KAN. The input and output sizes are restricted to 1, and there are two layers of width 5. <pre><code>pde_model.add_agent(\"xih\", \n    config={\n        \"hidden_units\": [40, 40], \n        \"activation_type\": ActivationType.SiLU, \n        \"positive\": True, \n        \"derivative_order\": 4}\n) \npde_model.add_endog(\"qa\", \n    config={\n        \"hidden_units\": [1, 5, 5, 1], \n        \"layer_type\": LayerType.KAN}\n)\n</code></pre></p>"},{"location":"usage/#defining-multiple-agentsendogvars","title":"Defining Multiple Agents/EndogVars","text":"<p>Multiple agents and endogenous variables with different configurations can be added to PDEModel at the same time using <code>add_agents</code> (API) and <code>add_endogs</code> (API). The following code adds two agents <code>\"xii\", \"xih\"</code>, and five endogenous variables <code>\"mue\", \"qa\", \"wia\", \"wha\", \"sigea\"</code> to a PDEModel. <pre><code>pde_model.add_agents([\"xii\", \"xih\"], \n    {\n        \"xii\": {\"positive\": True}, \n        \"xih\": {\"positive\": True}\n    }\n)\npde_model.add_endogs([\"mue\", \"qa\", \"wia\", \"wha\", \"sigea\"], \n    {\"qa\": {\"positive\": True}})\n</code></pre></p> <p>The parameter <code>configs</code> is now a multi-level dictionary, <code>{var1_name: {var1_configs}, var2_name: {var2_configs}, ...}</code>. Any configurations not provided by the user are set as default. </p>"},{"location":"usage/#conditions-for-agentendogvar","title":"Conditions for Agent/EndogVar","text":"<p><code>add_agent_condition</code> (API) and <code>add_endog_condition</code> (API) can be used for adding conditions (initial and boundary conditions, and any point that requires specific values)</p> <p>The following code adds a condition on a 1D endogenous variable \\(y\\): \\(y(0)=1\\). The key <code>SV</code> can be replaced by any variable that is not used in the system. For example, if <code>lhs</code> is defined as <code>\"y(X)\"</code>, then <code>lhs_state</code> should be replaced with <code>{\"X\": torch.zeros((1, 1))}</code>. Because rhs is a constant 1, there is no need for any <code>rhs_state</code> definition. An empty dictionary suffices. <code>weight=2</code> means the computed loss will have weight 2 in the PDEModel. <pre><code>pde_model.add_endog_condition(\"y\", \n    \"y(SV)\", {\"SV\": torch.zeros((1, 1))}, \n    Comparator.EQ, \n    \"1\", {}, \n    weight=2)\n</code></pre></p> <p><code>Comparator</code> is a <code>Enum</code> object that can be imported from the package. <pre><code>from deep_macrofin import Comparator\n# Comparator.LEQ = \"&lt;=\"\n# Comparator.GEQ = \"&gt;=\"\n# Comparator.LT = \"&lt;\"\n# Comparator.GT = \"&gt;\"\n# Comparator.EQ = \"=\"\n</code></pre></p> <p>The following code adds a condition on a 2D endogenous variable \\(z\\) s.t. \\(z(0, y)\\leq y\\) \\(\\forall y\\in[0,1]\\). Batch size is set to 100 here, but can be finetuned to get better accuracy. Each element in the batch requires two state variables. The first variable is \\(x=0\\), and the second variable \\(y\\) is set to be a linear space from 0 to 1. RHS is replaced with <code>zero_x[:, 1:2]</code> as \\(y\\). <pre><code>zero_x = torch.zeros((100, 2)) \nzero_x[:, 1] = torch.tensor(np.linspace(0, 1, num=100))\npde_model.add_endog_condition(\"y\", \n    \"y(SV)\", {\"SV\": zero_x}, \n    Comparator.LEQ, \n    \"rhs\", {\"rhs\": zero_x[:, 1:2]}\n)\n</code></pre></p>"},{"location":"usage/#add-equations-etc","title":"Add Equations, etc","text":"<p>Note:  - When an equation is provided using LaTex, it must be enclosed by <code>$</code> and defined as a raw string <code>r\"\"</code> - Derivatives should be provided as <code>\"y_x\"</code> or <code>\"y_xx\"</code> in raw python strings, or <code>\"\\frac{\\partial y}{\\partial x}\"</code> or <code>\"\\frac{\\partial^2 y}{\\partial x^2}\"</code> in LaTex strings.</p> <p><code>add_param</code> (API) and <code>add_params</code> (API) are used to define constant parameters. <code>add_equation</code> (API) defines new variables through an equation.  </p> <p>The following adds two parameters <code>zetai=1.0005</code> (\\(\\zeta^i\\)) and <code>gammai=2</code> (\\(\\gamma^i\\)) to the system, and defines a new variable \\(y=\\zeta^i + \\gamma^i\\) in the system.  <pre><code>from deep_macrofin import PDEModel\nlatex_var_mapping = {\n    r\"\\zeta^i\": \"zetai\",\n    r\"\\gamma^i\": \"gammai\"\n} # define mapping\npde_model = PDEModel(\"model_name\", latex_var_mapping=latex_var_mapping)\npde_model.add_params({\n    \"zetai\": 1.0005,\n    \"gammai\": 2,\n})\n# The following two equations are equivalent.\n# DO NOT copy and run the following code, because double definition could cause errors in the model\npde_model.add_equation(r\"$y=\\zeta^i + \\gamma^i$\") # latex version\npde_model.add_equation(\"y=zetai+gammai\") # pure python version\n</code></pre></p> <p><code>add_endog_equation</code> (API) defines endogenous equations (algebraic equations) that must be satisfied by the system. <code>add_hjb_equation</code> (API) defines HJB equation to maximize/minimize for each agent. <code>add_system</code> (API) defines constraint-activated systems to handle different components of a system.</p> <p>Endogenous equations, constraints, HJB equations and systems are used to compute losses.</p> <p>The following code defines an endogenous equation</p> \\[y'' + 6y' + 5*y = 0\\] <pre><code>pde_model.add_endog_equation(r\"$\\frac{\\partial^2 y}{\\partial x^2} + 6 * \\frac{\\partial y}{\\partial x} + 5 * y =0$\") # latex version\npde_model.add_endog_equation(\"y_xx+6*y_x+5*y=0\") # pure python version\n</code></pre> <p>The following code defines an HJB equation to maximize</p> \\[\\frac{\\rho^i}{1-\\frac{1}{\\zeta^i}} * \\left( \\left(\\frac{c_t^i}{\\xi_t^i} \\right)^{1-1/\\zeta^i}-1 \\right) + \\mu_t^{\\xi i} +  \\mu_t^{ni} - \\frac{\\gamma^i}{2} * (\\sigma_t^{nia})^2  - \\frac{\\gamma^i}{2} * (\\sigma_t^{\\xi ia})^2\\] <pre><code># Note: proper parsing of variables are required. See 1D Economic Problem for full example.\npde_model.add_hjb_equation(r\"$\\frac{\\rho^i}{1-\\frac{1}{\\zeta^i}} * \\left( \\left(\\frac{c_t^i}{\\xi_t^i} \\right)^{1-1/\\zeta^i}-1 \\right) + \\mu_t^{\\xi i} +  \\mu_t^{ni} - \\frac{\\gamma^i}{2} * (\\sigma_t^{nia})^2  - \\frac{\\gamma^i}{2} * (\\sigma_t^{\\xi ia})^2$\")\n</code></pre> <p>The systems (API) are defined using activation constraints.  The following code defines a system that is activated when \\(\\frac{\\partial y}{\\partial x} \\geq 0\\) and \\(x \\in [0,0.5]\\), and computes \\(z=x^2y\\), requiring \\(y+z = 1\\) in this case.</p> <pre><code>from deep_macrofin import System, Constraint\n# because the systems are separated from the main PDEModel when being defined, labels are required when initializing the constraints and the system.\ns = System([\n    Constraint(\"y_x\", Comparator.GEQ, \"0\"), # y_x&gt;=0\n    Constraint(\"x\", Comparator.GEQ, \"0\"), # x&gt;= 0\n    Constraint(\"x\", Comparator.LEQ, \"0.5\"), # x&lt;=0.5\n    ], \"sys1\")\ns.add_equation(\"z=x**2 * y\") \ns.add_endog_equation(\"y+z = 1\")\npde_model.add_system(s) # add the system to PDEModel\n</code></pre>"},{"location":"usage/#about-labels","title":"About Labels","text":"<p>For all equations, conditions, etc added to the model, there is an optional parameter <code>label</code> used to identify the equations. This can be automatically set based on the sequence the equation is added to the system, and usually doesn't need to be set by the user.  <pre><code>pde_model.add_equation(\"y=x+1\") # eq_1 is y=x+1\npde_model.add_equation(\"z=y*2\") # eq_2 is z=y*2\n</code></pre></p>"},{"location":"usage/#defining-derivatives-with-equations","title":"Defining Derivatives with Equations","text":"<p>By default, only parametrized functions have derivatives w.r.t. state variables as shown in Define Agent/EndogVar. However, we can also define implicit derivatives using equations and a function macro <code>deriv(y, x)</code>. For derivatives w.r.t. state variables, <code>x</code> needs to be set to <code>SV</code> explicitly, and array indexing is required in equation strings. </p> <pre><code>pde_model.set_state([\"x\", \"y\"])\npde_model.add_endog(\"k\")\npde_model.add_equation(\"q=k/x\")\npde_model.add_equation(\"p=k*y\")\npde_model.add_equation(\"q_k=deriv(q, k)\") # this computes the derivative dq/dk\npde_model.add_equation(\"q_x=deriv(q, SV)[:,:1]\") # this computes the derivative dq/dx, which is q w.r.t. the first state variable x\npde_model.add_equation(\"q_xy=deriv(q_x, SV)[:,1:2]\") # this computes the derivative d^2q/(dxdy)\npde_model.add_equation(\"p_y=deriv(p, SV)[:,1:2]\") # this computes the derivative dp/dy, which is p w.r.t. the second state variable y\npde_model.add_equation(\"p_xy=deriv(p_y, SV)[:,:1]\") # this computes the derivative d^2p/(dxdy)\n</code></pre>"},{"location":"usage/#training-and-evaluation","title":"Training and Evaluation","text":"<p>After the model is defined, <code>train_model</code> (API) can be used to train the models, and <code>eval_model</code> (API) can be used to eval models.  <code>load_model</code> (API) can be used to load a trained model. It will overwrite existing agent/endogenous variables in the PDEModel.</p> <p><code>train_model</code> will print out the full model configurations and save the best and final models. Losses are logged every <code>loss_log_interval</code> epochs during training in <code>modelname_loss.csv</code> file for plotting. Minimum (converging) losses are logged in a separate <code>modelname_min_loss.csv</code> file. Examples can be found in Basic Examples.</p>"},{"location":"usage/#print-the-model","title":"Print the Model","text":"<p>For easier debugging on the model setup and equation typing, <code>print(pde_model)</code> prints out a detailed configuration of the model. The following is a sample print out of log utility problem. The same summary is logged in the log file for each model. </p> <pre><code>=====================Summary of Model BruSan14_log_utility======================\nConfig: {\n \"batch_size\": 100,\n \"num_epochs\": 100,\n \"lr\": 0.001,\n \"loss_log_interval\": 10,\n \"optimizer_type\": \"Adam\"\n}\nLatex Variable Mapping:\n{\n \"\\\\sigma_t^q\": \"sigq\",\n \"\\\\sigma_t^\\\\theta\": \"sigtheta\",\n \"\\\\sigma_t^\\\\eta\": \"sige\",\n \"\\\\mu_t^\\\\eta\": \"mue\",\n \"\\\\mu_t^q\": \"muq\",\n \"\\\\mu_t^\\\\theta\": \"mutheta\",\n \"\\\\rho\": \"rho\",\n \"\\\\underline{a}\": \"ah\",\n \"\\\\underline{\\\\delta}\": \"deltah\",\n \"\\\\delta\": \"deltae\",\n \"\\\\sigma\": \"sig\",\n \"\\\\kappa\": \"kappa\",\n \"\\\\eta\": \"e\",\n \"\\\\theta\": \"theta\",\n \"\\\\psi\": \"psi\",\n \"\\\\iota\": \"iota\",\n \"\\\\Phi\": \"phi\"\n}\nUser Defined Parameters:\n{\n \"sig\": 0.1,\n \"deltae\": 0.05,\n \"deltah\": 0.05,\n \"rho\": 0.06,\n \"r\": 0.05,\n \"a\": 0.11,\n \"ah\": 0.07,\n \"kappa\": 2\n}\n\n================================State Variables=================================\ne: [0.0, 1.0]\n\n=====================================Agents=====================================\n\n================================Agent Conditions================================\n\n==============================Endogenous Variables==============================\nEndogenous Variable Name: q\nEndogVar(\n  (model): Sequential(\n    (linear_0): Linear(in_features=1, out_features=30, bias=True)\n    (activation_0): Tanh()\n    (linear_1): Linear(in_features=30, out_features=30, bias=True)\n    (activation_1): Tanh()\n    (linear_2): Linear(in_features=30, out_features=30, bias=True)\n    (activation_2): Tanh()\n    (linear_3): Linear(in_features=30, out_features=30, bias=True)\n    (activation_3): Tanh()\n    (final_layer): Linear(in_features=30, out_features=1, bias=True)\n    (positive_act): Softplus(beta=1.0, threshold=20.0)\n  )\n)\nNum parameters: 2881\n--------------------------------------------------------------------------------\nEndogenous Variable Name: psi\nEndogVar(\n  (model): Sequential(\n    (linear_0): Linear(in_features=1, out_features=30, bias=True)\n    (activation_0): Tanh()\n    (linear_1): Linear(in_features=30, out_features=30, bias=True)\n    (activation_1): Tanh()\n    (linear_2): Linear(in_features=30, out_features=30, bias=True)\n    (activation_2): Tanh()\n    (linear_3): Linear(in_features=30, out_features=30, bias=True)\n    (activation_3): Tanh()\n    (final_layer): Linear(in_features=30, out_features=1, bias=True)\n    (positive_act): Softplus(beta=1.0, threshold=20.0)\n  )\n)\nNum parameters: 2881\n--------------------------------------------------------------------------------\n\n========================Endogenous Variables Conditions=========================\nendogvar_q_cond_q_min: q(SV)=(2*ah*kappa + (kappa*r)**2 + 1)**0.5 - kappa*r with LHS evaluated at SV=[[0.0]] and RHS evaluated at sig=0.1deltae=0.05deltah=0.05rho=0.06r=0.05a=0.11ah=0.07kappa=2\nLoss weight: 1.0\n--------------------------------------------------------------------------------\n\n===================================Equations====================================\neq_1: \nRaw input: $\\iota = \\frac{q^2-1}{ 2 * \\kappa}$\nParsed: iota=(q**(2)-1)/( 2 * kappa)\neq_2: \nRaw input: $\\sigma_t^q = \\frac{\\sigma}{1 - \\frac{1}{q} * \\frac{\\partial q}{\\partial \\eta} * (\\psi - \\eta)} - \\sigma$\nParsed: sigq=(sig)/(1 - (1)/(q) * q_e * (psi - e)) - sig\neq_3: \nRaw input: $\\sigma_t^\\eta = \\frac{\\psi - \\eta}{\\eta} * (\\sigma + \\sigma_t^q)$\nParsed: sige=(psi - e)/(e) * (sig + sigq)\neq_4: \nRaw input: $\\mu_t^\\eta = (\\sigma_t^\\eta)^2 + \\frac{a - \\iota}{q} + (1-\\psi) * (\\underline{\\delta} - \\delta) - \\rho$\nParsed: mue=(sige)**(2) + (a - iota)/(q) + (1-psi) * (deltah - deltae) - rho\n\n==============================Endogenous Equations==============================\nendogeq_1: \nRaw input: $(\\sigma + \\sigma_t^q) ^2 * (\\psi / \\eta - (1-\\psi) / (1-\\eta)) = \\frac{a - \\underline{a}}{q} + \\underline{\\delta} - \\delta$\nParsed: (sig + sigq) **(2) * (psi / e - (1-psi) / (1-e))=(a - ah)/(q) + deltah - deltae\nLoss weight: 1.0\n--------------------------------------------------------------------------------\n\n==================================Constraints===================================\nconstraint_1: psi&lt;=1\nLoss weight: 1.0\n--------------------------------------------------------------------------------\n\n=================================HJB Equations==================================\n\n====================================Systems=====================================\nnon-opt: \nActivation Constraints:\nnon-opt: psi&lt;1\n===============Equations================\n\n==========Endogenous Equations==========\nsystem_non-opt_endogeq_1: \nRaw input: $(r*(1-\\eta) + \\rho * \\eta) * q = \\psi * a + (1-\\psi) * \\underline{a} - \\iota$\nParsed: (r*(1-e) + rho * e) * q=psi * a + (1-psi) * ah - iota\nLoss weight: 1.0\n----------------------------------------\n\nSystem loss weight: 1.0\n--------------------------------------------------------------------------------\nopt: \nActivation Constraints:\nopt: psi&gt;=1\n===============Equations================\n\n==========Endogenous Equations==========\nsystem_opt_endogeq_1: \nRaw input: $(r*(1-\\eta) + \\rho * \\eta) * q = a - \\iota$\nParsed: (r*(1-e) + rho * e) * q=a - iota\nLoss weight: 1.0\n----------------------------------------\n\nSystem loss weight: 1.0\n--------------------------------------------------------------------------------\n</code></pre> <p>When errors are found in equation validation before training begins, the system will save an error log.</p>"},{"location":"usage/#plot","title":"Plot","text":"<p>Once the models are trained, there are several values that can be plotted. However, plottings are only supported for 1D and 2D models.</p>"},{"location":"usage/#plot-learnable-variables","title":"Plot Learnable Variables","text":"<p>Each learnable variable has a plot function (API) to plot the function itself and associated derivatives.</p> <p>The following code plots only the original function \\(y\\) in an individual matplotlib figure on the domain \\([0, 1]\\). <pre><code>pde_model.agents[\"y\"].plot(\"y\", domain=[0, 1], ax=None)\n</code></pre></p> <p>The following code plots \\(y, y', y''\\) in one row three column: <pre><code>fig, ax = plt.subplots(1, 3, figsize=(16, 5))\npde_model.endog_vars[\"y\"].plot(\"y\", {\"x\": [0, 1]}, ax=ax[0])\npde_model.endog_vars[\"y\"].plot(\"y_x\", {\"x\": [0, 1]}, ax=ax[1])\npde_model.endog_vars[\"y\"].plot(\"y_xx\", {\"x\": [0, 1]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> <p>The following code plots \\(u(x,y)\\) in 2D: <pre><code>fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={\"projection\": \"3d\"})\npde_model.endog_vars[\"u\"].plot(\"u\", {\"x\": [0, 1.], \"y\": [0, 1.]}, ax=ax)\nplt.subplots_adjust()\nplt.show()\n</code></pre></p>"},{"location":"usage/#plot-variables-defined-by-equations","title":"Plot Variables Defined by Equations","text":"<p>PDEModel has a <code>plot_vars</code> function (API), which can plot any variables that are already defined in a <code>PDEModel</code> or additional variables of interests using new equations.</p> <p>The following code will plot \\(q_t^a, \\sigma_t^{qa}, w_t^{ia}, w_t^{ha}\\) and risk premium (rp) in a four-column format. The risk premium is a new variable defined by \\(r_t^{ka} - r_t\\). <pre><code>pde_model.plot_vars([\n    r\"$q_t^a$\", \n    r\"$\\sigma_t^{qa}$\", \n    r\"$w_t^{ia}$\", \n    r\"$w_t^{ha}$\",\n    r\"$rp = r_t^{ka} - r_t $\"])\n</code></pre></p> <p>The following code will plot \\(q_t^a, \\sigma_t^{qa}, w_t^{ia}, w_t^{ha}\\) in a 2x2 grid. <pre><code>pde_model.plot_vars([\n    r\"$q_t^a$\", \n    r\"$\\sigma_t^{qa}$\", \n    r\"$w_t^{ia}$\", \n    r\"$w_t^{ha}$\"], ncols=2)\n</code></pre></p>"},{"location":"usage/#plot-loss","title":"Plot Loss","text":"<p><code>plot_loss_df</code> (API) plots losses in the logged csv file.</p> <p>A general usage to plot all losses in the file <code>./models/1d_prob/1d_prob_loss.csv</code>: <pre><code>from deep_macrofin import plot_loss_df\nplot_loss_df(fn=\"./models/1d_prob/1d_prob_loss.csv\", loss_plot_fn=\"./models/1d_prob/1d_prob_loss.png\")\n</code></pre></p> <ol> <li> <p>Chenxi Wu, and Min Zhu, and Qinyang Tan, and Yadhu Kartha, and Lu Lu, \"A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks\", 2022-07-21, arXiv:2207.10289 \u21a9</p> </li> <li> <p>Rafael Bischof, and Michael Kraus, \"Multi-Objective Loss Balancing for Physics-Informed Deep Learning\", 2021-10-19, arXiv:2110.09813 \u21a9</p> </li> <li> <p>A. Ali Heydari, and Craig A. Thompson, and Asif Mehmood, \"SoftAdapt: Techniques for Adaptive Loss Weighting of Neural Networks with Multi-Part Loss Functions\", 2019-12-27, arXiv:1912.12355 \u21a9</p> </li> <li> <p>Yanjie Song, and He Wang, and He Yang, and Maria Luisa Taccari, and Xiaohui Chen, \"Loss-attentional physics-informed neural networks\", Journal of Computational Physics, Volume 501, 2024, link \u21a9</p> </li> </ol>"},{"location":"usage_timestep/","title":"Time Stepping Scheme","text":"<p>The time stepping scheme implementation in <code>PDEModelTimeStep</code> is mostly identical to the basic <code>PDEModel</code>. Here, we only highlight the differences.</p>"},{"location":"usage_timestep/#define-pde-system","title":"Define PDE System","text":"<p>Firstly, we import <code>PDEModelTimeStep</code> (API) from the package to define the problem to be solved. The following code snippet defines a PDE model with default training settings and name <code>example</code>:</p> <pre><code>from deep_macrofin import PDEModelTimeStep\npde_model = PDEModelTimeStep(\"model_name\")\n</code></pre> <p><code>PDEModelTimeStep</code> takes three parameters: <code>name: str, config: Dict[str, Any] = DEFAULT_CONFIG_TIME_STEP, latex_var_mapping: Dict[str, str] = {}</code>. Only <code>name</code> is required. The trained models will be saved with filename <code>name</code> by default.</p>"},{"location":"usage_timestep/#training-configs","title":"Training Configs","text":"<p>The default training configs set batch size = 100, learning rate = \\(10^{-3}\\), and Adam optimizer. In this setting, loss will be logged to a csv file every 100 epochs during training (<code>loss_log_interval</code>), and data points are sampled from a fixed grid.  <pre><code>DEFAULT_CONFIG_TIME_STEP = {\n    \"batch_size\": 100,\n    \"num_outer_iterations\": 100,\n    \"num_inner_iterations\": 5000,\n    \"lr\": 1e-3,\n    \"loss_log_interval\": 100,\n    \"optimizer_type\": OptimizerType.Adam,\n    \"min_t\": 0.0,\n    \"max_t\": 1.0,\n    \"outer_loop_convergence_thres\": 1e-4,\n    \"sampling_method\": SamplingMethod.FixedGrid,\n    \"time_batch_size\": None,\n}\n</code></pre></p> <p>This dictionary can be imported: <pre><code>from deep_macrofin import DEFAULT_CONFIG_TIME_STEP\n</code></pre></p>"},{"location":"usage_timestep/#set-initial-guess","title":"Set Initial Guess","text":"<pre><code># This sets an initial guess of 7 uniformly on the domain for variable p.\npde_model.set_initial_guess({\"p\": 7.0})\n</code></pre> <p>Set the initial guess (uniform value across the state variable domain) for agents or endogenous variables. This is the boundary condition at \\(t=T\\) in the first time iteration.</p> <p>Note that in most cases, the initial guess has no actual effects on the training. The default value is 1 for any neural network models.</p>"},{"location":"usage_timestep/#differences-from-pdemodel","title":"Differences from PDEModel","text":"<ul> <li> <p>Only Fixed Grid Sampling and Uniform Random Sampling are supported in time stepping scheme.</p> </li> <li> <p>The name <code>t</code> is reserved for implicit time dimension (a state variable). Therefore, it cannot be used as user defined variables. It should not be passed into the state variable list by the user either. The only thing the user can change for <code>t</code> is <code>min_t</code> and <code>max_t</code>, defining the size of the finite time interval to simulate infinite horizon.</p> </li> <li> <p>With the additional <code>t</code>, the actual problem dimension is \\(N+1\\), where \\(N\\) is the number of state variables defined by the user. The final sample is of shape \\((B^{N+1}, N+1)\\) for fixed grid sampling, or \\((B*B_t, N+1)\\) for uniform sampling, where \\(B_t\\) is the <code>time_batch_size</code> (default to \\(B\\)).</p> </li> <li> <p>Currently, no loss weight adjustment algorithm is implemented for time stepping scheme.</p> </li> <li> <p>During training, three models are saved</p> </li> <li><code>{file_prefix}_temp_best.pt</code>: the best model within current outer loop</li> <li><code>{file_prefix}_best.pt</code>: the best model over all the past outer loops</li> <li><code>{file_prefix}.pt</code>: the final model in the final outer loop.</li> </ul>"},{"location":"api/evaluations/","title":"deep_macrofin.evaluations","text":""},{"location":"api/evaluations/#formula","title":"Formula","text":"<pre><code>class Formula(formula_str: str, evaluation_method: Union[EvaluationMethod, str], latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Base class for string evaluations. Given a string representation of a formula, and a set of variables (state, value, prices, etc) in a model, parse the formula to a pytorch function that can be evaluated. </p> <p>Latex equation with restricted format is supported for initialization. When Latex is provided, it is first parsed into Python-evaluable strings with Latex-variable mappings provided by the user. For instance, to input the Cauchy-Euler equation: \\(x^2 y'' + 6xy' + 4y =0\\), the user can either type in the raw Python string <code>x**2*y_xx + 6*x*y_x + 4*y = 0</code> or the Latex version <code>$x^2 * \\frac{\\partial^2 y}{\\partial x^2} + 6*x*\\frac{\\partial y}{\\partial x} + 4*y = 0$</code>. The Latex version will be internally parsed to the raw Python string version for evaluation.</p> <p>Parameters:</p> <ul> <li>formula_str: str, the string version of the formula. If the provided formula_str is supposed to be a latex string, it must be $ enclosed and in the regular form, e.g. <code>formula_str=r\"$x^2*y$\"</code>, and all multiplication symbols must be explicitly provided as * in the equation.</li> <li>evaluation_method: Union[EvaluationMethod, str], Enum, select from <code>eval</code>, <code>sympy</code>, <code>ast</code>, corresponding to the four methods below. For now, only eval is supported.</li> <li> <p>latex_var_mapping: Dict[str, str], only used if the formula_str is in latex form, the keys should be the latex expression, and the values should be the corresponding python variable name. All strings with single slash in latex must be defined as a raw string. All spaces in the key must match exactly as in the input formula_str. </p> <p>Example: <pre><code>latex_var_map = {\n    r\"\\eta_t\": \"eta\",\n    r\"\\rho^i\": \"rhoi\",\n    r\"\\mu_t^{n h}\": \"munh\",\n    r\"\\sigma_t^{na}\": \"signa\",\n    r\"\\sigma_t^{n ia}\": \"signia\",\n    r\"\\sigma_t^{qa}\": \"sigqa\",\n    \"c_t^i\": \"ci\",\n    \"c_t^h\": \"ch\",\n}\n</code></pre></p> </li> </ul> <p>Evaluation Methods: <pre><code>class EvaluationMethod(str, Enum):\n    Eval = \"eval\"\n    Sympy = \"sympy\"\n    AST = \"ast\"\n</code></pre></p>"},{"location":"api/evaluations/#eval","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Evaluate the formula with existing functions and provided assignments to variables This evaluates the function by simple string parsing.</p> <p>Parameters:</p> <ul> <li>available_functions: Dict[str, Callable], function calls attached to a string variable. It can include all <code>LearnableVar</code> and derivatives.</li> <li>variables: Dict[str, torch.Tensor], values assigned to each variable.</li> </ul>"},{"location":"api/evaluations/#comparator","title":"Comparator","text":"<p>A Enum class representing comparitor symbols, used in Conditions and Constraint. </p> <pre><code>class Comparator(str, Enum):\n    LEQ = \"&lt;=\"\n    GEQ = \"&gt;=\"\n    LT = \"&lt;\"\n    GT = \"&gt;\"\n    EQ = \"=\"\n</code></pre>"},{"location":"api/evaluations/#baseconditions","title":"BaseConditions","text":"<pre><code>class BaseConditions(lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Define specific boundary/initial conditions for a specific agent. e.g. x(0)=0 or x(0)=x(1) (Periodic conditions). May also be an inequality, but it is very rare.</p> <p>The difference between a constraint and a condition is:</p> <ul> <li>a constraint must be satisfied at any state</li> <li>a condition is satisfied at a specific given state</li> </ul> <p>Parameters:</p> <ul> <li>lhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), endog_name(SV), or simply a constant value</li> <li>lhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate lhs at for the agent/endogenous variable</li> <li>comparator: Comparator</li> <li>rhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), endog_name(SV), or simply a constant value</li> <li>rhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate rhs at for the agent/endogenous variable, if rhs is a constant, this can be an empty dictionary</li> <li>label: str, label for the condition</li> <li>latex_var_mapping: Not implemented. only used if the formula_str is in latex form, the keys should be the latex expression, and the values should be the corresponding python variable name. </li> </ul> <p>Example: <pre><code>BaseConditions(lhs=\"f(SV)\", lhs_state={\"SV\": torch.zeros((1,1))}, \n                comparator\"=\", \n                rhs\"1\", rhs_state={}, \n                label=\"eg1\")\n'''\nThe condition is f(0)=1\n'''\n\n\nBaseConditions(lhs=\"f(SV)\", lhs_state={\"SV\": torch.zeros((1,1))}, \n                comparator\"=\", \n                rhs\"f(SV)\", rhs_state={\"SV\": torch.ones((1,1))}, \n                label=\"eg2\")\n'''\nThe condition is f(0)=f(1)\n'''\n</code></pre></p>"},{"location":"api/evaluations/#eval_1","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], \n                loss_reduction: LossReductionMethod=LossReductionMethod.MSE):\n</code></pre> <p>Computes the loss based on the required conditions:</p> \\[\\mathcal{L}_{cond} = \\frac{1}{|U|} \\sum_{x\\in U}\\|\\mathcal{C}(v_i, x)\\|_2^2\\] <p>Details for evaluating non-equality conditions are the same as Constraint</p>"},{"location":"api/evaluations/#agentconditions","title":"AgentConditions","text":"<pre><code>class AgentConditions(agent_name: str, \n                    lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Subclass of <code>BaseConditions</code>. Defines conditions on agent with name <code>agent_name</code>.</p>"},{"location":"api/evaluations/#endogvarconditions","title":"EndogVarConditions","text":"<pre><code>class EndogVarConditions(endog_name: str, \n                        lhs: str, lhs_state: Dict[str, torch.Tensor], \n                        comparator: Comparator, \n                        rhs: str, rhs_state: Dict[str, torch.Tensor], \n                        label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Subclass of <code>BaseConditions</code>. Defines conditions on endogenous variable with name <code>endog_name</code>.</p>"},{"location":"api/evaluations/#constraint","title":"Constraint","text":"<pre><code>class Constraint(lhs: str, comparator: Comparator, rhs: str, \n            label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of a constraint (equality or inequality), and a set of variables (state, value, prices) in a model, parse the equation to a pytorch function that can be evaluated. If the constraint is an inequality, loss should be penalized whenever the inequality is not satisfied. </p>"},{"location":"api/evaluations/#eval_2","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor], \n                loss_reduction: LossReductionMethod=LossReductionMethod.MSE):\n</code></pre> <p>Example:  <pre><code>Constraint(lhs=\"L\", comparator=Comparator.GEQ, rhs=\"R\", \"eg1\")\n</code></pre> This represents the inequality: \\(L\\geq R\\), it is not satisfied when \\(R-L &gt; 0\\), so the loss is formulated as:</p> \\[\\mathcal{L}_{const} = \\frac{1}{B} \\|\\text{ReLU}(R(x)-L(x))\\|_2^2\\] <p>If strict inequality is required, <pre><code>Constraint(lhs=\"L\", comparator=Comparator.LT, rhs=\"R\", \"eg1\")\n</code></pre></p> <p>This represents the inequality: \\(L&lt; R\\), it is not satisfied when \\(L-R \\geq 0\\), an additional \\(\\epsilon=10^{-8}\\) is added to the ReLU activation to ensure strictness.</p>"},{"location":"api/evaluations/#endogequation","title":"EndogEquation","text":"<pre><code>class EndogEquation(eq: str, label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of an endogenuous equation, and a set of variables (state, value, prices) in a model. parse the LHS and RHS of the equation to pytorch functions that can be evaluated. This is used to define the algebraic (partial differential) equations as loss functions.</p>"},{"location":"api/evaluations/#eval_3","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor], \n                loss_reduction: LossReductionMethod=LossReductionMethod.MSE):\n</code></pre> <p>Computes the loss based on the equation:</p> \\[\\mathcal{L}_{endog} = \\frac{1}{B} \\|l(x)-r(x)\\|_2^2\\]"},{"location":"api/evaluations/#equation","title":"Equation","text":"<pre><code>class Equation(eq: str, label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of new variable definition, properly evaluate it with agent, endogenous variables, and constants. Assign new value to LHS.</p>"},{"location":"api/evaluations/#eval_4","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Compute and return the value of RHS, which will be assigned to LHS variable.</p>"},{"location":"api/evaluations/#hjbequation","title":"HJBEquation","text":"<pre><code>class HJBEquation(eq: str, label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of a Hamilton-Jacobi-Bellman equation, and a set of variables in a model, parse the equation to a pytorch function that can be evaluated.</p>"},{"location":"api/evaluations/#eval_5","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor], \n                loss_reduction: LossReductionMethod=LossReductionMethod.MSE):\n</code></pre> <p>Compute the MSE with zero as min/max problem.</p>"},{"location":"api/evaluations/#system","title":"System","text":"<pre><code>class System(activation_constraints: List[Constraint], \n            label: str=None, \n            latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Represents a system to be evaluated when <code>activation_constraints</code> are all satisfied.</p>"},{"location":"api/evaluations/#add_equation","title":"add_equation","text":"<p><pre><code>def add_equation(self, eq: str, label: str=None)\n</code></pre> Add an equation to define a new variable within the system</p>"},{"location":"api/evaluations/#add_endog_equation","title":"add_endog_equation","text":"<p><pre><code>def add_endog_equation(self, eq: str, label: str=None, weight=1.0, loss_reduction: LossReductionMethod=LossReductionMethod.MSE)\n</code></pre> Add an equation for loss computation within the system</p> <p>Note: None reduction is not supported in a system</p>"},{"location":"api/evaluations/#add_constraint","title":"add_constraint","text":"<p><pre><code>def add_constraint(self, lhs: str, comparator: Comparator, rhs: str, label: str=None, weight=1.0, loss_reduction: LossReductionMethod=LossReductionMethod.MSE)\n</code></pre> Add an inequality constraint for loss computation within the system</p> <p>Note: None reduction is not supported in a system</p>"},{"location":"api/evaluations/#compute_constraint_mask","title":"compute_constraint_mask","text":"<p><pre><code>def compute_constraint_mask(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor])\n</code></pre> Check if the constraint is satisfied. Need to check for each individual batch element. Get a mask \\(\\mathbb{1}_{mask}\\) for loss in each batch element.</p>"},{"location":"api/evaluations/#eval_6","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor])\n</code></pre> <p>Compute the loss based on the system constraint. Only elements in a batch that satisfy the <code>activation_constraints</code> are used in the loss computation.</p> \\[\\mathcal{L}_{endog, i} = \\frac{1}{\\sum \\mathbb{1}_{mask}} \\langle (l-r)^2, \\mathbb{1}_{mask}\\rangle\\] \\[\\mathcal{L}_{sys} = \\sum_{i=1}^N \\lambda_i \\mathcal{L}_{endog, i}\\]"},{"location":"api/evaluations/#loss-compute-methods","title":"Loss Compute Methods","text":"<p>The constants determine which loss reduction method to use.</p> <pre><code>class LossReductionMethod(str, Enum):\n    MSE = \"MSE\" # mean squared error\n    MAE = \"MAE\" # mean absolute error\n    SSE = \"SSE\" # sum squared error\n    SAE = \"SAE\" # sum absolute error\n    NONE = \"None\" # no reduction\n\nLOSS_REDUCTION_MAP = {\n    LossReductionMethod.MSE: lambda x: torch.mean(torch.square(x)),\n    LossReductionMethod.MAE: lambda x: torch.mean(torch.abs(x)),\n    LossReductionMethod.SSE: lambda x: torch.sum(torch.square(x)),\n    LossReductionMethod.SAE: lambda x: torch.sum(torch.absolute(x)),\n    LossReductionMethod.NONE: lambda x: x,\n}\n</code></pre>"},{"location":"api/models/","title":"deep_macrofin.models","text":""},{"location":"api/models/#learnablevar","title":"LearnableVar","text":"<pre><code>class LearnableVar(name: str, state_variables: List[str], config: Dict[str, Any])\n</code></pre> <p>Base class for agents and endogenous variables. This is a subclass of <code>torch.nn</code> module.</p> <p>Parameters:  </p> <ul> <li>name: str, The name of the model.  </li> <li>state_variables: List[str], List of state variables.  </li> <li>config: Dict[str, Any], specifies number of layers/hidden units of the neural network and highest order of derivatives to take. <ul> <li>device: str, the device to run the model on (e.g., \"cpu\", \"cuda\"), default will be chosen based on whether or not GPU is available  </li> <li>hidden_units: List[int], number of units in each layer, default: [30, 30, 30, 30]  </li> <li>layer_type: str, a selection from the LayerType enum, default: LayerType.MLP  </li> <li>activation_type: str, a selection from the ActivationType enum, default: ActivationType.Tanh  </li> <li>positive: bool, apply softplus to the output to be always positive if true, default: false  </li> <li>hardcode_function: a lambda function for hardcoded forwarding function, default: None  </li> <li>derivative_order: int, an additional constraint for the number of derivatives to take, so for a function with one state variable, we can still take multiple derivatives, default: number of state variables  </li> </ul> </li> </ul>"},{"location":"api/models/#get_all_derivatives","title":"get_all_derivatives","text":"<p>Get all derivatives of the current variable, upto a specific order defined by the user with <code>derivative_order</code>. The construction of derivatives can be found in derivative_utils.</p> <pre><code>def get_all_derivatives(self):\n    '''\n    Returns a dictionary of derivative functional mapping \n    e.g. if name=\"qa\", state_variables=[\"e\", \"t\"], derivative_order=2, it will return \n    {\n        \"qa\": self.forward\n        \"qa_e\": lambda x:self.compute_derivative(x, \"e\")\n        \"qa_t\": lambda x:self.compute_derivative(x, \"t\"),\n        \"qa_ee\": lambda x:self.compute_derivative(x, \"ee\"),\n        \"qa_tt\": lambda x:self.compute_derivative(x, \"tt\"),\n        \"qa_et\": lambda x:self.compute_derivative(x, \"et\"),\n        \"qa_te\": lambda x:self.compute_derivative(x, \"te\"),\n    }\n\n    Note that the last two will be the same for C^2 functions, \n    but we keep them for completeness. \n    '''\n</code></pre>"},{"location":"api/models/#plot","title":"plot","text":"<p>Plot a specific function attached to the learnable variable over the domain.</p> <pre><code>def plot(self, target: str, domain: Dict[str, List[np.float32]]={}, ax=None):\n    '''\n    Inputs:\n        target: name for the original function, or the associated derivatives to plot\n        domain: the range of state variables to plot. \n        If state_variables=[\"x\", \"y\"] domain = {\"x\": [0,1], \"y\":[-1,1]}, it will be plotted on the region [0,1]x[-1,1].\n        If one of the variable is not provided in the domain, [0,1] will be taken as the default\n        ax: a matplotlib.Axes object to plot on, if not provided, it will be plotted on a new figure\n\n    This function is only supported for 1D or 2D state_variables.\n    '''\n</code></pre>"},{"location":"api/models/#to_dict","title":"to_dict","text":"<pre><code>def to_dict(self)\n</code></pre> <p>Save all the configurations and weights to a dictionary.</p> <pre><code>dict_to_save = {\n    \"name\": self.name,\n    \"model\": self.state_dict(),\n    \"model_config\": self.config,\n    \"system_rng\": random.getstate(),\n    \"numpy_rng\": np.random.get_state(),\n    \"torch_rng\": torch.random.get_rng_state(),\n}\n</code></pre>"},{"location":"api/models/#from_dict","title":"from_dict","text":"<pre><code>def from_dict(self, dict_to_load: Dict[str, Any])\n</code></pre> <p>Load all the configurations and weights from a dictionary.</p>"},{"location":"api/models/#agent","title":"Agent","text":"<pre><code>class Agent(name: str, state_variables: List[str], config: Dict[str, Any])\n</code></pre> <p>Subclass of <code>LearnableVar</code>. Defines agent wealth multipliers. </p>"},{"location":"api/models/#endogvar","title":"EndogVar","text":"<pre><code>class EndogVar(name: str, state_variables: List[str], config: Dict[str, Any])\n</code></pre> <p>Subclass of <code>LearnableVar</code>. Defines endogenous variables. </p>"},{"location":"api/models/#derivative_utils","title":"derivative_utils","text":""},{"location":"api/models/#get_derivs_1order","title":"get_derivs_1order","text":"<p><pre><code>def get_derivs_1order(y, x, idx):\n</code></pre> Returns the first order derivatives of \\(y\\) w.r.t. \\(x\\). Automatic differentiation used.</p> <p>Example: <pre><code>x = torch.tensor([[1.0, 2.0]]) # assuming two variables x1, x2\nx.requires_grad_(True)\ny1 = x**2\nprint(get_derivs_1order(y1, x, 0)) \n'''\nOutput: [[2.]] dy1/dx1 = 2\n'''\nprint(get_derivs_1order(y1, x, 1))\n'''\nOutput: [[4.]] dy1/dx2 = 4\n'''\n\nx = torch.tensor([[1.0, 2.0]]) # assuming two variables x1, x2\nx.requires_grad_(True)\ny2 = x[:,0:1] * x[:,1:2] \nprint(get_derivs_1order(y2, x, 0)) \n'''\nOutput: [[2.]] dy2/dx1 = 2\n'''\nprint(get_derivs_1order(y2, x, 1))\n'''\nOutput: [[1.]] dy2/dx2 = 1\n'''\n</code></pre></p>"},{"location":"api/models/#get_all_derivs","title":"get_all_derivs","text":"<pre><code>def get_all_derivs(target_var_name=\"f\", all_vars: List[str] = [\"x\", \"y\", \"z\"], derivative_order = 2) -&gt; Dict[str, Callable]:\n</code></pre> <p>Implements Algorithm 1 in the paper. Higher order derivatives are computed iteratively using dynamic programming and first order derivative.</p> <p>Example: <pre><code>derivs = get_all_derivs(target_var_name=\"qa\", all_vars= [\"e\", \"t\"], derivative_order = 2)\n'''\nderivs = {\n    \"qa_e\": lambda y, x, idx=0: get_derivs_1order(y, x, idx)\n    \"qa_t\": lambda y, x, idx=1: get_derivs_1order(y, x, idx),\n    \"qa_ee\": lambda y, x, idx=0: get_derivs_1order(y, x, idx), # here y=qa_e\n    \"qa_et\": lambda y, x, idx=1: get_derivs_1order(y, x, idx), # here y=qa_e\n    \"qa_te\": lambda y, x, idx=0: get_derivs_1order(y, x, idx), # here y=qa_t\n    \"qa_tt\": lambda y, x, idx=1: get_derivs_1order(y, x, idx), # here y=qa_t\n}\n\nAfterwards, [e,t] should be passed as x into the lambda function\n'''\n</code></pre></p>"},{"location":"api/models/#activation-functions","title":"Activation Functions","text":"<p>The supported activation functions are listed in <code>ActivationTypes</code> below. ReLU, SiLU, Sigmoid and Tanh are default PyTorch activation functions. Wavelet is from Zhao, Ding, and Prakash 2024<sup>1</sup>. It is a learnable activation function of the form:</p> \\[\\text{Wavelet}(x) = w_1 \\sin(x) + w_2 \\cos(x),\\] <p>where \\(w_1\\) and \\(w_2\\) are learnable parameters.</p>"},{"location":"api/models/#constants","title":"Constants","text":"<p>These constants are used to identify model/layer/activation types for initialization.</p> <pre><code>class LearnableModelType(str, Enum):\n    Agent=\"Agent\"\n    EndogVar=\"EndogVar\"\n\nclass LayerType(str, Enum):\n    MLP=\"MLP\"\n    KAN=\"KAN\"\n    MultKAN=\"MultKAN\"\n\nclass ActivationType(str, Enum):\n    ReLU=\"relu\"\n    SiLU=\"silu\"\n    Sigmoid=\"sigmoid\"\n    Tanh=\"tanh\"\n    Wavelet=\"wavelet\"\n</code></pre> <ol> <li> <p>Zhiyuan Zhao, Xueying Ding, and B. Aditya Prakash, \"PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks\", The Twelfth International Conference on Learning Representations, 2024\u00a0\u21a9</p> </li> </ol>"},{"location":"api/pde_model/","title":"deep_macrofin.pde_model","text":"<p>This is the main interface to construct the PDE system to solve.</p>"},{"location":"api/pde_model/#pdemodel","title":"PDEModel","text":"<pre><code>class PDEModel(name: str, \n                config: Dict[str, Any] = DEFAULT_CONFIG, \n                latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Initialize a PDEModel with the provided name and config. </p> <p>Parameters:</p> <ul> <li>name: str, name of the PDE model.</li> <li>config: Dict[str, Any], defines the training configuration. Check Constants for default values.<ul> <li>batch_size: int</li> <li>num_epochs: int</li> <li>lr: float, learning rate for optimizer</li> <li>loss_log_interval: int, the interval at which loss should be reported/recorded</li> <li>optimizer_type: OptimizerType.Adam, OptimizerType.AdamW or OptimizerType.LBFGS</li> <li>sampling_method: SamplingMethod.UniformRandom, SamplingMethod.FixedGrid, SamplingMethod.ActiveLearning</li> <li>refinement_sample_interval: int,</li> <li>loss_balancing: bool, use Relative Loss Balancing with Random Lookback (ReLoBRaLo) for loss weight update</li> <li>bernoulli_prob: float, parameter for loss balancing </li> <li>loss_balancing_temp: float, parameter for loss balancing</li> <li>loss_balancing_alpha: float, parameter for loss balancing</li> <li>soft_adapt_interval: int, if larger than 0, use soft adapt for loss weight update, and the value is set to be the look-back interval.</li> <li>loss_soft_attention: bool, use soft attention for grid-wise loss weight updates.</li> </ul> </li> <li>latex_var_mapping: Dict[str, str], it should include all possible latex to python name conversions. Otherwise latex parsing will fail. Can be omitted if all the input equations/formula are not in latex form. For details, check <code>Formula</code>.</li> </ul>"},{"location":"api/pde_model/#set_state","title":"set_state","text":"<p><pre><code>def set_state(self, names: List[str], constraints: Dict[str, List] = {})\n</code></pre> Set the state variables (\"grid\") of the problem. By default, the constraints will be [-1, 1] (for easier sampling). Only rectangular regions are supported. Once an agent or endogenous variable has been added, calling set_state will raise an error.</p>"},{"location":"api/pde_model/#set_state_constraints","title":"set_state_constraints","text":"<p><pre><code>def set_state_constraints(self, constraints: Dict[str, List] = {})\n</code></pre> Overwrite the constraints for state variables, without changing the number of state variables. This can be used after adding an agent or endogenous variable and after loading a pre-trained model.</p>"},{"location":"api/pde_model/#add_param","title":"add_param","text":"<p><pre><code>def add_param(self, name: str, value: torch.Tensor)\n</code></pre> Add a single parameter (constant in the PDE system) with name and value.</p>"},{"location":"api/pde_model/#add_params","title":"add_params","text":"<p><pre><code>def add_params(self, params: Dict[str, Any])\n</code></pre> Add a dictionary of parameters (constants in the PDE system) for the system.</p>"},{"location":"api/pde_model/#add_agent","title":"add_agent","text":"<pre><code>def add_agent(self, name: str, \n            config: Dict[str, Any] = DEFAULT_LEARNABLE_VAR_CONFIG,\n            overwrite=False)\n</code></pre> <p>Add a single Agent, with relevant config of neural network representation. If called before states are set, should raise an error.</p> <p>Parameters:</p> <ul> <li>name: unique identifier of agent.</li> <li>Config: specifies number of layers/hidden units of the neural network. Check Constants for default values.</li> <li>overwrite: overwrite the previous agent with the same name, used for loading, default: False</li> </ul>"},{"location":"api/pde_model/#add_agents","title":"add_agents","text":"<p><pre><code>def add_agents(self, names: List[str], \n               configs: Dict[str, Dict[str, Any]]={})\n</code></pre> Add multiple Agents at the same time, each with different configurations.</p>"},{"location":"api/pde_model/#add_agent_condition","title":"add_agent_condition","text":"<p><pre><code>def add_agent_condition(self, name: str, \n                    lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str=None,\n                    weight: float=1.0, \n                    loss_reduction: LossReductionMethod=LossReductionMethod.MSE):\n</code></pre> Add boundary/initial condition for a specific agent with associated weight</p> <p>Parameters:</p> <ul> <li>name: str, agent name, </li> <li>lhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>lhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate lhs at for the agent/endogenous variable</li> <li>comparator: Comparator</li> <li>rhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>rhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate rhs at for the agent/endogenous variable, if rhs is a constant, this can be an empty dictionary</li> <li>label: str label for the condition, by default, it will self-increment <code>agent_cond_1</code>, <code>agent_cond_2</code>,...</li> <li>weight: float, weight in total loss computation</li> <li>loss_reduction: LossReductionMethod, <code>LossReductionMethod.MSE</code> for mean squared error, or <code>LossReductionMethod.MAE</code> for mean absolute error</li> </ul>"},{"location":"api/pde_model/#add_endog","title":"add_endog","text":"<pre><code>def add_endog(self, name: str, \n            config: Dict[str, Any] = DEFAULT_LEARNABLE_VAR_CONFIG,\n            overwrite=False)\n</code></pre> <p>Add a single EndogVar, with relevant config of neural network representation. If called before states are set, should raise an error.</p> <p>Parameters:</p> <ul> <li>name: unique identifier of the endogenous variable.</li> <li>Config: specifies number of layers/hidden units of the neural network. Check Constants for default values.</li> <li>overwrite: overwrite the previous endogenous variable with the same name, used for loading, default: False</li> </ul>"},{"location":"api/pde_model/#add_endogs","title":"add_endogs","text":"<p><pre><code>def add_endogs(self, names: List[str], \n               configs: Dict[str, Dict[str, Any]]={})\n</code></pre> Add multiple EndogVars at the same time, each with different configurations.</p>"},{"location":"api/pde_model/#add_endog_condition","title":"add_endog_condition","text":"<p><pre><code>def add_endog_condition(self, name: str, \n                    lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str=None,\n                    weight: float=1.0, \n                    loss_reduction: LossReductionMethod=LossReductionMethod.MSE):\n</code></pre> Add boundary/initial condition for a specific endogenous variable with associated weight</p> <p>Parameters:</p> <ul> <li>name: str, agent name, </li> <li>lhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>lhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate lhs at for the agent/endogenous variable</li> <li>comparator: Comparator</li> <li>rhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>rhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate rhs at for the agent/endogenous variable, if rhs is a constant, this can be an empty dictionary</li> <li>label: str label for the condition, by default, it will self-increment <code>endog_cond_1</code>, <code>endog_cond_2</code>,...</li> <li>weight: float, weight in total loss computation</li> <li>loss_reduction: LossReductionMethod, <code>LossReductionMethod.MSE</code> for mean squared error, or <code>LossReductionMethod.MAE</code> for mean absolute error</li> </ul>"},{"location":"api/pde_model/#add_equation","title":"add_equation","text":"<pre><code>def add_equation(self, eq: str, label: str=None)\n</code></pre> <p>Add an equation to define a new variable. </p>"},{"location":"api/pde_model/#add_endog_equation","title":"add_endog_equation","text":"<pre><code>def add_endog_equation(self, eq: str, label: str=None, weight=1.0, loss_reduction: LossReductionMethod=LossReductionMethod.MSE)\n</code></pre> <p>Add an endogenous equation for loss computation.</p>"},{"location":"api/pde_model/#add_constraint","title":"add_constraint","text":"<pre><code>def add_constraint(self, lhs: str, comparator: Comparator, rhs: str, label: str=None, weight=1.0, loss_reduction: LossReductionMethod=LossReductionMethod.MSE)\n</code></pre> <p>Add a constraint for loss computation.</p>"},{"location":"api/pde_model/#add_hjb_equation","title":"add_hjb_equation","text":"<pre><code>def add_hjb_equation(self, eq: str, label: str=None, weight=1.0, loss_reduction: LossReductionMethod=LossReductionMethod.MSE)\n</code></pre> <p>Add an HJB Equation for loss computation.</p>"},{"location":"api/pde_model/#add_system","title":"add_system","text":"<pre><code>def add_system(self, system: System, weight=1.0)\n</code></pre> <p>Add a System for loss computation.</p>"},{"location":"api/pde_model/#set_config","title":"set_config","text":"<pre><code>def set_config(self, config: Dict[str, Any] = DEFAULT_CONFIG)\n</code></pre> <p>This function overwrites the existing configurations. Can be used for L-BFGS finetuning</p>"},{"location":"api/pde_model/#train_model","title":"train_model","text":"<pre><code>def train_model(self, model_dir: str=\"./\", filename: str=None, full_log=False)\n</code></pre> <p>The entire loop of training</p> <p>Parameters:</p> <ul> <li>model_dir: str, the directory to save the model. If the directory doesn't exist, it will be created automatically.</li> <li>filename: str, the filename of the model, it will be the prefix for loss table and log file.</li> <li>full_log: bool, whether or not log all individual losses in the log file.</li> </ul>"},{"location":"api/pde_model/#eval_model","title":"eval_model","text":"<pre><code>def eval_model(self, full_log=False)\n</code></pre> <p>The entire loop of evaluation</p>"},{"location":"api/pde_model/#validate_model_setup","title":"validate_model_setup","text":"<pre><code>def validate_model_setup(self, model_dir=\"./\")\n</code></pre> <p>Check that all the equations/constraints given are valid. If not, log the errors in a file, and raise an ultimate error.</p>"},{"location":"api/pde_model/#save_model","title":"save_model","text":"<pre><code>def save_model(self, model_dir: str = \"./\", filename: str=None, verbose=False)\n</code></pre> <p>Save all the agents, endogenous variables (pytorch model and configurations), and all other configurations of the PDE model.</p> <p>Parameters:</p> <ul> <li>model_dir: str, the directory to save the model</li> <li>filename: str, the filename to save the model without suffix, default: self.name </li> </ul>"},{"location":"api/pde_model/#load_model","title":"load_model","text":"<pre><code>def load_model(self, dict_to_load: Dict[str, Any])\n</code></pre> <p>Load all the agents, endogenous variables (pytorch model and configurations) from the dictionary.</p>"},{"location":"api/pde_model/#plot_vars","title":"plot_vars","text":"<pre><code>def plot_vars(self, vars_to_plot: List[str], ncols: int=4)\n</code></pre> <p>Parameters:</p> <ul> <li>vars_to_plot: List[str], variable names to plot, can be an equation defining a new variable. If Latex, need to be enclosed by $$ symbols</li> <li>ncols: int, number of columns to plot, default: 4</li> <li>elev, azim, roll: view angles for 3D plots. See Matplotlib Document for details.</li> </ul>"},{"location":"api/pde_model_time_step/","title":"deep_macrofin.pde_model_time_step","text":"<p>This is a subclass of PDEModel that implements the time stepping scheme with neural network. It has exactly the same API as the base class.</p>"},{"location":"api/pde_model_time_step/#pdemodeltimestep","title":"PDEModelTimeStep","text":"<pre><code>class PDEModelTimeStep(PDEModel):\n'''\nPDEModelTimeStep uses time stepping scheme + neural network to solve for optimality\n\nPDEModel class to assign variables, equations &amp; constraints, etc.\n\nAlso initialize the neural network architectures for each agent/endogenous variables \nwith some config dictionary.\n'''\n</code></pre>"},{"location":"api/pde_model_time_step/#set_initial_guess","title":"set_initial_guess","text":"<pre><code>def set_initial_guess(self, initial_guess: Dict[str, float])\n</code></pre> <p>Set the initial guess (uniform value across the state variable domain) for agents or endogenous variables. This is the boundary condition at \\(t=T\\) in the first time iteration.</p>"},{"location":"api/utils/","title":"deep_macrofin.utils","text":""},{"location":"api/utils/#set_seeds","title":"set_seeds","text":"<pre><code>def set_seeds(seed)\n</code></pre> <p>Set the random seeds of <code>random</code>, <code>numpy</code>, and <code>torch</code> to the provided seed. It is used by default before training loop.</p>"},{"location":"api/utils/#plot_loss_df","title":"plot_loss_df","text":"<pre><code>def plot_loss_df(fn: str=None, loss_df: pd.DataFrame=None, losses_to_plot: list=None, loss_plot_fn: str= \"./plot.jpg\")\n</code></pre> <p>Plot the provided loss df, with all losses listed in the losses_to_plot.</p> <p>Parameters:</p> <ul> <li>fn: str, the relative path to loss df csv, default: None</li> <li>loss_df: pd.DataFrame, the loaded loss df, default: None, at least one of fn and loss_df should not be None.</li> <li>losses_to_plot: List[str], the losses to plot, if None, all losses in the df will be plotted, default: None</li> <li>loss_plot_fn: str, the path to save the loss plot, default: \"./plot.jpg\"</li> </ul>"},{"location":"api/utils/#constants","title":"Constants","text":"<pre><code>class OptimizerType(str, Enum):\n    Adam = \"Adam\"\n    AdamW = \"AdamW\"\n    LBFGS = \"LBFGS\"\n\nclass SamplingMethod(str, Enum):\n    UniformRandom = \"UniformRandom\"\n    FixedGrid = \"FixedGrid\"\n    ActiveLearning = \"ActiveLearning\"\n    RARG = \"RAR-G\"\n    RARD = \"RAR-D\"\n\nDEFAULT_CONFIG = {\n    \"batch_size\": 100,\n    \"num_epochs\": 1000,\n    \"lr\": 1e-3,\n    \"loss_log_interval\": 100,\n    \"optimizer_type\": OptimizerType.AdamW,\n    \"sampling_method\": SamplingMethod.UniformRandom,\n    \"loss_balancing\": False,\n    \"bernoulli_prob\": 0.9999,\n    \"loss_balancing_temp\": 0.1,\n    \"loss_balancing_alpha\": 0.999,\n    \"soft_adapt_interval\": -1,\n    \"loss_soft_attention\": False,\n}\n\nDEFAULT_CONFIG_TIME_STEP = {\n    \"batch_size\": 100,\n    \"num_outer_iterations\": 100,\n    \"num_inner_iterations\": 5000,\n    \"lr\": 1e-3,\n    \"loss_log_interval\": 100,\n    \"optimizer_type\": OptimizerType.Adam,\n    \"min_t\": 0.0,\n    \"max_t\": 1.0,\n    \"outer_loop_convergence_thres\": 1e-4,\n    \"sampling_method\": SamplingMethod.FixedGrid,\n    \"time_batch_size\": None,\n}\n\nDEFAULT_LEARNABLE_VAR_CONFIG = {\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"hidden_units\": [30, 30, 30, 30],\n    \"layer_type\": LayerType.MLP,\n    \"activation_type\": ActivationType.Tanh,\n    \"positive\": False,\n    \"derivative_order\": 2,\n}\n</code></pre>"},{"location":"examples/approx/discont/","title":"Discontinuous and Oscillating Function","text":"<p>The full solution can be found at function_approximation.ipynb.</p>"},{"location":"examples/approx/discont/#problem-setup","title":"Problem Setup","text":"\\[ y= \\begin{cases}      5 + \\sum_{k=1}^4 \\sin(kx), x&lt;0\\\\     \\cos(10x), x\\geq 0  \\end{cases} \\]"},{"location":"examples/approx/discont/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we use the default training configuration, and default setup for learnable endogenous variable. <pre><code>discont_approx2 = PDEModel(\"discontinuous_approximator\", {\"num_epochs\": 50000}) # define PDE model with 50,000 epochs\ndiscont_approx2.set_state([\"x\"], {\"x\": [-3., 3.]}) # set the state variable \"x\" with its range\ndiscont_approx2.add_endog(\"y\", { \n    \"hidden_units\": [40, 40], # specify the architecture with two hidden layers of 40 units each\n    \"activation_type\": ActivationType.SiLU, # set the activation function to SiLU\n})\ndiscont_approx2.add_endog_equation(\"y=5+sin(x)+sin(2*x)+sin(3*x)+sin(4*x)\") \ndiscont_approx2.add_endog_equation(\"y=cos(10*x)\") \n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>discont_approx2.train_model(\"./models/discont_approx2\", \"discont_approx2.pt\", True)\ndiscont_approx2.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>discont_approx2.load_model(torch.load(\"./models/discont_approx2/discont_approx2_best.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 3, figsize=(18, 6))\nx = np.linspace(-3, 3)\nx_neg = np.linspace(-3, 0)\nx_pos = np.linspace(0, 3)\nax[0].plot(x_neg, 5+np.sin(x_neg)+np.sin(2*x_neg)+np.sin(3*x_neg)+np.sin(4*x_neg), label=\"5+sin(x)+sin(2x)+sin(3x)+sin(4x)\", color=\"red\")\nax[0].plot(x_pos, np.cos(10*x_pos), label=\"cos(10x)\", color=\"red\")\nax[1].plot(x_neg, np.cos(x_neg)+2*np.cos(2*x_neg)+3*np.cos(3*x_neg)+4*np.cos(4*x_neg), label=\"cos(x)+2cos(2x)+3cos(3x)+4cos(4x)\", color=\"red\")\nax[1].plot(x_pos, -10*np.sin(10*x_pos), label=\"-10sin(10x)\", color=\"red\")\nax[2].plot(x_neg, -np.sin(x_neg)-4*np.sin(2*x_neg)-9*np.sin(3*x_neg)-16*np.sin(4*x_neg), label=\"-sin(x)-4sin(2x)-9sin(3x)-16sin(4x)\", color=\"red\")\nax[2].plot(x_pos, -100*np.cos(10*x_pos), label=\"-100cos(10x)\", color=\"red\")\ndiscont_approx2.endog_vars[\"y\"].plot(\"y\", {\"x\": [-3, 3]}, ax=ax[0])\ndiscont_approx2.endog_vars[\"y\"].plot(\"y_x\", {\"x\": [-3, 3]}, ax=ax[1])\ndiscont_approx2.endog_vars[\"y\"].plot(\"y_xx\", {\"x\": [-3, 3]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/macrofinance_models/1d_problem/","title":"1D Problem","text":"<p>The full solution can be found at problem1.ipynb.</p>"},{"location":"examples/macrofinance_models/1d_problem/#problem-setup","title":"Problem Setup","text":""},{"location":"examples/macrofinance_models/1d_problem/#parameter-and-variable-definitions","title":"Parameter and Variable Definitions","text":"Parameter Definition Value \\(\\gamma^j\\) relative risk aversion \\(\\gamma^i=2, \\gamma^h=5\\) \\(\\rho^j\\) discount rate \\(\\rho^i=\\rho^h=0.05\\) \\(\\zeta^j\\) intertemporal elasticity of substitution \\(\\zeta^i=\\zeta^h=1.00005\\) \\(\\mu^a\\) growth rate of capital \\(\\mu^a=0.04\\) \\(\\sigma^a\\) volatility of capital \\(\\sigma^{a}=0.2\\) \\(\\mu^O\\) sentiment factor \\(\\mu^O=0.04\\) \\(\\alpha^a\\) productivity \\(\\alpha^a=0.1\\) \\(\\kappa\\) investment cost \\(\\kappa=10000\\) Type Definition State Variables \\(\\eta_t\\in[0,1]\\) (\\(\\eta\\)) Agents \\(\\xi_t^i\\), \\(\\xi_t^h\\) Endogenous Variables \\(\\mu_t^{\\eta}\\), \\(\\sigma_t^{\\eta a}\\), \\(q_t^a\\), \\(w_t^{ia}\\), \\(w_t^{ha}\\)"},{"location":"examples/macrofinance_models/1d_problem/#equations","title":"Equations","text":"\\[\\iota_t^a = \\frac{q_t^a - 1}{\\kappa}\\] \\[\\Phi(\\iota_t^a) = \\frac{1}{\\kappa}\\log(1+\\kappa\\iota_t^a)\\] \\[c_t^j = (\\rho^j)^{\\zeta^j}(\\xi_t^j)^{1-\\zeta^j}\\] \\[\\sigma_t^{qa} = \\frac{1}{q_t^a} \\frac{\\partial q_t^a}{\\partial \\eta_t} \\sigma_t^{\\eta a} \\eta_t\\] \\[\\sigma_t^{nja} = w_t^{ja}(\\sigma^a + \\sigma_t^{qa})\\] \\[\\sigma_t^{\\xi ja}  = \\frac{1}{\\xi_t^j}\\frac{\\partial \\xi_t^j}{\\partial \\eta_t}\\sigma_t^{\\eta a}\\eta_t \\] \\[\\sigma_t^{na} = \\eta_t\\sigma_t^{nia} + (1-\\eta_t)\\sigma_t^{nha}\\] \\[\\mu_t^{qa} = \\frac{1}{q_t^a} \\left(\\frac{\\partial q_t^a}{\\partial \\eta_t} \\mu_t^{\\eta} \\eta_t + \\frac{1}{2} \\frac{\\partial^2 q_t^a}{\\partial \\eta_t^2} (\\sigma_t^{\\eta a} \\eta_t)^2\\right)\\] \\[r_t^{ka} = \\mu_t^{qa} + \\mu^a + \\Phi_t^a + \\sigma^a\\sigma^{qa} + \\frac{\\alpha^a - \\iota_t^a}{q_t^a}\\] \\[r_t = r_t^{ka} - \\gamma^hw_t^{ha}(\\sigma^a + \\sigma_t^{qa})^2 + (1-\\gamma^h)\\sigma_t^{\\xi ha}(\\sigma^a + \\sigma_t^{qa})\\] \\[\\mu_t^{nj} = r_t - c_t^j + w_t^{ja}(r_t^{ka} - r_t)\\] \\[\\mu_t^{\\xi j} = \\frac{1}{\\xi_t^j}\\left(\\frac{\\partial \\xi_t^j}{\\partial \\eta_t}\\mu_t^{\\eta}\\eta_t + \\frac{1}{2}\\frac{\\partial^2 \\xi_t^j}{\\partial \\eta_t^2}(\\sigma_t^{\\eta a}\\eta_t)^2\\right)\\] \\[\\hat{r_t^{ka}} = r_t^{ka} + \\frac{\\mu^O - \\mu^a}{\\sigma^a}(\\sigma^a + \\sigma_t^{qa})\\]"},{"location":"examples/macrofinance_models/1d_problem/#endogenous-equations","title":"Endogenous Equations","text":"\\[\\mu_t^{\\eta} = (1-\\eta_t)(\\mu_t^{ni} - \\mu_t^{nh}) +(\\sigma_t^{na})^2  - \\sigma_t^{nia}\\sigma_t^{na}\\] \\[\\sigma_t^{\\eta a} = (1-\\eta_t)(\\sigma_t^{nia} - \\sigma_t^{nha})\\] \\[\\hat{r_t^{ka}} - r_t = \\gamma^iw_t^{ia}(\\sigma^a  + \\sigma_t^{qa})^2 - (1-\\gamma^i)\\sigma_t^{\\xi ia}(\\sigma^{a}  + \\sigma_t^{qa})\\] \\[1 = w_t^{ia}\\eta_t + w_t^{ha}(1-\\eta_t)\\] \\[\\alpha^a - \\iota_t^a = (c_t^i\\eta_t + c_t^h(1 - \\eta_t))q_t^a\\]"},{"location":"examples/macrofinance_models/1d_problem/#hjb-equations","title":"HJB Equations","text":"\\[ \\frac{\\rho^i}{1-\\frac{1}{\\zeta^i}}\\left( \\left(\\frac{c_t^i}{\\xi_t^i} \\right)^{1-1/\\zeta^i}-1 \\right) + \\mu_t^{\\xi i} +  \\mu_t^{ni} - \\frac{\\gamma^i}{2}(\\sigma_t^{nia})^2  - \\frac{\\gamma^i}{2}(\\sigma_t^{\\xi ia})^2 + (1-\\gamma^i)\\sigma_t^{\\xi ia}\\sigma_t^{nia}\\] \\[ \\frac{\\rho^h}{1-\\frac{1}{\\zeta^h}}\\left( \\left(\\frac{c_t^h}{\\xi_t^h} \\right)^{1-1/\\zeta^h}-1 \\right) + \\mu_t^{\\xi h} +  \\mu_t^{nh} - \\frac{\\gamma^h}{2}(\\sigma_t^{nha})^2  - \\frac{\\gamma^h}{2}(\\sigma_t^{\\xi ha})^2 + (1-\\gamma^h)\\sigma_t^{\\xi ha}\\sigma_t^{nha}\\]"},{"location":"examples/macrofinance_models/1d_problem/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import OptimizerType, plot_loss_df, set_seeds\n</code></pre></p> </li> <li> <p>Define LaTex-variable mapping so that we can parse LaTex equations <pre><code>latex_var_mapping = {\n    # constants\n    r\"\\gamma^i\": \"gammai\",\n    r\"\\gamma^h\": \"gammah\",\n    r\"\\rho^i\": \"rhoi\",\n    r\"\\rho^h\": \"rhoh\",\n    r\"\\sigma^a\": \"siga\",\n    r\"\\sigma^{a}\": \"siga\",\n    r\"\\mu^a\": \"mua\",\n    r\"\\mu^O\": \"muO\",\n    r\"\\zeta^i\": \"zetai\",\n    r\"\\zeta^h\": \"zetah\",\n    r\"\\alpha^a\": \"aa\",\n    r\"\\kappa\": \"kappa\",\n\n    # state variables\n    r\"\\eta_t\": \"eta\",\n\n    # learnable variables (agents + endogenous variables)\n    r\"\\xi_t^i\": \"xii\",\n    r\"\\xi_t^h\": \"xih\",\n    r\"\\mu_t^{\\eta}\": \"mue\",\n    r\"\\sigma_t^{\\eta a}\": \"sigea\",\n    r\"q_t^a\": \"qa\",\n    r\"w_t^{ia}\": \"wia\",\n    r\"w_t^{ha}\": \"wha\",\n\n    # variables defined by equations\n    r\"\\iota_t^a\": \"iota_a\",\n    r\"\\Phi_t^a\": \"phi_a\",\n    r\"c_t^i\": \"ci\",\n    r\"c_t^h\": \"ch\",\n    r\"\\sigma^{qa}\": \"sigqa\",\n    r\"\\sigma_t^{qa}\": \"sigqa\",\n    r\"\\sigma_t^{nia}\": \"signia\",\n    r\"\\sigma_t^{nha}\": \"signha\",\n    r\"\\sigma_t^{\\xi ia}\": \"sigxia\",\n    r\"\\sigma_t^{\\xi ha}\": \"sigxha\",\n    r\"\\sigma_t^{na}\": \"signa\",\n    r\"\\mu_t^{qa}\": \"muqa\",\n    r\"\\hat{r_t^{ka}}\": \"rka_hat\", # define hat first, and replace hat first\n    r\"r_t^{ka}\": \"rka\",\n    r\"\\mu_t^{ni}\": \"muni\",\n    r\"\\mu_t^{nh}\": \"munh\",\n    r\"\\mu_t^{\\xi i}\": \"muxi\",\n    r\"\\mu_t^{\\xi h}\": \"muxh\",\n}\n</code></pre></p> </li> <li> <p>Define the problem <pre><code>set_seeds(0)\npde_model = PDEModel(\"problem1\", {\"num_epochs\": 2000, \"loss_log_interval\": 100, \"optimizer_type\": OptimizerType.Adam}, latex_var_mapping)\npde_model.set_state([\"eta\"], {\"eta\": [0.01, 0.99]})\npde_model.add_agents([\"xii\", \"xih\"], {\"xii\": {\"positive\": True}, \"xih\": {\"positive\": True}})\npde_model.add_endogs([\"mue\", \"qa\", \"wia\", \"wha\", \"sigea\"], {\"qa\": {\"positive\": True}})\npde_model.add_params({\n    \"gammai\": 2.0,\n    \"gammah\": 5.0,\n    \"rhoi\": 0.05,\n    \"rhoh\": 0.05,\n    \"siga\": 0.2, #\\sigma^{a}\n    \"mua\": 0.04,\n    \"muO\": 0.04,\n    \"zetai\":1.00005,\n    \"zetah\":1.00005,\n    \"aa\":0.1,\n    \"kappa\":10000\n})\npde_model.add_equation(r\"$\\iota_t^a = \\frac{q_t^a - 1}{\\kappa}$\")\npde_model.add_equation(r\"$\\Phi_t^a = \\frac{1}{\\kappa} * \\log(1+\\kappa * \\iota_t^a)$\")\npde_model.add_equation(r\"$c_t^i = (\\rho^i)^{\\zeta^i} * (\\xi_t^i)^{1-\\zeta^i}$\")\npde_model.add_equation(r\"$c_t^h = (\\rho^h)^{\\zeta^h} * (\\xi_t^h)^{1-\\zeta^h}$\")\npde_model.add_equation(r\"$\\sigma_t^{qa} = \\frac{1}{q_t^a}  * \\frac{\\partial q_t^a}{\\partial \\eta_t}  * \\sigma_t^{\\eta a}  * \\eta_t$\")\npde_model.add_equation(r\"$\\sigma_t^{nia} = w_t^{ia}*(\\sigma^a + \\sigma_t^{qa})$\")\npde_model.add_equation(r\"$\\sigma_t^{nha} = w_t^{ha}*(\\sigma^a + \\sigma_t^{qa})$\")\npde_model.add_equation(r\"$\\sigma_t^{\\xi ia}  = \\frac{1}{\\xi_t^i} * \\frac{\\partial \\xi_t^i}{\\partial \\eta_t} * \\sigma_t^{\\eta a} * \\eta_t$\")\npde_model.add_equation(r\"$\\sigma_t^{\\xi ha}  = \\frac{1}{\\xi_t^h} * \\frac{\\partial \\xi_t^h}{\\partial \\eta_t} * \\sigma_t^{\\eta a} * \\eta_t$\")\npde_model.add_equation(r\"$\\sigma_t^{na} = \\eta_t * \\sigma_t^{nia} + (1-\\eta_t) * \\sigma_t^{nha}$\")\npde_model.add_equation(r\"$\\mu_t^{qa} = \\frac{1}{q_t^a}  * \\left(\\frac{\\partial q_t^a}{\\partial \\eta_t}  * \\mu_t^{\\eta}  * \\eta_t + \\frac{1}{2}  * \\frac{\\partial^2 q_t^a}{\\partial \\eta_t^2}  * (\\sigma_t^{\\eta a}  * \\eta_t)^2\\right)$\")\npde_model.add_equation(r\"$r_t^{ka} = \\mu_t^{qa} + \\mu^a + \\Phi_t^a + \\sigma^a * \\sigma^{qa} + \\frac{\\alpha^a - \\iota_t^a}{q_t^a}$\")\npde_model.add_equation(r\"$r_t = r_t^{ka} - \\gamma^h * w_t^{ha} * (\\sigma^a + \\sigma_t^{qa})^2 + (1-\\gamma^h) * \\sigma_t^{\\xi ha} * (\\sigma^a + \\sigma_t^{qa})$\")\npde_model.add_equation(r\"$\\mu_t^{ni} = r_t - c_t^i + w_t^{ia} * (r_t^{ka} - r_t)$\")\npde_model.add_equation(r\"$\\mu_t^{nh} = r_t - c_t^h + w_t^{ha} * (r_t^{ka} - r_t)$\")\npde_model.add_equation(r\"$\\mu_t^{\\xi i} = \\frac{1}{\\xi_t^i} * \\left(\\frac{\\partial \\xi_t^i}{\\partial \\eta_t} * \\mu_t^{\\eta} * \\eta_t + \\frac{1}{2} * \\frac{\\partial^2 \\xi_t^i}{\\partial \\eta_t^2} * (\\sigma_t^{\\eta a} * \\eta_t)^2\\right)$\")\npde_model.add_equation(r\"$\\mu_t^{\\xi h} = \\frac{1}{\\xi_t^h} * \\left(\\frac{\\partial \\xi_t^h}{\\partial \\eta_t} * \\mu_t^{\\eta} * \\eta_t + \\frac{1}{2} * \\frac{\\partial^2 \\xi_t^h}{\\partial \\eta_t^2} * (\\sigma_t^{\\eta a} * \\eta_t)^2\\right)$\")\npde_model.add_equation(r\"$\\hat{r_t^{ka}} = r_t^{ka} + \\frac{\\mu^O - \\mu^a}{\\sigma^a} * (\\sigma^a + \\sigma_t^{qa})$\")\n\npde_model.add_endog_equation(r\"$\\mu_t^{\\eta} = (1-\\eta_t) * (\\mu_t^{ni} - \\mu_t^{nh}) +(\\sigma_t^{na})^2  - \\sigma_t^{nia} * \\sigma_t^{na}$\")\npde_model.add_endog_equation(r\"$\\sigma_t^{\\eta a} = (1-\\eta_t) * (\\sigma_t^{nia} - \\sigma_t^{nha})$\")\npde_model.add_endog_equation(r\"$\\hat{r_t^{ka}} - r_t = \\gamma^i * w_t^{ia} * (\\sigma^a  + \\sigma_t^{qa})^2 - (1-\\gamma^i) * \\sigma_t^{\\xi ia} * (\\sigma^{a}  + \\sigma_t^{qa})$\")\npde_model.add_endog_equation(r\"$1 = w_t^{ia} * \\eta_t + w_t^{ha} * (1-\\eta_t)$\")\npde_model.add_endog_equation(r\"$\\alpha^a - \\iota_t^a = (c_t^i*\\eta_t + c_t^h * (1 - \\eta_t)) * q_t^a$\")\n\npde_model.add_hjb_equation(r\"$\\frac{\\rho^i}{1-\\frac{1}{\\zeta^i}} * \\left( \\left(\\frac{c_t^i}{\\xi_t^i} \\right)^{1-1/\\zeta^i}-1 \\right) + \\mu_t^{\\xi i} +  \\mu_t^{ni} - \\frac{\\gamma^i}{2} * (\\sigma_t^{nia})^2  - \\frac{\\gamma^i}{2} * (\\sigma_t^{\\xi ia})^2 + (1-\\gamma^i) * \\sigma_t^{\\xi ia} * \\sigma_t^{nia}$\")\npde_model.add_hjb_equation(r\"$\\frac{\\rho^h}{1-\\frac{1}{\\zeta^h}} * \\left( \\left(\\frac{c_t^h}{\\xi_t^h} \\right)^{1-1/\\zeta^h}-1 \\right) + \\mu_t^{\\xi h} +  \\mu_t^{nh} - \\frac{\\gamma^h}{2} * (\\sigma_t^{nha})^2  - \\frac{\\gamma^h}{2} * (\\sigma_t^{\\xi ha})^2 + (1-\\gamma^h) * \\sigma_t^{\\xi ha} * \\sigma_t^{nha}$\")\n\nprint(pde_model)\n</code></pre></p> </li> <li> <p>Train and evaluate the best model with min loss <pre><code>pde_model.train_model(\"./models/1d_prob\", \"1d_prob.pt\", True)\npde_model.load_model(torch.load(\"./models/1d_prob/1d_prob_best.pt\"))\npde_model.eval_model(True)\n</code></pre></p> </li> <li> <p>Plot the solutions, with additional variables defined by equations. <pre><code>pde_model.plot_vars([r\"$q_t^a$\", r\"$\\sigma_t^{qa}$\", r\"$w_t^{ia}$\", r\"$w_t^{ha}$\", \"rp_a = rka-r_t\"])\n</code></pre></p> </li> </ol>"},{"location":"examples/macrofinance_models/di_tella/","title":"Di Tella (2017)","text":"<p>The full solution can be found at ditella_with_investment.ipynb.</p>"},{"location":"examples/macrofinance_models/di_tella/#problem-setup","title":"Problem Setup","text":"<p>This is the stochastic volatility model from Di Tella 2017<sup>1</sup></p>"},{"location":"examples/macrofinance_models/di_tella/#parameter-and-variable-definitions","title":"Parameter and Variable Definitions","text":"Parameter Definition Value \\(a\\) relative risk aversion \\(a=1\\) \\(\\sigma\\) volatility of TFP shocks \\(\\sigma=0.0125\\) \\(\\lambda\\) mean reversion coefficient for idiosyncratic risk \\(\\lambda=1.38\\) \\(\\bar{v}\\) long-run mean of idiosyncratic risk \\(\\bar{v}=0.25\\) \\(\\bar{\\sigma_v}\\) idiosyncratic volatility of capital on aggregate risk \\(\\bar{\\sigma_v}=-0.17\\) \\(\\rho\\) discount rate \\(\\rho=0.0665\\) \\(\\gamma\\) risk aversion rate \\(\\gamma=5\\) \\(\\psi\\) inverse of elasticity of intertemporal substitution \\(\\psi=0.5\\) s.t. EIS=2 \\(\\tau\\) Poisson retirement rate for experts \\(\\tau=1.15\\) \\(\\phi\\) moral hazzard \\(\\phi=0.2\\) \\(A\\) second order coefficient for investment function \\(A=53\\) \\(B\\) first order coefficient for investment function \\(B=-0.8668571428571438\\) \\(\\delta\\) shift for investment function \\(\\delta=0.05\\) Type Definition State Variables \\((x, v)\\in [0.05, 0.95]\\times [0.05,0.95]\\) Agents \\(\\xi\\) (experts), \\(\\zeta\\) (households) Endogenous Variables \\(p\\) (price), \\(r\\) (risk-free rate) <p>Note: For an ideal model, \\((x,v)\\in (0,1)\\times (0,\\infty)\\), we use \\([0.05, 0.95]\\times [0.05,0.95]\\) to be consistent with the original paper.</p>"},{"location":"examples/macrofinance_models/di_tella/#equations","title":"Equations","text":"\\[g = \\frac{1}{2A}(p - B) - \\delta\\] \\[\\iota = A(g+\\delta)^2 + B(g+\\delta)\\] \\[\\mu_v = \\lambda(\\bar{v} - v)\\] \\[\\sigma_v = \\bar{\\sigma_v}\\sqrt{v} \\] \\[\\hat{e} = \\rho^{1/\\psi}\\xi^{(\\psi-1)/\\psi}\\] \\[\\hat{c} = \\rho^{1/\\psi}\\zeta^{(\\psi-1)/\\psi}\\] \\[\\sigma_{x,1} = (1-x)x\\frac{1-\\gamma}{\\gamma}\\left( \\frac{1}{\\xi}\\frac{\\partial \\xi}{\\partial v} - \\frac{1}{\\zeta}\\frac{\\partial \\zeta}{\\partial v} \\right)\\] \\[\\sigma_{x,2} = 1 - (1-x)x\\frac{1-\\gamma}{\\gamma}\\left( \\frac{1}{\\xi}\\frac{\\partial \\xi}{\\partial x} - \\frac{1}{\\zeta}\\frac{\\partial \\zeta}{\\partial x} \\right)\\] \\[\\sigma_x = \\frac{\\sigma_{x,1}}{\\sigma_{x,2}}\\sigma_v\\] \\[\\sigma_p = \\frac{1}{p}\\left( \\frac{\\partial p}{\\partial v}\\sigma_v + \\frac{\\partial p}{\\partial x}\\sigma_x \\right)\\] \\[\\sigma_\\xi = \\frac{1}{\\xi}\\left( \\frac{\\partial \\xi}{\\partial v}\\sigma_v + \\frac{\\partial \\xi}{\\partial x}\\sigma_x \\right)\\] \\[\\sigma_\\zeta = \\frac{1}{\\zeta}\\left( \\frac{\\partial \\zeta}{\\partial v}\\sigma_v + \\frac{\\partial \\zeta}{\\partial x}\\sigma_x \\right)\\] \\[\\sigma_n = \\sigma + \\sigma_p + \\frac{\\sigma_x}{x}\\] \\[\\pi = \\gamma\\sigma_n + (\\gamma-1)\\sigma_\\xi\\] \\[\\sigma_w = \\frac{\\pi}{\\gamma} - \\frac{\\gamma-1}{\\gamma} \\sigma_\\zeta\\] \\[\\mu_w = r + \\pi\\sigma_w\\] \\[\\mu_n = r + \\frac{\\gamma}{x^2}(\\phi v)^2 + \\pi\\sigma_n\\] \\[\\tilde{\\sigma_n} = \\frac{\\phi}{x}v\\] \\[\\mu_x = x\\left(\\mu_n - \\hat{e} - \\tau + \\frac{a-\\iota}{p} - r - \\pi(\\sigma+\\sigma_p) - \\frac{\\gamma}{x}(\\phi v)^2 + (\\sigma + \\sigma_p)^2 - \\sigma_n(\\sigma + \\sigma_p)\\right)\\] \\[\\mu_p = \\frac{1}{p}\\left( \\mu_v\\frac{\\partial p}{\\partial v} + \\mu_x\\frac{\\partial p}{\\partial x} + \\frac{1}{2}\\left( \\sigma_v^2\\frac{\\partial^2 p}{\\partial v^2} + 2\\sigma_v\\sigma_x\\frac{\\partial^2 p}{\\partial v \\partial x} + \\sigma_x^2\\frac{\\partial^2 p}{\\partial x^2} \\right)\\right)\\] \\[\\mu_\\xi = \\frac{1}{\\xi}\\left( \\mu_v\\frac{\\partial \\xi}{\\partial v} + \\mu_x\\frac{\\partial \\xi}{\\partial x} + \\frac{1}{2}\\left( \\sigma_v^2\\frac{\\partial^2 \\xi}{\\partial v^2} + 2\\sigma_v\\sigma_x\\frac{\\partial^2 \\xi}{\\partial v \\partial x} + \\sigma_x^2\\frac{\\partial^2 \\xi}{\\partial x^2} \\right)\\right)\\] \\[\\mu_\\zeta = \\frac{1}{\\zeta}\\left( \\mu_v\\frac{\\partial \\zeta}{\\partial v} + \\mu_x\\frac{\\partial \\zeta}{\\partial x} + \\frac{1}{2}\\left( \\sigma_v^2\\frac{\\partial^2 \\zeta}{\\partial v^2} + 2\\sigma_v\\sigma_x\\frac{\\partial^2 \\zeta}{\\partial v \\partial x} + \\sigma_x^2\\frac{\\partial^2 \\zeta}{\\partial x^2} \\right)\\right)\\]"},{"location":"examples/macrofinance_models/di_tella/#endogenous-equations","title":"Endogenous Equations","text":"\\[a - \\iota = p(\\hat{e}x + \\hat{c}(1-x)) \\] \\[\\sigma + \\sigma_p = \\sigma_nx + \\sigma_w(1-x)\\] \\[\\frac{a-\\iota}{p} + g + \\mu_p + \\sigma\\sigma_p - r = (\\sigma + \\sigma_p)\\pi + \\gamma\\frac{1}{x}(\\phi v)^2\\]"},{"location":"examples/macrofinance_models/di_tella/#hjb-equations","title":"HJB Equations","text":"\\[\\frac{\\rho}{1-\\psi} = \\max \\left\\{\\frac{\\hat{e}^{1-\\psi}}{1-\\psi}\\rho\\xi^{\\psi-1} + \\frac{\\tau}{1-\\gamma}\\left(\\left(\\frac{\\zeta}{\\xi} \\right)^{1-\\gamma}-1 \\right) + \\mu_n - \\hat{e} + \\mu_\\xi - \\frac{\\gamma}{2}\\left( \\sigma_n^2 + \\sigma_\\xi^2 - 2\\frac{1-\\gamma}{\\gamma}\\sigma_n\\sigma_\\xi + \\tilde{\\sigma_n}^2 \\right)\\right\\}\\] \\[\\frac{\\rho}{1-\\psi} = \\max \\left\\{\\frac{\\hat{c}^{1-\\psi}}{1-\\psi}\\rho\\zeta^{\\psi-1} + \\mu_w - \\hat{c} + \\mu_\\zeta - \\frac{\\gamma}{2}\\left( \\sigma_w^2 + \\sigma_\\zeta^2 - 2\\frac{1-\\gamma}{\\gamma}\\sigma_w\\sigma_\\zeta \\right) \\right\\}\\]"},{"location":"examples/macrofinance_models/di_tella/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, OptimizerType, SamplingMethod, plot_loss_df, set_seeds\n</code></pre></p> </li> <li> <p>Define LaTex-variable mapping so that we can parse LaTex equations <pre><code>latex_var_mapping = {\n    # variables\n    r\"\\iota\": \"iota\",\n    r\"\\hat{e}\": \"e_hat\",\n    r\"\\hat{c}\": \"c_hat\",\n    r\"\\sigma_{x,1}\": \"sigxtop\",\n    r\"\\sigma_{x,2}\": \"sigxbot\",\n    r\"\\sigma_x\": \"sigx\",\n    r\"\\sigma_p\": \"sigp\",\n    r\"\\sigma_\\xi\": \"sigxi\",\n    r\"\\sigma_\\zeta\": \"sigzeta\",\n    r\"\\tilde{\\sigma_n}\": \"signtilde\",\n    r\"\\sigma_n\": \"sign\",\n    r\"\\pi\": \"signxi\",\n    r\"\\sigma_w\": \"sigw\",\n    r\"\\mu_n\": \"mun\",\n    r\"\\mu_x\": \"mux\",\n    r\"\\mu_p\": \"mup\",\n    r\"\\mu_\\xi\": \"muxi\",\n    r\"\\mu_\\zeta\": \"muzeta\",\n    r\"\\mu_w\": \"muw\",\n\n    # agents\n    r\"\\xi\": \"xi\",\n    r\"\\zeta\": \"zeta\",\n\n    # constants\n    r\"\\bar{\\sigma_v}\": \"sigv_mean\",\n    r\"\\sigma_v\": \"sigv\",\n    r\"\\mu_v\": \"muv\",\n    r\"\\sigma\": \"sigma\",\n    r\"\\lambda\": \"lbd\",\n    r\"\\bar{v}\": \"v_mean\",\n    r\"\\rho\": \"rho\",\n    r\"\\gamma\": \"gamma\",\n    r\"\\psi\": \"psi\",\n    r\"\\tau\": \"tau\",\n    r\"\\delta\": \"delta\",\n    r\"\\phi\": \"phi\",\n}\n</code></pre></p> </li> <li> <p>Define the problem. We train on a fixed grid of \\(50\\times 50\\) points. <pre><code>set_seeds(0)\npde_model = PDEModel(\"ditella\", \n                     {\"batch_size\": 50, \"num_epochs\": 10000, \"loss_log_interval\": 100, \n                      \"optimizer_type\": OptimizerType.Adam, \"sampling_method\": SamplingMethod.FixedGrid}, \n                     latex_var_mapping)\npde_model.set_state([\"x\", \"v\"], {\"x\": [0.05, 0.95], \"v\": [0.05, 0.95]})\npde_model.add_agents([\"xi\", \"zeta\"], \n                     {\"xi\": {\n                         \"positive\": True, \n                        }, \n                      \"zeta\": {\n                          \"positive\": True, \n                          }\n                     })\npde_model.add_endogs([\"p\", \"r\"], \n                     {\"p\": {\n                         \"positive\": True, \n                         },\n                     })\npde_model.add_params({\n    \"a\": 1,\n    \"sigma\": 0.0125,\n    \"lbd\": 1.38,\n    \"v_mean\": 0.25,\n    \"sigv_mean\": -0.17,\n    \"rho\": 0.0665,\n    \"gamma\": 5,\n    \"psi\": 0.5,\n    \"tau\": 1.15,\n    \"phi\": 0.2,\n\n    \"A\": 53.2,\n    \"B\": -0.8668571428571438,\n    \"delta\": 0.05,\n})\npde_model.add_equation(r\"$g &amp;= \\frac{1}{2*A} * (p - B) - \\delta$\") # g &amp;= \\frac{1}{2*A} * (p - B) - \\delta\npde_model.add_equation(r\"$\\iota &amp;= A * (g+\\delta)^2 + B * (g+\\delta)$\") # \\iota &amp;= A * (g+\\delta)^2 + B * (g+\\delta)\npde_model.add_equation(r\"$\\mu_v &amp;= \\lambda * (\\bar{v} - v)$\")\npde_model.add_equation(r\"$\\sigma_v &amp;= \\bar{\\sigma_v} * \\sqrt{v}$\")\npde_model.add_equation(r\"$\\hat{e} &amp;= \\rho^{1/\\psi} * \\xi^{(\\psi-1)/\\psi}$\")\npde_model.add_equation(r\"$\\hat{c} &amp;= \\rho^{1/\\psi} * \\zeta^{(\\psi-1)/\\psi}$\")\npde_model.add_equation(r\"$\\sigma_{x,1} &amp;= (1-x) * x * \\frac{1-\\gamma}{\\gamma} * \\left( \\frac{1}{\\xi} * \\frac{\\partial \\xi}{\\partial v} - \\frac{1}{\\zeta} * \\frac{\\partial \\zeta}{\\partial v} \\right)$\")\npde_model.add_equation(r\"$\\sigma_{x,2} &amp;= 1 - (1-x) * x * \\frac{1-\\gamma}{\\gamma} * \\left( \\frac{1}{\\xi} * \\frac{\\partial \\xi}{\\partial x} - \\frac{1}{\\zeta} * \\frac{\\partial \\zeta}{\\partial x} \\right)$\")\npde_model.add_equation(r\"$\\sigma_x &amp;= \\frac{\\sigma_{x,1}}{\\sigma_{x,2}} * \\sigma_v$\")\npde_model.add_equation(r\"$\\sigma_p &amp;= \\frac{1}{p} * \\left( \\frac{\\partial p}{\\partial v} * \\sigma_v + \\frac{\\partial p}{\\partial x} * \\sigma_x \\right)$\")\npde_model.add_equation(r\"$\\sigma_\\xi &amp;= \\frac{1}{\\xi} * \\left( \\frac{\\partial \\xi}{\\partial v} * \\sigma_v + \\frac{\\partial \\xi}{\\partial x} * \\sigma_x \\right)$\")\npde_model.add_equation(r\"$\\sigma_\\zeta &amp;= \\frac{1}{\\zeta} * \\left( \\frac{\\partial \\zeta}{\\partial v} * \\sigma_v + \\frac{\\partial \\zeta}{\\partial x} * \\sigma_x \\right)$\")\npde_model.add_equation(r\"$\\sigma_n &amp;= \\sigma + \\sigma_p + \\frac{\\sigma_x}{x}$\")\npde_model.add_equation(r\"$\\pi &amp;= \\gamma * \\sigma_n + (\\gamma-1) * \\sigma_\\xi$\")\npde_model.add_equation(r\"$\\sigma_w &amp;= \\frac{\\pi}{\\gamma} - \\frac{\\gamma-1}{\\gamma} *  \\sigma_\\zeta$\")\npde_model.add_equation(r\"$\\mu_w &amp;= r + \\pi * \\sigma_w$\")\npde_model.add_equation(r\"$\\mu_n &amp;= r + \\frac{\\gamma}{x^2} * (\\phi * v)^2 + \\pi * \\sigma_n$\")\npde_model.add_equation(r\"$\\tilde{\\sigma_n} &amp;= \\frac{\\phi}{x} * v$\")\npde_model.add_equation(r\"$\\mu_x &amp;= x * \\left(\\mu_n - \\hat{e} - \\tau + \\frac{a-\\iota}{p} - r - \\pi * (\\sigma+\\sigma_p) - \\frac{\\gamma}{x} * (\\phi * v)^2 + (\\sigma + \\sigma_p)^2 - \\sigma_n * (\\sigma + \\sigma_p)\\right)$\")\npde_model.add_equation(r\"$\\mu_p &amp;= \\frac{1}{p} * \\left( \\mu_v * \\frac{\\partial p}{\\partial v} + \\mu_x * \\frac{\\partial p}{\\partial x} + \\frac{1}{2} * \\left( \\sigma_v^2 * \\frac{\\partial^2 p}{\\partial v^2} + 2 * \\sigma_v * \\sigma_x * \\frac{\\partial^2 p}{\\partial v \\partial x} + \\sigma_x^2 * \\frac{\\partial^2 p}{\\partial x^2} \\right)\\right)$\")\npde_model.add_equation(r\"$\\mu_\\xi &amp;= \\frac{1}{\\xi} * \\left( \\mu_v * \\frac{\\partial \\xi}{\\partial v} + \\mu_x * \\frac{\\partial \\xi}{\\partial x} + \\frac{1}{2} * \\left( \\sigma_v^2 * \\frac{\\partial^2 \\xi}{\\partial v^2} + 2 * \\sigma_v * \\sigma_x * \\frac{\\partial^2 \\xi}{\\partial v \\partial x} + \\sigma_x^2 * \\frac{\\partial^2 \\xi}{\\partial x^2} \\right)\\right)$\")\npde_model.add_equation(r\"$\\mu_\\zeta &amp;= \\frac{1}{\\zeta} * \\left( \\mu_v * \\frac{\\partial \\zeta}{\\partial v} + \\mu_x * \\frac{\\partial \\zeta}{\\partial x} + \\frac{1}{2} * \\left( \\sigma_v^2 * \\frac{\\partial^2 \\zeta}{\\partial v^2} + 2 * \\sigma_v * \\sigma_x * \\frac{\\partial^2 \\zeta}{\\partial v \\partial x} + \\sigma_x^2 * \\frac{\\partial^2 \\zeta}{\\partial x^2} \\right)\\right)$\")\n\npde_model.add_endog_equation(r\"$a - \\iota &amp;= p * (\\hat{e} * x + \\hat{c} * (1-x))$\")\npde_model.add_endog_equation(r\"$\\sigma + \\sigma_p &amp;= \\sigma_n * x + \\sigma_w * (1-x)$\")\npde_model.add_endog_equation(r\"$\\frac{a-\\iota}{p} + g + \\mu_p + \\sigma * \\sigma_p - r &amp;= (\\sigma + \\sigma_p) * \\pi + \\gamma * \\frac{1}{x} * (\\phi * v)^2$\")\n\npde_model.add_hjb_equation(r\"$\\frac{\\hat{e}^{1-\\psi}}{1-\\psi} * \\rho * \\xi^{\\psi-1} + \\frac{\\tau}{1-\\gamma} * \\left(\\left(\\frac{\\zeta}{\\xi} \\right)^{1-\\gamma}-1 \\right) + \\mu_n - \\hat{e} + \\mu_\\xi - \\frac{\\gamma}{2} * \\left( \\sigma_n^2 + \\sigma_\\xi^2 - 2 * \\frac{1-\\gamma}{\\gamma} * \\sigma_n * \\sigma_\\xi + \\tilde{\\sigma_n}^2 \\right) - \\frac{\\rho}{1-\\psi}$\")\npde_model.add_hjb_equation(r\"$\\frac{\\hat{c}^{1-\\psi}}{1-\\psi} * \\rho * \\zeta^{\\psi-1} + \\mu_w - \\hat{c} + \\mu_\\zeta - \\frac{\\gamma}{2} * \\left( \\sigma_w^2 + \\sigma_\\zeta^2 - 2 * \\frac{1-\\gamma}{\\gamma} * \\sigma_w * \\sigma_\\zeta \\right) - \\frac{\\rho}{1-\\psi}$\")\nprint(pde_model)\n</code></pre></p> </li> <li> <p>Train and evaluate the best model with min loss <pre><code>pde_model.train_model(\"./models/ditella_with_investments\", \"model.pt\", True)\npde_model.load_model(torch.load(\"./models/ditella_with_investments/model_best.pt\"))\npde_model.eval_model(True)\n</code></pre></p> </li> <li> <p>Plot the solutions, with additional variables defined by equations. <pre><code>pde_model.plot_vars([r\"$\\xi$\", r\"$\\zeta$\", \"p\", \n                     r\"$\\sigma+\\sigma_p = \\sigma + \\sigma_p$\", r\"$\\pi$\", \"r\"], ncols=3)\n</code></pre></p> </li> <li> <p>Plot variables in 1D. Specifically, we plot \\(p\\) (price of capital), \\(\\sigma_x\\) (volatility of \\(x\\)), \\(\\Omega = \\frac{\\xi}{\\zeta}\\) (relative investment opportunities), \\(\\sigma+\\sigma_p\\) (aggregate risk), \\(\\pi\\) (price of risk), and \\(r\\) (risk-free rate) as functions of \\(v\\) for \\(x=0.05, 0.1, 0.2\\), and as functions of \\(x\\) for \\(v=0.1, 0.25, 0.6\\). <pre><code>fig, ax = plt.subplots(2, 3, figsize=(18, 12))\nfor x_val, linestyle in [(0.05, \"-\"), (0.1, \":\"), (0.2, \"--\")]:\n    sv = torch.ones((100, 2), device=pde_model.device) * x_val\n    sv[:, 1] = torch.linspace(0.05, 0.95, 100)\n    for i, sv_name in enumerate(pde_model.state_variables):\n        pde_model.variable_val_dict[sv_name] = sv[:, i:i+1]\n    pde_model.update_variables(sv)\n    p = pde_model.variable_val_dict[\"p\"]\n    sigx = pde_model.variable_val_dict[\"sigx\"]\n    omega = pde_model.variable_val_dict[\"xi\"] / pde_model.variable_val_dict[\"zeta\"]\n    ax[0][0].plot(sv[:, 1].detach().cpu().numpy(), p.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"x={x_val}\")\n    ax[0][1].plot(sv[:, 1].detach().cpu().numpy(), sigx.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"x={x_val}\")\n    ax[0][2].plot(sv[:, 1].detach().cpu().numpy(), omega.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"x={x_val}\")\n    ax[0][0].set_title(r\"p\")\n    ax[0][1].set_title(r\"$\\sigma_x$\")\n    ax[0][2].set_title(r\"$\\Omega = \\xi/\\zeta$\")\n    ax[0][0].legend()\n    ax[0][1].legend()\n    ax[0][2].legend()\n\nfor v_val, linestyle in [(0.1, \"-\"), (0.25, \":\"), (0.6, \"--\")]:\n    sv = torch.ones((100, 2), device=pde_model.device) * v_val\n    sv[:, 0] = torch.linspace(0.05, 0.95, 100)\n    for i, sv_name in enumerate(pde_model.state_variables):\n        pde_model.variable_val_dict[sv_name] = sv[:, i:i+1]\n    pde_model.update_variables(sv)\n    p = pde_model.variable_val_dict[\"p\"]\n    sigx = pde_model.variable_val_dict[\"sigx\"]\n    omega = pde_model.variable_val_dict[\"xi\"] / pde_model.variable_val_dict[\"zeta\"]\n    ax[1][0].plot(sv[:, 0].detach().cpu().numpy(), p.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"v={v_val}\")\n    ax[1][1].plot(sv[:, 0].detach().cpu().numpy(), sigx.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"v={v_val}\")\n    ax[1][2].plot(sv[:, 0].detach().cpu().numpy(), omega.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"v={v_val}\")\n    ax[1][0].set_title(r\"p\")\n    ax[1][1].set_title(r\"$\\sigma_x$\")\n    ax[1][2].set_title(r\"$\\Omega = \\xi/\\zeta$\")\n    ax[1][0].legend()\n    ax[1][1].legend()\n    ax[1][2].legend()\nplt.subplots_adjust()\nplt.show()\n\nfig, ax = plt.subplots(2, 3, figsize=(18, 12))\nfor x_val, linestyle in [(0.05, \"-\"), (0.1, \":\"), (0.2, \"--\")]:\n    sv = torch.ones((100, 2), device=pde_model.device) * x_val\n    sv[:, 1] = torch.linspace(0.05, 0.95, 100)\n    for i, sv_name in enumerate(pde_model.state_variables):\n        pde_model.variable_val_dict[sv_name] = sv[:, i:i+1]\n    pde_model.update_variables(sv)\n    sigsigp = pde_model.variable_val_dict[\"sigma\"] + pde_model.variable_val_dict[\"sigp\"]\n    pi = pde_model.variable_val_dict[\"signxi\"]\n    r = pde_model.variable_val_dict[\"r\"]\n    ax[0][0].plot(sv[:, 1].detach().cpu().numpy(), sigsigp.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"x={x_val}\")\n    ax[0][1].plot(sv[:, 1].detach().cpu().numpy(), pi.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"x={x_val}\")\n    ax[0][2].plot(sv[:, 1].detach().cpu().numpy(), r.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"x={x_val}\")\n    ax[0][0].set_title(r\"$\\sigma+\\sigma_p$\")\n    ax[0][1].set_title(r\"$\\pi$\")\n    ax[0][2].set_title(\"r\")\n    ax[0][0].legend()\n    ax[0][1].legend()\n    ax[0][2].legend()\nfor v_val, linestyle in [(0.1, \"-\"), (0.25, \":\"), (0.6, \"--\")]:\n    sv = torch.ones((100, 2), device=pde_model.device) * v_val\n    sv[:, 0] = torch.linspace(0.05, 0.95, 100)\n    for i, sv_name in enumerate(pde_model.state_variables):\n        pde_model.variable_val_dict[sv_name] = sv[:, i:i+1]\n    pde_model.update_variables(sv)\n    sigsigp = pde_model.variable_val_dict[\"sigma\"] + pde_model.variable_val_dict[\"sigp\"]\n    pi = pde_model.variable_val_dict[\"signxi\"]\n    r = pde_model.variable_val_dict[\"r\"]\n    ax[1][0].plot(sv[:, 0].detach().cpu().numpy(), sigsigp.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"v={v_val}\")\n    ax[1][1].plot(sv[:, 0].detach().cpu().numpy(), pi.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"v={v_val}\")\n    ax[1][2].plot(sv[:, 0].detach().cpu().numpy(), r.detach().cpu().numpy().reshape(-1), linestyle=linestyle, label=f\"v={v_val}\")\n    ax[1][0].set_title(r\"$\\sigma+\\sigma_p$\")\n    ax[1][1].set_title(r\"$\\pi$\")\n    ax[1][2].set_title(\"r\")\n    ax[1][0].legend()\n    ax[1][1].legend()\n    ax[1][2].legend()\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol> <ol> <li> <p>Sebastian Di Tella, \"Uncertainty Shocks and Balance Sheet Recessions\", Journal of Political Economy, 125(6): 2038-2081, 2017\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/macrofinance_models/log_utility/","title":"Log Utility Problem","text":"<p>The full solution can be found at 1d_problem.ipynb. The split and merge solution can be found at 1d_problem_split.ipynb.</p>"},{"location":"examples/macrofinance_models/log_utility/#problem-setup","title":"Problem Setup","text":"<p>This is Proposition 4 from Brunnermeier and Sannikov 2014<sup>1</sup></p> <p>In the deep neural network, we don't have to fit the initial guess function as in PyMacroFin any more.</p> <p>\\(q\\) should satisfy \\(q(0)=\\frac{\\underline{a} - \\iota(q(0))}{r}\\). We rewrite with \\(\\iota(q) = \\frac{q^2-1}{2\\kappa}\\), and simplify: </p> \\[q(0) r + \\frac{q(0)^2 - 1}{2\\kappa} = \\underline{a}.\\] <p>We rewrite the equations defining \\(\\sigma_t^q\\), and use the following equations and endogenous equations for training the model, with an additional constraint \\(\\psi \\leq 1\\).</p> <p>Equations:</p> \\[\\iota = \\frac{q^2-1}{ 2 \\kappa}\\] \\[\\sigma_t^q = \\frac{\\sigma}{1 - \\frac{1}{q} \\frac{\\partial q}{\\partial \\eta} (\\psi - \\eta)} - \\sigma\\] \\[\\sigma_t^\\eta = \\frac{\\psi - \\eta}{\\eta} (\\sigma + \\sigma_t^q)\\] \\[\\mu_t^\\eta = (\\sigma_t^\\eta)^2 + \\frac{a - \\iota}{q} + (1-\\psi) (\\underline{\\delta} - \\delta) - \\rho\\] <p>We constrain \\(\\psi\\) by \\(\\psi \\leq 1\\)</p> <p>Endogenous equations:</p> \\[(\\sigma + \\sigma_t^q)^2 (\\psi / \\eta - (1-\\psi) / (1-\\eta)) = \\frac{a - \\underline{a}}{q} + \\underline{\\delta} - \\delta\\] \\[(r(1-\\eta) + \\rho \\eta) q = \\psi a + (1-\\psi) \\underline{a} - \\iota\\]"},{"location":"examples/macrofinance_models/log_utility/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, Constraint, SamplingMethod, System, OptimizerType, plot_loss_df, set_seeds\n</code></pre></p> </li> <li> <p>Define latex variable mapping <pre><code>latex_var_mapping = {\n    r\"\\sigma_t^q\": \"sigq\",\n    r\"\\sigma_t^\\theta\": \"sigtheta\",\n    r\"\\sigma_t^\\eta\": \"sige\",\n    r\"\\mu_t^\\eta\": \"mue\",\n    r\"\\mu_t^q\": \"muq\",\n    r\"\\mu_t^\\theta\": \"mutheta\",\n\n\n    r\"\\rho\": \"rho\",\n    r\"\\underline{a}\": \"ah\",\n    r\"\\underline{\\delta}\": \"deltah\",\n    r\"\\delta\": \"deltae\",\n    r\"\\sigma\": \"sig\",\n    r\"\\kappa\": \"kappa\",\n\n    r\"\\eta\": \"e\",\n\n    r\"\\theta\": \"theta\",\n    r\"\\psi\": \"psi\",\n    r\"\\iota\": \"iota\",\n    r\"\\Phi\": \"phi\",\n\n}\n</code></pre></p> </li> <li> <p>Define the problem. We use ReLU as activation function, due to the discontinuity in the first order derivative. Also, we apply a fixed grid sampling method. <pre><code>set_seeds(0)\npde_model = PDEModel(\"BruSan14_log_utility\", {\"sampling_method\": SamplingMethod.FixedGrid, \"batch_size\": 200,\n    \"num_epochs\": 10000, \"loss_log_interval\": 100, \"optimizer_type\": OptimizerType.Adam}, latex_var_mapping=latex_var_mapping)\npde_model.set_state([\"e\"], {\"e\": [0.01, 0.99]})\npde_model.add_endogs([\"q\", \"psi\"], configs={\n    \"q\": {\n        \"positive\": True,\n        \"hidden_units\": [50, 50], \n        \"activation_type\": ActivationType.ReLU,\n    },\n    \"psi\": {\n        \"positive\": True, \n        \"hidden_units\": [50, 50], \n        \"activation_type\": ActivationType.ReLU,\n    }\n})\npde_model.add_params({\n    \"sig\": .1,\n    \"deltae\": .05,\n    \"deltah\": .05,\n    \"rho\": .06,\n    \"r\": .05,\n    \"a\": .11,\n    \"ah\": .07,\n    \"kappa\": 2,\n})\npde_model.add_endog_condition(\"q\", \n                              \"q(SV)*r + (q(SV) * q(SV) - 1) / (2*kappa) - ah\", {\"SV\": torch.zeros((1, 1)), \n                                                                                 \"r\": 0.05, \"ah\": .07, \"kappa\": 2,},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"q_min\")\npde_model.add_equation(r\"$\\iota = \\frac{q^2-1}{ 2 * \\kappa}$\")\npde_model.add_equation(r\"$\\sigma_t^q = \\frac{\\sigma}{1 - \\frac{1}{q} * \\frac{\\partial q}{\\partial \\eta} * (\\psi - \\eta)} - \\sigma$\")\npde_model.add_equation(r\"$\\sigma_t^\\eta = \\frac{\\psi - \\eta}{\\eta} * (\\sigma + \\sigma_t^q)$\")\npde_model.add_equation(r\"$\\mu_t^\\eta = (\\sigma_t^\\eta)^2 + \\frac{a - \\iota}{q} + (1-\\psi) * (\\underline{\\delta} - \\delta) - \\rho$\")\n\npde_model.add_constraint(\"psi\", Comparator.LEQ, \"1\")\npde_model.add_endog_equation(r\"$(\\sigma + \\sigma_t^q) ^2 * (\\psi / \\eta - (1-\\psi) / (1-\\eta)) = \\frac{a - \\underline{a}}{q} + \\underline{\\delta} - \\delta$\")\npde_model.add_endog_equation(r\"$(r*(1-\\eta) + \\rho * \\eta) * q = \\psi * a + (1-\\psi) * \\underline{a} - \\iota$\")\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>pde_model.load_model(torch.load(\"./models/BruSan14_log_utility/model_init_best.pt\"))\npde_model.train_model(\"./models/BruSan14_log_utility\", \"model.pt\", True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>pde_model.load_model(torch.load(\"./models/BruSan14_log_utility/model_init_best.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>pde_model.plot_vars([\"q\", \"psi\",\n                    r\"$\\sigma_t^q$\",\n                     r\"$\\eta\\sigma^\\eta = \\eta*\\sigma_t^\\eta$\",\n                     r\"$\\eta\\mu^\\eta = \\eta*\\mu_t^\\eta$\",\n                     \"er = psi/e*(sig+sigq)**2\"], ncols=3)\n</code></pre></p> </li> </ol>"},{"location":"examples/macrofinance_models/log_utility/#implementation-split-and-merge","title":"Implementation (Split and Merge)","text":"<p>Instead of solving the problem on both regions as a whole, we can separately solve two problems and merge the solutions.</p> <p>In the first region \\(\\psi &lt; 1\\), we need to solve for both \\(q\\) and \\(\\psi\\). The boundary conditions are</p> \\[\\begin{cases} q(0)=-\\kappa r + \\sqrt{\\kappa^2 r^2+1+2 \\underline{a} \\kappa}\\\\ q(1)=-\\kappa \\rho + \\sqrt{\\kappa^2 \\rho^2+1+2 a \\kappa}\\\\ \\psi(0)=0\\\\ \\psi(1)=1 \\end{cases}\\] <p>The equations to solve for equilibrium are</p> \\[ \\begin{cases} (r(1-\\eta) + \\rho \\eta) q = \\psi a + (1-\\psi) \\underline{a} - \\iota\\\\ (\\sigma + \\sigma_t^q) ^2 \\frac{q (\\psi - \\eta)}{\\eta (1-\\eta)} = (a - \\underline{a}) + (\\underline{\\delta} - \\delta) q \\end{cases} \\] <p>In the second region \\(\\psi = 1\\), we only need to solve for \\(q\\) with a single equation:</p> \\[(r (1-\\eta) + \\rho \\eta) q = a - \\iota\\] <ol> <li> <p>Solve for region 1. <pre><code>set_seeds(0)\npde_model = PDEModel(\"BruSan14_log_utility\", {\"sampling_method\": SamplingMethod.FixedGrid, \"batch_size\": 1000,\n    \"num_epochs\": 20000, \"loss_log_interval\": 100, \"optimizer_type\": OptimizerType.Adam}, latex_var_mapping=latex_var_mapping)\npde_model.set_state([\"e\"], {\"e\": [0.001, 0.999]})\npde_model.add_endogs([\"q\", \"psi\"], configs={\n    \"q\": {\n        \"positive\": True,\n        \"activation_type\": ActivationType.SiLU,\n    },\n    \"psi\": {\n        \"positive\": True, \n        \"activation_type\": ActivationType.SiLU,\n    }\n})\npde_model.add_params({\n    \"sig\": .1,\n    \"deltae\": .05,\n    \"deltah\": .05,\n    \"rho\": .06,\n    \"r\": .05,\n    \"a\": .11,\n    \"ah\": .07,\n    \"kappa\": 2,\n})\npde_model.add_endog_condition(\"q\", \n                              \"q(SV)\", \n                              {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"-kappa*r + (kappa**2*r**2 + 1 + 2*ah*kappa)**0.5\", {\"r\": 0.05, \"ah\": .07, \"kappa\": 2},\n                              label=\"q_min\", weight=100)\npde_model.add_endog_condition(\"q\", \n                              \"q(SV)\", \n                              {\"SV\": torch.ones((1, 1))},\n                              Comparator.EQ,\n                              \"-kappa*rho + (kappa**2*rho**2 + 1 + 2*a*kappa)**0.5\", {\"rho\": 0.06, \"a\": .11, \"kappa\": 2},\n                              label=\"q_max\", weight=100)\npde_model.add_endog_condition(\"psi\", \n                              \"psi(SV)\", \n                              {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"psi_min\", weight=100)\npde_model.add_endog_condition(\"psi\", \n                              \"psi(SV)\", \n                              {\"SV\": torch.ones((1, 1))},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"psi_max\", weight=100)\n\npde_model.add_equation(r\"$\\iota = \\frac{q^2-1}{ 2 * \\kappa}$\")\npde_model.add_equation(r\"$\\sigma_t^q = \\frac{\\sigma}{1 - \\frac{1}{q} * \\frac{\\partial q}{\\partial \\eta} * (\\psi - \\eta)} - \\sigma$\")\npde_model.add_equation(r\"$\\sigma_t^\\eta = \\frac{\\psi - \\eta}{\\eta} * (\\sigma + \\sigma_t^q)$\")\npde_model.add_equation(r\"$\\mu_t^\\eta = (\\sigma_t^\\eta)^2 + \\frac{a - \\iota}{q} + (1-\\psi) * (\\underline{\\delta} - \\delta) - \\rho$\")\n\npde_model.add_endog_equation(r\"$(r*(1-\\eta) + \\rho * \\eta) * q = \\psi * a + (1-\\psi) * \\underline{a} - \\iota$\", loss_reduction=LossReductionMethod.SSE)\npde_model.add_endog_equation(r\"$(\\sigma + \\sigma_t^q) ^2 * \\frac{q * (\\psi - \\eta)}{\\eta * (1-\\eta)} = (a - \\underline{a}) + (\\underline{\\delta} - \\delta) * q$\", loss_reduction=LossReductionMethod.SSE, weight=2)\npde_model.train_model(\"./models/BruSan14_log_utility_relu_split\", \"region1.pt\", True)\npde_model.load_model(torch.load(\"./models/BruSan14_log_utility_relu_split/region1_best.pt\"))\npde_model.eval_model(True)\n</code></pre></p> </li> <li> <p>Compute and plot the results specifically for region 1. We can simply use <code>update_variables</code> function in <code>PDEModel</code> class to update all variables of interest. <pre><code>N = 1000\nSV = torch.linspace(0, 1, N, device=pde_model.device).reshape(-1, 1)\nx_plot = SV.detach().cpu().numpy().reshape(-1)\nfor i, sv_name in enumerate(pde_model.state_variables):\n    pde_model.variable_val_dict[sv_name] = SV[:, i:i+1]\npde_model.update_variables(SV)\nq_region1 = pde_model.variable_val_dict[\"q\"].detach().cpu().numpy().reshape(-1)\npsi_region1 = pde_model.variable_val_dict[\"psi\"].detach().cpu().numpy().reshape(-1)\nsigq_region1 = pde_model.variable_val_dict[\"sigq\"].detach().cpu().numpy().reshape(-1)\nesige_region1 = (SV * pde_model.variable_val_dict[\"sige\"]).detach().cpu().numpy().reshape(-1)\nemue_region1 = (SV * pde_model.variable_val_dict[\"mue\"]).detach().cpu().numpy().reshape(-1)\ner_region1 = (pde_model.variable_val_dict[\"psi\"] / (SV * (pde_model.variable_val_dict[\"sig\"] + pde_model.variable_val_dict[\"sigq\"])**2)).detach().cpu().numpy().reshape(-1)\n\nxlabel = \"$\\eta$\"\nplot_args = [\n    {\"y\": q_region1, \"ylabel\": r\"$q$\", \"title\": r\"$q$ vs. $\\eta$\"},\n    {\"y\": psi_region1, \"ylabel\": r\"$\\psi$\", \"title\": r\"$\\psi$ vs. $\\eta$\"},\n    {\"y\": sigq_region1, \"ylabel\": r\"$\\sigma^q$\", \"title\": r\"$\\sigma^q$ vs. $\\eta$\"},\n    {\"y\": esige_region1, \"ylabel\": r\"$\\eta\\sigma^{\\eta}$\", \"title\": r\"$\\eta\\sigma^{\\eta}$ vs. $\\eta$\"},\n    {\"y\": emue_region1, \"ylabel\": r\"$\\eta\\mu^{\\eta}$\", \"title\": r\"$\\eta\\mu^{\\eta}$ vs. $\\eta$\"},\n    {\"y\": er_region1, \"ylabel\": r\"$E[dr_t^k-dr_t]/dt$\", \"title\": r\"$E[dr_t^k-dr_t]/dt$ vs. $\\eta$\"},\n]\n\nfig, ax = plt.subplots(2, 3, figsize=(18, 12))\nfor i, plot_arg in enumerate(plot_args):\n    row = i // 3\n    col = i % 3\n    curr_ax = ax[row, col]\n    curr_ax.plot(x_plot, plot_arg[\"y\"])\n    curr_ax.set_xlabel(xlabel)\n    curr_ax.set_ylabel(plot_arg[\"ylabel\"])\n    curr_ax.set_title(plot_arg[\"title\"])\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Solve for region 2. <pre><code>set_seeds(0)\npde_model = PDEModel(\"BruSan14_log_utility\", {\"sampling_method\": SamplingMethod.FixedGrid, \"batch_size\": 1000,\n    \"num_epochs\": 2000, \"loss_log_interval\": 100, \"optimizer_type\": OptimizerType.Adam}, latex_var_mapping=latex_var_mapping)\npde_model.set_state([\"e\"], {\"e\": [0.001, 0.999]})\npde_model.add_endogs([\"q\"], configs={\n    \"q\": {\n        \"positive\": True,\n        \"activation_type\": ActivationType.SiLU,\n    }\n})\npde_model.add_params({\n    \"sig\": .1,\n    \"deltae\": .05,\n    \"deltah\": .05,\n    \"rho\": .06,\n    \"r\": .05,\n    \"a\": .11,\n    \"ah\": .07,\n    \"kappa\": 2,\n})\n\npde_model.add_equation(r\"$\\iota = \\frac{q^2-1}{ 2 * \\kappa}$\")\npde_model.add_equation(r\"$\\sigma_t^q = \\frac{\\sigma}{1 - \\frac{1}{q} * \\frac{\\partial q}{\\partial \\eta} * (1 - \\eta)} - \\sigma$\")\npde_model.add_equation(r\"$\\sigma_t^\\eta = \\frac{1 - \\eta}{\\eta} * (\\sigma + \\sigma_t^q)$\")\npde_model.add_equation(r\"$\\mu_t^\\eta = (\\sigma_t^\\eta)^2 + \\frac{a - \\iota}{q} - \\rho$\")\n\npde_model.add_endog_equation(r\"$(r*(1-\\eta) + \\rho * \\eta) * q = a - \\iota$\", loss_reduction=LossReductionMethod.SSE)\n\npde_model.train_model(\"./models/BruSan14_log_utility_relu_split\", \"region2.pt\", True)\npde_model.load_model(torch.load(\"./models/BruSan14_log_utility_relu_split/region2_best.pt\"))\npde_model.eval_model(True)\n</code></pre></p> </li> <li> <p>Compute and plot the results specifically for region 2. <pre><code>N = 1000\nSV = torch.linspace(0, 1, N, device=pde_model.device).reshape(-1, 1)\nx_plot = SV.detach().cpu().numpy().reshape(-1)\nfor i, sv_name in enumerate(pde_model.state_variables):\n    pde_model.variable_val_dict[sv_name] = SV[:, i:i+1]\npde_model.update_variables(SV)\nq_region2 = pde_model.variable_val_dict[\"q\"].detach().cpu().numpy().reshape(-1)\nsigq_region2 = pde_model.variable_val_dict[\"sigq\"].detach().cpu().numpy().reshape(-1)\nesige_region2 = (SV * pde_model.variable_val_dict[\"sige\"]).detach().cpu().numpy().reshape(-1)\nemue_region2 = (SV * pde_model.variable_val_dict[\"mue\"]).detach().cpu().numpy().reshape(-1)\ner_region2 = (1 / (SV * (pde_model.variable_val_dict[\"sig\"] + pde_model.variable_val_dict[\"sigq\"])**2)).detach().cpu().numpy().reshape(-1)\n\nxlabel = \"$\\eta$\"\nplot_args = [\n    {\"y\": q_region2, \"ylabel\": r\"$q$\", \"title\": r\"$q$ vs. $\\eta$\"},\n    {\"y\": np.ones_like(x_plot), \"ylabel\": r\"$\\psi$\", \"title\": r\"$\\psi$ vs. $\\eta$\"},\n    {\"y\": sigq_region2, \"ylabel\": r\"$\\sigma^q$\", \"title\": r\"$\\sigma^q$ vs. $\\eta$\"},\n    {\"y\": esige_region2, \"ylabel\": r\"$\\eta\\sigma^{\\eta}$\", \"title\": r\"$\\eta\\sigma^{\\eta}$ vs. $\\eta$\"},\n    {\"y\": emue_region2, \"ylabel\": r\"$\\eta\\mu^{\\eta}$\", \"title\": r\"$\\eta\\mu^{\\eta}$ vs. $\\eta$\"},\n    {\"y\": er_region2, \"ylabel\": r\"$E[dr_t^k-dr_t]/dt$\", \"title\": r\"$E[dr_t^k-dr_t]/dt$ vs. $\\eta$\"},\n]\n\nfig, ax = plt.subplots(2, 3, figsize=(18, 12))\nfor i, plot_arg in enumerate(plot_args):\n    row = i // 3\n    col = i % 3\n    curr_ax = ax[row, col]\n    curr_ax.plot(x_plot, plot_arg[\"y\"])\n    curr_ax.set_xlabel(xlabel)\n    curr_ax.set_ylabel(plot_arg[\"ylabel\"])\n    curr_ax.set_title(plot_arg[\"title\"])\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Merge the results using \\(\\psi\\) as the final solution. <pre><code>index_unconstrain = (psi_region1 &lt; 1)\nindex_constrain = (psi_region1 &gt;= 1)\n\nq_nn = q_region1 * index_unconstrain + q_region2 * index_constrain\npsi_nn = psi_region1 * index_unconstrain + 1 * index_constrain\nsigq_nn = sigq_region1 * index_unconstrain + sigq_region2 * index_constrain\nesig_nn = esige_region1 * index_unconstrain + esige_region2 * index_constrain\nemue_nn = emue_region1 * index_unconstrain + emue_region2 * index_constrain\ner_nn = er_region1 * index_unconstrain + er_region2 * index_constrain\n\nxlabel = \"$\\eta$\"\nplot_args = [\n    {\"y\": q_nn, \"ylabel\": r\"$q$\", \"title\": r\"$q$ vs. $\\eta$\"},\n    {\"y\": psi_nn, \"ylabel\": r\"$\\psi$\", \"title\": r\"$\\psi$ vs. $\\eta$\"},\n    {\"y\": sigq_nn, \"ylabel\": r\"$\\sigma^q$\", \"title\": r\"$\\sigma^q$ vs. $\\eta$\"},\n    {\"y\": esig_nn, \"ylabel\": r\"$\\eta\\sigma^{\\eta}$\", \"title\": r\"$\\eta\\sigma^{\\eta}$ vs. $\\eta$\"},\n    {\"y\": emue_nn, \"ylabel\": r\"$\\eta\\mu^{\\eta}$\", \"title\": r\"$\\eta\\mu^{\\eta}$ vs. $\\eta$\"},\n    {\"y\": er_nn, \"ylabel\": r\"$E[dr_t^k-dr_t]/dt$\", \"title\": r\"$E[dr_t^k-dr_t]/dt$ vs. $\\eta$\"},\n]\n\nfig, ax = plt.subplots(2, 3, figsize=(18, 12))\nfor i, plot_arg in enumerate(plot_args):\n    row = i // 3\n    col = i % 3\n    curr_ax = ax[row, col]\n    curr_ax.plot(x_plot, plot_arg[\"y\"])\n    curr_ax.set_xlabel(xlabel)\n    curr_ax.set_ylabel(plot_arg[\"ylabel\"])\n    curr_ax.set_title(plot_arg[\"title\"])\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> </ol> <ol> <li> <p>Brunnermeier, Markus K. and Sannikov, Yuliy, \"A Macroeconomic Model with a Financial Sector\", SIAM Review, 104(2): 379\u2013421, 2014\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/macrofinance_models/pymacrofin_2d_problem/","title":"PyMacroFin 2D Problem","text":"<p>The full solution can be found at 2d_problem.ipynb and 2d_problem (time stepping).ipynb.</p>"},{"location":"examples/macrofinance_models/pymacrofin_2d_problem/#problem-setup","title":"Problem Setup","text":"<p>This is an extension of Brunnermeier and Sannikov 2014<sup>1</sup> with two agents and time-varying aggregate volatility. The PyMacroFin setup can be found here.</p>"},{"location":"examples/macrofinance_models/pymacrofin_2d_problem/#implementation-loss-balancing","title":"Implementation (Loss Balancing)","text":"<p>With default training scheme, the loss does not converge to a small enough number, so we apply Relative Loss Balancing with Random Lookback (ReLoBRaLo) algorithm. The implementation can be found in 2d_problem.ipynb.</p> <ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel, plot_loss_df\nfrom deep_macrofin import ActivationType, Comparator, Constraint, System, set_seeds, OptimizerType, SamplingMethod\n</code></pre></p> </li> <li> <p>Define the problem, with loss balancing, \\(\\mathbb{E}(\\rho)=0.999\\), \\(\\mathcal{T}=0.1\\), \\(\\alpha=0.999\\). <pre><code>set_seeds(0)\npde_model = PDEModel(\"BruSan\", {\"batch_size\": 50, \"num_epochs\": 20000, \"lr\": 1e-3, \"optimizer_type\": OptimizerType.Adam, \"sampling_method\": SamplingMethod.FixedGrid,\n                                \"loss_balancing\": True, \"bernoulli_prob\": 0.9999, \"loss_balancing_temp\": 0.1, \"loss_balancing_alpha\": 0.999})\npde_model.set_state([\"e\", \"z\"], {\"e\": [0.05, 0.95], \"z\": [0.05, 0.95]})\npde_model.add_endogs([\"q\", \"psi\", \"mue\", \"sigqk\", \"sigqs\"], configs={\n    \"q\": {\"positive\": True},\n    \"psi\": {\"positive\": True},\n})\npde_model.add_agents([\"vi\", \"vh\"], configs={\n    \"vi\": {\"positive\": True},\n    \"vh\": {\"positive\": True},\n})\npde_model.add_params({\n    \"gammai\": 2,\n    \"gammah\": 3,\n    \"ai\": .1,\n    \"ah\": .1,\n    \"rhoi\": .04,\n    \"rhoh\": .04,\n    \"sigz\": .01,\n    \"sigbar\": .5,\n    \"deltai\": .04,\n    \"deltah\": .04,\n    \"kappa_p\": 2,\n    \"kappa_z\": 5,\n    \"zetai\": 1.15,\n    \"zetah\": 1.15,\n    \"kappa_l\": .9,\n    \"ebar\": .5,\n})\n\npde_model.add_equation(\"sigma = z\")\npde_model.add_equation(\"wi = psi/e\")\npde_model.add_equation(\"wh = (1-psi)/(1-e)\")\npde_model.add_equation(\"ci = vi**((1-zetai)/(1-gammai))\")\npde_model.add_equation(\"ch = vh**((1-zetah)/(1-gammah))\")\npde_model.add_equation(\"iotai = (q-1)/kappa_p\")\npde_model.add_equation(\"iotah = (q-1)/kappa_p\")\npde_model.add_equation(\"phii = log(1+kappa_p*iotai)/kappa_p-deltai\")\npde_model.add_equation(\"phih = log(1+kappa_p*iotah)/kappa_p-deltah\")\npde_model.add_equation(\"muz = kappa_z*(sigbar-sigma)\")\npde_model.add_equation(\"muk = psi*phii+(1-psi)*phih\")\npde_model.add_equation(\"signis = wi*sigqs\")\npde_model.add_equation(\"signhs = wh*sigqs\")\npde_model.add_equation(\"signik = wi*(sigqk+sigma)\")\npde_model.add_equation(\"signhk = wh*(sigqk+sigma)\")\npde_model.add_equation(\"siges = e*(1-e)*(signis -sigqs)\")\npde_model.add_equation(\"sigek = e*(1-e)*(signik - (sigqk+sigma))\")\npde_model.add_equation(\"sigxik = vi_e/vi*sigek*e\")\npde_model.add_equation(\"sigxhk = vh_e/vh*sigek*e\")\npde_model.add_equation(\"sigxis = vi_e/vi*siges*e + vi_z/vi*sigz*z\")\npde_model.add_equation(\"sigxhs = vh_e/vh*siges*e + vh_z/vh*sigz*z\")\npde_model.add_equation(\"muee=mue*e\")\npde_model.add_equation(\"muzz=muz*z\")\npde_model.add_equation(\"sigee=(siges*e)**2+(sigek*e)**2\")\npde_model.add_equation(\"sigzz=(sigz*z)**2\")\npde_model.add_equation(\"sigcross=siges*e*sigz*z\")\npde_model.add_equation(\"muq = q_e/q*muee + q_z/q*muzz + 1/2*q_ee/q*sigee + 1/2*q_zz/q*sigzz + q_ez/q*sigcross\")\npde_model.add_equation(\"muri = (ai-iotai)/q + phii + muq + sigma*sigqk\")\npde_model.add_equation(\"murh = (ah-iotah)/q + phih + muq + sigma*sigqk\")\npde_model.add_equation(\"r = muri - gammai*wi*((sigqs**2)+(sigma+sigqk)**2) + sigqs*sigxis + (sigqk+sigma)*sigxik\")\npde_model.add_equation(\"muni = r + wi*(muri-r)-ci\")\npde_model.add_equation(\"munh = r + wh*(murh-r)-ch\")\npde_model.add_equation(\"muxi = vi_e/vi*muee + vi_z/vi*muzz + 1/2*vi_ee/vi*sigee + 1/2*vi_zz/vi*sigzz + vi_ez/vi*sigcross\")\npde_model.add_equation(\"muxh = vh_e/vh*muee + vh_z/vh*muzz + 1/2*vh_ee/vh*sigee + 1/2*vh_zz/vh*sigzz + vh_ez/vh*sigcross\")\n\npde_model.add_endog_equation(\"kappa_l/e*(ebar-e)+(1-e)*(muni - muk - muq - sigma*sigqk + (sigqk+sigma)**2 + sigqs**2 - wi*sigqs**2 - wi*(sigqk+sigma)**2) - mue=0\")\npde_model.add_endog_equation(\"(ci*e+ch*(1-e))*q - psi*(ai-iotai) - (1-psi)*(ah-iotah)=0\")\npde_model.add_endog_equation(\"muri - murh + gammah*wh*((sigqs**2)+(sigqk+sigma)**2) - gammai*wi*((sigqs)**2+(sigqk+sigma)**2) + sigqs*sigxis + (sigqk+sigma)*sigxik - sigqs*sigxhs - (sigqk+sigma)*sigxhk=0\")\npde_model.add_endog_equation(\"(sigz*z*q_z + siges*e*q_e)-sigqs*q=0\")\npde_model.add_endog_equation(\"sigek*e*q_e - sigqk*q=0\")\n\npde_model.add_hjb_equation(\"1/(1-1/zetai)*(ci-(rhoi+kappa_l)) + r - ci + gammai/2*(wi * sigqs)**2 + gammai/2*(wi*sigma+wi*sigqk)**2 + muxi / (1-gammai)\")\npde_model.add_hjb_equation(\"1/(1-1/zetah)*(ch-(rhoh+kappa_l)) + r - ch + gammah/2*(wh * sigqs)**2 + gammah/2*(wh*sigma+wh*sigqk)**2 + muxh / (1-gammah)\")\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>pde_model.train_model(\"./models/2D_fixed_grid_loss_balance\", \"model.pt\", True)\npde_model.load_model(torch.load(\"./models/2D_fixed_grid_loss_balance/model_best.pt\"))\npde_model.eval_model(True)\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>pde_model.plot_vars([\"q\", r\"$\\psi=psi$\", r\"$\\mu^{\\eta}=mue$\", r\"$\\sigma^{q,k}=sigqk$\",\n                     r\"$\\sigma^{q,\\sigma}=sigqs$\", r\"$\\xi^i=vi$\", r\"$\\xi^h=vh$\"], ncols=4)\n</code></pre></p> </li> </ol>"},{"location":"examples/macrofinance_models/pymacrofin_2d_problem/#implementation-time-stepping","title":"Implementation (Time Stepping)","text":"<p>Here we show the time stepping scheme that is similar to what PyMacroFin implements, but in a neural network approach. The implementation can be found in 2d_problem (time stepping).ipynb.</p> <ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModelTimeStep, plot_loss_df\nfrom deep_macrofin import ActivationType, Comparator, Constraint, System, set_seeds, OptimizerType, SamplingMethod\n</code></pre></p> </li> <li> <p>Define the problem. The HJB equations now involve a false transient term in time. <pre><code>set_seeds(0)\npde_model = PDEModelTimeStep(\"BruSan\", {\"batch_size\": 20,\n    \"num_outer_iterations\": 50,\n    \"num_inner_iterations\": 5000,\n    \"lr\": 1e-3,\n    \"loss_log_interval\": 100,\n    \"optimizer_type\": OptimizerType.Adam,\n    \"min_t\": 0.0,\n    \"max_t\": 1.0,\n    \"outer_loop_convergence_thres\": 1e-4})\npde_model.set_state([\"e\", \"z\"], {\"e\": [0.05, 0.95], \"z\": [0.05, 0.95]})\npde_model.add_endogs([\"q\", \"psi\", \"mue\", \"sigqk\", \"sigqs\"], configs={\n    \"q\": {\"positive\": True},\n    \"psi\": {\"positive\": True},\n})\npde_model.add_agents([\"vi\", \"vh\"], configs={\n    \"vi\": {\"positive\": True},\n    \"vh\": {\"positive\": True},\n})\npde_model.add_params({\n    \"gammai\": 2,\n    \"gammah\": 3,\n    \"ai\": .1,\n    \"ah\": .1,\n    \"rhoi\": .04,\n    \"rhoh\": .04,\n    \"sigz\": .01,\n    \"sigbar\": .5,\n    \"deltai\": .04,\n    \"deltah\": .04,\n    \"kappa_p\": 2,\n    \"kappa_z\": 5,\n    \"zetai\": 1.15,\n    \"zetah\": 1.15,\n    \"kappa_l\": .9,\n    \"ebar\": .5,\n})\n\npde_model.add_equation(\"sigma = z\")\npde_model.add_equation(\"wi = psi/e\")\npde_model.add_equation(\"wh = (1-psi)/(1-e)\")\npde_model.add_equation(\"ci = vi**((1-zetai)/(1-gammai))\")\npde_model.add_equation(\"ch = vh**((1-zetah)/(1-gammah))\")\npde_model.add_equation(\"iotai = (q-1)/kappa_p\")\npde_model.add_equation(\"iotah = (q-1)/kappa_p\")\npde_model.add_equation(\"phii = log(1+kappa_p*iotai)/kappa_p-deltai\")\npde_model.add_equation(\"phih = log(1+kappa_p*iotah)/kappa_p-deltah\")\npde_model.add_equation(\"muz = kappa_z*(sigbar-sigma)\")\npde_model.add_equation(\"muk = psi*phii+(1-psi)*phih\")\npde_model.add_equation(\"signis = wi*sigqs\")\npde_model.add_equation(\"signhs = wh*sigqs\")\npde_model.add_equation(\"signik = wi*(sigqk+sigma)\")\npde_model.add_equation(\"signhk = wh*(sigqk+sigma)\")\npde_model.add_equation(\"siges = e*(1-e)*(signis -sigqs)\")\npde_model.add_equation(\"sigek = e*(1-e)*(signik - (sigqk+sigma))\")\npde_model.add_equation(\"sigxik = vi_e/vi*sigek*e\")\npde_model.add_equation(\"sigxhk = vh_e/vh*sigek*e\")\npde_model.add_equation(\"sigxis = vi_e/vi*siges*e + vi_z/vi*sigz*z\")\npde_model.add_equation(\"sigxhs = vh_e/vh*siges*e + vh_z/vh*sigz*z\")\npde_model.add_equation(\"muee=mue*e\")\npde_model.add_equation(\"muzz=muz*z\")\npde_model.add_equation(\"sigee=(siges*e)**2+(sigek*e)**2\")\npde_model.add_equation(\"sigzz=(sigz*z)**2\")\npde_model.add_equation(\"sigcross=siges*e*sigz*z\")\npde_model.add_equation(\"muq = q_e/q*muee + q_z/q*muzz + 1/2*q_ee/q*sigee + 1/2*q_zz/q*sigzz + q_ez/q*sigcross\")\npde_model.add_equation(\"muri = (ai-iotai)/q + phii + muq + sigma*sigqk\")\npde_model.add_equation(\"murh = (ah-iotah)/q + phih + muq + sigma*sigqk\")\npde_model.add_equation(\"r = muri - gammai*wi*((sigqs**2)+(sigma+sigqk)**2) + sigqs*sigxis + (sigqk+sigma)*sigxik\")\npde_model.add_equation(\"muni = r + wi*(muri-r)-ci\")\npde_model.add_equation(\"munh = r + wh*(murh-r)-ch\")\npde_model.add_equation(\"rvi=-1*(1-gammai)*(1/(1-1/zetai)*(ci-(rhoi+kappa_l))+r-ci+gammai/2*(wi*(sigqs)**2 +wi*(sigqk+sigma)**2))\")\npde_model.add_equation(\"rvh=-1*(1-gammah)*(1/(1-1/zetah)*(ch-(rhoh+kappa_l))+r-ch+gammah/2*(wh*(sigqs)**2 +wh*(sigqk+sigma)**2))\")\n\npde_model.add_endog_equation(\"kappa_l/e*(ebar-e)+(1-e)*(muni - muk - muq - sigma*sigqk + (sigqk+sigma)**2 + sigqs**2 - wi*sigqs**2 - wi*(sigqk+sigma)**2) - mue=0\")\npde_model.add_endog_equation(\"(ci*e+ch*(1-e))*q - psi*(ai-iotai) - (1-psi)*(ah-iotah)=0\")\npde_model.add_endog_equation(\"muri - murh + gammah*wh*((sigqs**2)+(sigqk+sigma)**2) - gammai*wi*((sigqs)**2+(sigqk+sigma)**2) + sigqs*sigxis + (sigqk+sigma)*sigxik - sigqs*sigxhs - (sigqk+sigma)*sigxhk=0\")\npde_model.add_endog_equation(\"(sigz*z*q_z + siges*e*q_e)-sigqs*q=0\")\npde_model.add_endog_equation(\"sigek*e*q_e - sigqk*q=0\")\n\npde_model.add_hjb_equation(\"muee*vi_e+muzz*vi_z+1/2*(sigee)**2*vi_ee+1/2*(sigzz)**2*vi_zz+sigcross*vi_ez+vi_t-rvi*vi\")\npde_model.add_hjb_equation(\"muee*vh_e+muzz*vh_z+1/2*(sigee)**2*vh_ee+1/2*(sigzz)**2*vh_zz+sigcross*vh_ez+vh_t-rvh*vh\")\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>pde_model.train_model(\"./models/2D_timestep\", \"model.pt\", True)\npde_model.load_model(torch.load(\"./models/2D_timestep/model_best.pt\"))\npde_model.eval_model(True)\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>pde_model.plot_vars([\"q\", r\"$\\psi=psi$\", r\"$\\mu^{\\eta}=mue$\", r\"$\\sigma^{q,k}=sigqk$\",\n                     r\"$\\sigma^{q,\\sigma}=sigqs$\", r\"$\\xi^i=vi$\", r\"$\\xi^h=vh$\"], ncols=4)\n</code></pre></p> </li> </ol> <ol> <li> <p>Brunnermeier, Markus K. and Sannikov, Yuliy, \"A Macroeconomic Model with a Financial Sector\", SIAM Review, 104(2): 379\u2013421, 2014\u00a0\u21a9</p> </li> </ol>"},{"location":"examples/odes/basic_ode1/","title":"Base ODE 1","text":"<p>The full solution can be found at basic_odes.ipynb.</p>"},{"location":"examples/odes/basic_ode1/#problem-setup","title":"Problem Setup","text":"\\[\\frac{dx}{dt} = 2t, x(0)=1, t\\in[-2,2]\\] <p>The solution is \\(x(t)=t^2+1\\)</p>"},{"location":"examples/odes/basic_ode1/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages  <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we use the default training configuration, and default setup for learnable endogenous variable. <pre><code>ode1 = PDEModel(\"ode1\") # define PDE model to solve\node1.set_state([\"t\"], {\"t\": [-2., 2.]}) # set the state variable, which defines the dimensionality of the problem\node1.add_endog(\"x\") # we use endogenous variable to represent the function we want to approximate\node1.add_endog_equation(r\"$\\frac{\\partial x}{\\partial t} = 2 * t$\", label=\"base_ode\") # endogenous equations are used to represent the ODE\node1.add_endog_condition(\"x\", \n                              \"x(SV)\", {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"initial_condition\") # define initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>ode1.train_model(\"./models/ode1\", \"ode1.pt\", True)\node1.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>ode1.load_model(torch.load(\"./models/ode1/ode1.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 3, figsize=(16, 5))\nt = np.linspace(-2, 2)\nax[0].plot(t, t**2+1, label=\"t^2+1\")\nax[1].plot(t, 2*t, label=\"2*t\")\nax[2].plot(t, np.ones_like(t) * 2, label=\"2\")\node1.endog_vars[\"x\"].plot(\"x\", {\"t\": [-2, 2]}, ax=ax[0])\node1.endog_vars[\"x\"].plot(\"x_t\", {\"t\": [-2, 2]}, ax=ax[1])\node1.endog_vars[\"x\"].plot(\"x_tt\", {\"t\": [-2, 2]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/odes/basic_ode2/","title":"Base ODE 2","text":"<p>The full solution can be found at basic_odes.ipynb.</p>"},{"location":"examples/odes/basic_ode2/#problem-setup","title":"Problem Setup","text":"\\[\\frac{d x}{d t} = x, x(0)=1\\] <p>The solution is \\(x(t)=e^t\\)</p>"},{"location":"examples/odes/basic_ode2/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we use the default training configuration, and default setup for learnable endogenous variable. <pre><code>ode2 = PDEModel(\"ode2\") # define PDE model to solve\node2.set_state([\"t\"], {\"t\": [-2., 2.]}) # set the state variable, which defines the dimensionality of the problem\node2.add_endog(\"x\") # we use endogenous variable to represent the function we want to approximate\node2.add_endog_equation(\"x_t=x\", label=\"base_ode\") \node2.add_endog_condition(\"x\", \n                              \"x(SV)\", {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"initial_condition\") \n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>ode2.train_model(\"./models/ode2\", \"ode2.pt\", True)\node2.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>ode2.load_model(torch.load(\"./models/ode2/ode2.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 3, figsize=(18, 6))\nt = np.linspace(-2, 2)\nax[0].plot(t, np.exp(t), label=\"e^t\")\nax[1].plot(t, np.exp(t), label=\"e^t\")\nax[2].plot(t, np.exp(t), label=\"e^t\")\node2.endog_vars[\"x\"].plot(\"x\", {\"t\": [-2, 2]}, ax=ax[0])\node2.endog_vars[\"x\"].plot(\"x_t\", {\"t\": [-2, 2]}, ax=ax[1])\node2.endog_vars[\"x\"].plot(\"x_tt\", {\"t\": [-2, 2]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/odes/cauchy-euler/","title":"Cauchy-Euler Equation","text":"<p>The full solution can be found at base_ode.ipynb.</p>"},{"location":"examples/odes/cauchy-euler/#problem-setup","title":"Problem Setup","text":"\\[ x^2 y'' + 6xy' + 4y =0, y(1)=6, y(2)=\\frac{5}{4}\\] <p>Solution: \\(y=4x^{-4} + 2 x^{-1}\\)</p>"},{"location":"examples/odes/cauchy-euler/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we set up the endogenous variable, endogenous equation, and initial conditions. <pre><code>ode = PDEModel(\"cauchy_euler\") # define PDE model to solve\node.set_state([\"x\"], {\"x\": [1., 2.]}) # set the state variable, which defines the dimensionality of the problem\node.add_endog(\"y\") # we use endogenous variable to represent the function we want to approximate\node.add_endog_equation(\"x**2 * y_xx + 6*x*y_x + 4*y=0\", label=\"base_ode\") # endogenous equations are used to represent the ODE\node.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": torch.ones((1, 1))},\n                              Comparator.EQ,\n                              \"6\", {},\n                              label=\"ic1\") # define initial condition\node.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": 2 * torch.ones((1, 1))},\n                              Comparator.EQ,\n                              \"5/4\", {},\n                              label=\"ic2\") # define second initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>ode.train_model(\"./models/cauchy_euler\", \"cauchy_euler.pt\", True)\node.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>ode.load_model(torch.load(\"./models/cauchy_euler/cauchy_euler.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 3, figsize=(18, 6))\nx = np.linspace(1, 2)\nax[0].plot(x, 4*x**(-4) + 2 * x**(-1), label=\"4x^{-4}+2x^{-1}\")\nax[1].plot(x, -16*x**(-5)-2*x**(-2), label=\"-16x^{-5}-2x^{-2}\")\nax[2].plot(x, 80*x**(-6) + 4*x**(-3), label=\"80x^{-6}+4x^{-3}\")\node.endog_vars[\"y\"].plot(\"y\", {\"x\": [1, 2]}, ax=ax[0])\node.endog_vars[\"y\"].plot(\"y_x\", {\"x\": [1, 2]}, ax=ax[1])\node.endog_vars[\"y\"].plot(\"y_xx\", {\"x\": [1, 2]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/odes/cauchy-euler/#kan-approach","title":"KAN Approach","text":"<p>Here, we try using the KAN layer for the endogenous variable. The rest of the steps are the same. <pre><code>from deep_macrofin import OptimizerType, set_seeds, LayerType\nset_seeds(0) # set random seed for reproducibility\node = PDEModel(\"cauchy_euler\", config={\"num_epochs\": 100, \"lr\": 1, \"loss_log_interval\": 10}) # define PDE model to solve\node.set_state([\"x\"], {\"x\": [1., 2.]}) # set the state variable, which defines the dimensionality of the problem\node.add_endog(\"y\", config={\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"hidden_units\": [1, 5, 5, 1],\n    \"layer_type\": LayerType.KAN,\n    \"activation_type\": ActivationType.SiLU,\n    \"positive\": False,\n    \"derivative_order\": 2,\n}) # we use endogenous variable to represent the function we want to approximate\node.add_endog_equation(\"x**2 * y_xx + 6*x*y_x + 4*y=0\", label=\"base_ode\") # endogenous equations are used to represent the ODE\node.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": torch.ones((1, 1))},\n                              Comparator.EQ,\n                              \"6\", {},\n                              label=\"ic1\") # define initial condition\node.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": 2 * torch.ones((1, 1))},\n                              Comparator.EQ,\n                              \"5/4\", {},\n                              label=\"ic2\") # define second initial condition\n</code></pre></p>"},{"location":"examples/odes/predator_prey/","title":"Predator-Prey Model","text":"<p>The full solution can be found at system_ode.ipynb.</p>"},{"location":"examples/odes/predator_prey/#problem-setup","title":"Problem Setup","text":"\\[\\begin{bmatrix}\\dot{x} \\\\ \\dot{y}\\end{bmatrix} = \\begin{bmatrix} \\alpha x - \\beta xy \\\\ \\delta xy - \\gamma y\\end{bmatrix}\\] <p>In the example, \\(\\alpha=1.1, \\beta=0.4, \\delta=0.4, \\gamma=0.1\\)</p>"},{"location":"examples/odes/predator_prey/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we define predator-prey dynamics with specific initial conditions and training epochs. <pre><code>lv = PDEModel(\"lotka_volterra\", {\"num_epochs\": 2000}) # define PDE model to solve\nlv.set_state([\"t\"], {\"t\": [0., 5.]}) # set the state variable, which defines the dimensionality of the problem\nlv.add_endog(\"x\") \nlv.add_endog(\"y\") # we use endogenous variable to represent the function we want to approximate\nlv.add_endog_equation(\"x_t = 1.1 * x - 0.4*x*y\", label=\"base_ode1\")\nlv.add_endog_equation(\"y_t = 0.4 * x * y - 0.1 * y\") # endogenous equations are used to represent the ODE\nlv.add_endog_condition(\"x\", \n                              \"x(SV)\", {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"initial_condition\") # define initial condition\nlv.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"initial_condition\") # define second initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>lv.train_model(\"./models/lotka_volterra\", \"lotka_volterra.pt\", True)\nlv.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>lv.load_model(torch.load(\"./models/lotka_volterra/lotka_volterra.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(2, 2, figsize=(11, 11))\nt = np.linspace(0., 5.)\nx = lv.endog_vars[\"x\"].derivatives[\"x\"](torch.Tensor(t).unsqueeze(-1).to(lv.device)).detach().cpu().numpy()\ny = lv.endog_vars[\"y\"].derivatives[\"y\"](torch.Tensor(t).unsqueeze(-1).to(lv.device)).detach().cpu().numpy()\nlv.endog_vars[\"x\"].plot(\"x\", {\"t\": [0., 5.]}, ax=ax[0][0])\nlv.endog_vars[\"y\"].plot(\"y\", {\"t\": [0., 5.]}, ax=ax[0][1])\nax[1][0].plot(t, 1.1*x-0.4*x*y, label=\"1.1x-0.4xy\")\nax[1][1].plot(t, 0.4*x*y-0.1*y, label=\"0.4xy-0.1y\")\nlv.endog_vars[\"x\"].plot(\"x_t\", {\"t\": [0., 5.]}, ax=ax[1][0])\nlv.endog_vars[\"y\"].plot(\"y_t\", {\"t\": [0., 5.]}, ax=ax[1][1])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/odes/second_order/","title":"Second-Order ODE","text":"<p>The full solution can be found at basic_odes2.ipynb. DeepXDE result can be found on their website.</p>"},{"location":"examples/odes/second_order/#problem-setup","title":"Problem Setup","text":"\\[y''-10y'+9y=5t, y(0)=-1, y'(0)=2\\] <p>The solution is \\(y=\\frac{50}{81} + \\frac{5}{9}t + \\frac{31}{81}e^{9t} - 2e^t\\)</p>"},{"location":"examples/odes/second_order/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem In this example, we define a second-order linear PDE model with specific initial conditions and training configurations. <pre><code>ode = PDEModel(\"second_order_linear\", config={\"num_epochs\": 10000}) # define PDE model to solve\node.set_state([\"t\"], {\"t\": [0., 0.25]}) # set the state variable, which defines the dimensionality of the problem\node.add_endog(\"y\", config={\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"hidden_units\": [50, 50, 50],\n    \"activation_type\": ActivationType.Tanh,\n    \"positive\": False,\n    \"derivative_order\": 2,\n}) # we use endogenous variable to represent the function we want to approximate\node.add_endog_equation(\"y_tt-10*y_t+9*y=5*t\", label=\"base_ode\", weight=0.01) # endogenous equations are used to represent the ODE\node.add_endog_condition(\"y\", \n                            \"y(SV)\", {\"SV\": torch.zeros((1, 1))},\n                            Comparator.EQ,\n                            \"-1\", {},\n                            label=\"ic1\") # define initial condition\node.add_endog_condition(\"y\", \n                            \"y_t(SV)\", {\"SV\": torch.zeros((1, 1))},\n                            Comparator.EQ,\n                            \"2\", {},\n                            label=\"ic2\") # add a second initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>ode.train_model(\"./models/second_order_ode3\", \"second_order_ode3.pt\", True)\node.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>ode1.load_model(torch.load(\"./models/ode1/ode1.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 3, figsize=(18, 6))\nx = np.linspace(0, 0.25)\nax[0].plot(x, 50/81+5/9*x+31/81*np.exp(9*x)-2*np.exp(x), label=\"y_true\")\nax[1].plot(x, 5/9+31/9*np.exp(9*x)-2*np.exp(x), label=\"y_t_true\")\nax[2].plot(x, 31*np.exp(9*x)-2*np.exp(x), label=\"y_tt_true\")\node.endog_vars[\"y\"].plot(\"y\", {\"t\": [0, 0.25]}, ax=ax[0])\node.endog_vars[\"y\"].plot(\"y_t\", {\"t\": [0, 0.25]}, ax=ax[1])\node.endog_vars[\"y\"].plot(\"y_tt\", {\"t\": [0, 0.25]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/pdes/diffusion/","title":"Diffusion Equation","text":"<p>The full solution can be found at diffusion_equations.ipynb.</p>"},{"location":"examples/pdes/diffusion/#problem-setup","title":"Problem Setup","text":"\\[\\frac{\\partial y}{\\partial t} = \\frac{\\partial^2 y}{\\partial x^2} - e^{-t} (\\sin(\\pi x) - \\pi^2 \\sin (\\pi x))\\] <p>with \\(x\\in [-1,1], t\\in[0,1]\\), \\(y(x,0)=\\sin(\\pi x)\\), \\(y(-1,t)=y(1,t)=0\\).</p> <p>Solution: \\(y=e^{-t}\\sin(\\pi x)\\)</p>"},{"location":"examples/pdes/diffusion/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we set up the endogenous variable, endogenous equation, boundary and initial conditions. <pre><code>pde1 = PDEModel(\"diffusion_1d\") # define pde model to solve\npde1.set_state([\"x\", \"t\"], {\"x\": [-1., 1.], \"t\": [0, 1.]}) # set the state variable, which defines the dimensionality of the problem\npde1.add_endog(\"y\") # we use endogenous variable to represent the function we want to approximate\npde1.add_endog_equation(\"y_t = y_xx - exp(-t) * (sin(pi * x) - pi**2 * sin(pi*x))\", label=\"base_pde\") # endogenous equations are used to represent the PDE\n\nmone_xs = -1 * torch.ones((100, 2)) # Create a tensor for boundary condition at x = -1, with t values from 0 to 1\nmone_xs[:, 1] = torch.Tensor(np.linspace(0, 1, 100))\n\none_xs = torch.ones((100, 2)) # Create a tensor for boundary condition at x = 1, with t values from 0 to 1\none_xs[:, 1] = torch.Tensor(np.linspace(0, 1, 100))\n\nzero_ts = torch.zeros((100, 2)) # Create a tensor for initial condition at t = 0, with x values from -1 to 1\nzero_ts[:, 0] = torch.Tensor(np.linspace(-1, 1, 100))\ny_zero_ts = torch.sin(torch.pi * zero_ts[:, 0:1])\n\npde1.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": mone_xs},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"bc_zerox\") # Add boundary condition \npde1.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": one_xs},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"bc_onex\") # Add boundary condition\npde1.add_endog_condition(\"y\", \n                              \"y(SV)\", {\"SV\": zero_ts},\n                              Comparator.EQ,\n                              \"y_zero_ts\", {\"y_zero_ts\": y_zero_ts},\n                              label=\"ic\") # Add initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>pde1.train_model(\"./models/diffusion_1d\", \"diffusion_1d.pt\", True)\npde1.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>pde1.load_model(torch.load(\"./models/diffusion_1d/diffusion_1d.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={\"projection\": \"3d\"})\nx_np = np.linspace(-1, 1, 100)\nt_np = np.linspace(0, 1, 100)\nX, T = np.meshgrid(x_np, t_np)\nexact_Y = np.exp(-T) * np.sin(np.pi*X)\nax.plot_surface(X, T, exact_Y, label=\"exact\", alpha=0.6)\npde1.endog_vars[\"y\"].plot(\"y\", {\"x\": [-1., 1.], \"t\": [0, 1.]}, ax=ax)\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/pdes/heat/","title":"Time-dependent Heat Equation","text":"<p>The full solution can be found at basic_pdes.ipynb.</p>"},{"location":"examples/pdes/heat/#problem-setup","title":"Problem Setup","text":"\\[\\frac{\\partial u}{\\partial t} = 0.4 \\frac{\\partial^2 u}{\\partial x^2}, x\\in [0,1], t\\in[0,1],\\] \\[u(0,t)=u(1,t)=0, u(x,0)=\\sin(\\pi x)\\] <p>Solution is \\(u(x,t)=e^{-0.4\\pi^2t} \\sin(\\pi x)\\)</p>"},{"location":"examples/pdes/heat/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we first set up the endogenous variables, equations, and initialize boundary conditions. <pre><code>pde2 = PDEModel(\"time_heat\") # define pde model to solve\npde2.set_state([\"x\", \"t\"], {\"x\": [0, 1.], \"t\": [0, 1.]}) # set the state variable, which defines the dimensionality of the problem\npde2.add_endog(\"u\") # we use endogenous variable to represent the function we want to approximate\npde2.add_endog_equation(r\"$\\frac{\\partial u}{\\partial t} = 0.4 * \\frac{\\partial^2 u}{\\partial x^2}$\", label=\"base_pde\") # endogenous equations are used to represent the PDE\n\nzero_xs = torch.zeros((100, 2)) # Create a tensor for boundary condition at x = 0, with t values from 0 to 1\nzero_xs[:, 1] = torch.Tensor(np.linspace(0, 1, 100))\n\none_xs = torch.ones((100, 2)) # Create a tensor for boundary condition at x = 1, with t values from 0 to 1\none_xs[:, 1] = torch.Tensor(np.linspace(0, 1, 100))\n\nzero_ts = torch.zeros((100, 2)) # Create a tensor for initial condition at t = 0, with x values from 0 to 1\nzero_ts[:, 0] = torch.Tensor(np.linspace(0, 1, 100))\nu_zero_ts = torch.sin(torch.pi * zero_ts[:, 0:1])\n\npde2.add_endog_condition(\"u\", \n                              \"u(SV)\", {\"SV\": zero_xs},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"bc_zerox\") # Add boundary condition\npde2.add_endog_condition(\"u\", \n                              \"u(SV)\", {\"SV\": one_xs},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"bc_onex\") # Add boundary condition\npde2.add_endog_condition(\"u\", \n                              \"u(SV)\", {\"SV\": zero_ts},\n                              Comparator.EQ,\n                              \"u_zero_ts\", {\"u_zero_ts\": u_zero_ts},\n                              label=\"ic\") # Add initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>pde2.train_model(\"./models/time_heat\", \"time_heat.pt\", True)\npde2.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>pde2.load_model(torch.load(\"./models/time_heat/time_heat.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={\"projection\": \"3d\"})\nx_np = np.linspace(0, 1, 100)\nt_np = np.linspace(0, 1, 100)\nX, T = np.meshgrid(x_np, t_np)\nexact_Z = np.exp(-0.4*np.pi**2 * T) * np.sin(np.pi*X)\nax.plot_surface(X, T, exact_Z, label=\"exact\", alpha=0.6)\npde2.endog_vars[\"u\"].plot(\"u\", {\"x\": [0, 1.], \"y\": [0, 1.]}, ax=ax)\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/pdes/laplace/","title":"Laplace Equation Dirichlet Problem","text":"<p>The full solution can be found at basic_pdes.ipynb.</p>"},{"location":"examples/pdes/laplace/#problem-setup","title":"Problem Setup","text":"\\[\\nabla^2 T = \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} = 0, T(x,0)=T(x,\\pi)=0, T(0,y)=1\\] <p>The solution is \\(T(x,y) = \\frac{2}{\\pi} \\arctan\\frac{\\sin y}{\\sinh x}\\)</p>"},{"location":"examples/pdes/laplace/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem Here, we first set up the endogenous variables, equations, and initialize boundary conditions. <pre><code>pde1 = PDEModel(\"laplace_dirichlet\") # define PDE model for Laplace's equation with Dirichlet boundary conditions\npde1.set_state([\"x\", \"y\"], {\"x\": [0, 3.], \"y\": [0, np.pi]}) # set state variables \"x\" and \"y\" with their respective ranges\npde1.add_endog(\"T\") # add endogenous variable \"T\" to represent the temperature\n\npde1.add_endog_equation(r\"$\\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} = 0$\", label=\"base_pde\") \n# add Laplace's equation as the endogenous equation\n\nzero_ys = torch.zeros((100, 2)) # create tensor for boundary condition at y=0\nzero_ys[:, 0] = torch.Tensor(np.linspace(0, 3, 100)) # set x-values from 0 to 3\n\npi_ys = torch.zeros((100, 2)) # create tensor for boundary condition at y=pi\npi_ys[:, 0] = torch.Tensor(np.linspace(0, 3, 100)) # set x-values from 0 to 3\npi_ys[:, 1] = torch.pi # set y-value to pi\n\nzero_xs = torch.zeros((100, 2)) # create tensor for boundary condition at x=0\nzero_xs[:, 1] = torch.Tensor(np.linspace(0, np.pi, 100)) # set y-values from 0 to pi\n\npde1.add_endog_condition(\"T\", \n                              \"T(SV)\", {\"SV\": zero_ys},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"bc_zeroy\") # add boundary condition for T=0 at y=0\n\npde1.add_endog_condition(\"T\", \n                              \"T(SV)\", {\"SV\": pi_ys},\n                              Comparator.EQ,\n                              \"0\", {},\n                              label=\"bc_piy\") # add boundary condition for T=0 at y=pi\n\npde1.add_endog_condition(\"T\", \n                              \"T(SV)\", {\"SV\": zero_xs},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"bc_zerox\") # add boundary condition for T=1 at x=0\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>pde1.train_model(\"./models/laplace_dirichlet\", \"laplace_dirichlet.pt\", True)\npde1.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>pde1.load_model(torch.load(\"./models/laplace_dirichlet/laplace_dirichlet.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 1, figsize=(5, 5), subplot_kw={\"projection\": \"3d\"})\nx_np = np.linspace(0, 3, 100)\ny_np = np.linspace(0, np.pi, 100)\nX, Y = np.meshgrid(x_np, y_np)\nexact_Z = 2. / np.pi * np.arctan(np.sin(Y) / np.sinh(X))\nax.plot_surface(X, Y, exact_Z, label=\"exact\", alpha=0.6)\npde1.endog_vars[\"T\"].plot(\"T\", {\"x\": [0, 3.], \"y\": [0, np.pi]}, ax=ax)\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"}]}