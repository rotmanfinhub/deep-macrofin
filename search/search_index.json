{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep-MacroFin","text":"<p>Deep-MacroFin is a comprehensive deep-learning framework designed to solve partial differential equations, with a particular focus on models in continuous time economics.  It is inspired from PyMacroFin<sup>1</sup> and DeepXDE<sup>2</sup> </p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#install-from-pypi","title":"Install from PyPI","text":"<p>The stable version of the package can be installed from PyPI.</p> <pre><code>pip install deep-macrofin\n</code></pre>"},{"location":"#build-from-source","title":"Build from Source","text":"<p>For developers, you should clone the folder to your local machine and install from the local folder.</p> <ol> <li> <p>Clone the repository <pre><code>git clone https://github.com/rotmanfinhub/deep-macrofin.git\n</code></pre></p> </li> <li> <p>Create a virtual environment (Optional, but recommended) <pre><code>python -m venv venv\nsource venv/bin/activate # venv/Scripts/activate using Windows powershell\n</code></pre></p> </li> <li> <p>Install dependencies <pre><code>pip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-doc.txt\n</code></pre></p> </li> <li> <p>Install the package <pre><code>pip install -e .\n</code></pre></p> </li> </ol>"},{"location":"#docs","title":"Docs","text":"<p>The documentation site is based on mkdocs and mkdocs-mateiral.</p> <p>Layouts <pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre></p> <p>To see the site locally, run the following command: <pre><code>mkdocs serve\n</code></pre></p> <ol> <li> <p>Adrien d'Avernas and Damon Petersen and Quentin Vandeweyer, \"Macro-financial Modeling in Python: PyMacroFin\", 2021-11-18\u00a0\u21a9</p> </li> <li> <p>Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em, \"DeepXDE: A deep learning library for solving differential equations\", SIAM Review, 63(1): 208\u2013228, 2021\u00a0\u21a9</p> </li> </ol>"},{"location":"usage/","title":"Basic Usage","text":""},{"location":"usage/#define-pde-system","title":"Define PDE System","text":"<p>Training configuration etc</p>"},{"location":"usage/#define-agentendogvar","title":"Define Agent/EndogVar","text":"<p>Model details, MLP/KAN instantiation etc.</p>"},{"location":"usage/#add-equations-etc","title":"Add Equations, etc","text":"<p>Latex/Non-Latex Parsing</p>"},{"location":"api/evaluations/","title":"deep_macrofin.evaluations","text":""},{"location":"api/evaluations/#formula","title":"Formula","text":"<pre><code>class Formula(formula_str: str, evaluation_method: Union[EvaluationMethod, str], latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Base class for string evaluations. Given a string representation of a formula, and a set of variables (state, value, prices, etc) in a model, parse the formula to a pytorch function that can be evaluated. </p> <p>Latex equation with restricted format is supported for initialization. </p> <p>Parameters:</p> <ul> <li>formula_str: str, the string version of the formula. If the provided formula_str is supposed to be a latex string, it must be $ enclosed and in the regular form, e.g. <code>formula_str=r\"$x^2*y$\"</code>, and all multiplication symbols must be explicitly provided as * in the equation.</li> <li>evaluation_method: Union[EvaluationMethod, str], Enum, select from <code>eval</code>, <code>sympy</code>, <code>ast</code>, corresponding to the four methods below. For now, only eval is supported.</li> <li> <p>latex_var_mapping: Dict[str, str], only used if the formula_str is in latex form, the keys should be the latex expression, and the values should be the corresponding python variable name. All strings with single slash in latex must be defined as a raw string. All spaces in the key must match exactly as in the input formula_str. </p> <p>Example: <pre><code>latex_var_map = {\n    r\"\\eta_t\": \"eta\",\n    r\"\\rho^i\": \"rhoi\",\n    r\"\\mu^{n h}_t\": \"munh\",\n    r\"\\sigma^{na}_t\": \"signa\",\n    r\"\\sigma^{n ia}_t\": \"signia\",\n    r\"\\sigma_t^{qa}\": \"sigqa\",\n    \"c_t^i\": \"ci\",\n    \"c_t^h\": \"ch\",\n}\n</code></pre></p> </li> </ul> <p>Evaluation Methods: <pre><code>class EvaluationMethod(str, Enum):\n    Eval = \"eval\"\n    Sympy = \"sympy\"\n    AST = \"ast\"\n</code></pre></p>"},{"location":"api/evaluations/#eval","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Evaluate the formula with existing functions and provided assignments to variables This evaluates the function by simple string parsing.</p> <p>Parameters:</p> <ul> <li>available_functions: Dict[str, Callable], function calls attached to a string variable. It can include all <code>LearnableVar</code> and derivatives.</li> <li>variables: Dict[str, torch.Tensor], values assigned to each variable.</li> </ul>"},{"location":"api/evaluations/#comparator","title":"Comparator","text":"<p>A Enum class representing comparitor symbols, used in Conditions and Constraint. </p> <pre><code>class Comparator(str, Enum):\n    LEQ = \"&lt;=\"\n    GEQ = \"&gt;=\"\n    LT = \"&lt;\"\n    GT = \"&gt;\"\n    EQ = \"=\"\n</code></pre>"},{"location":"api/evaluations/#baseconditions","title":"BaseConditions","text":"<pre><code>class BaseConditions(lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Define specific boundary/initial conditions for a specific agent. e.g. x(0)=0 or x(0)=x(1) (Periodic conditions). May also be an inequality, but it is very rare.</p> <p>The difference between a constraint and a condition is:</p> <ul> <li>a constraint must be satisfied at any state</li> <li>a condition is satisfied at a specific given state</li> </ul> <p>Parameters:</p> <ul> <li>lhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), endog_name(SV), or simply a constant value</li> <li>lhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate lhs at for the agent/endogenous variable</li> <li>comparator: Comparator</li> <li>rhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), endog_name(SV), or simply a constant value</li> <li>rhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate rhs at for the agent/endogenous variable, if rhs is a constant, this can be an empty dictionary</li> <li>label: str, label for the condition</li> <li>latex_var_mapping: Not implemented. only used if the formula_str is in latex form, the keys should be the latex expression, and the values should be the corresponding python variable name. </li> </ul> <p>Example: <pre><code>BaseConditions(lhs=\"f(SV)\", lhs_state={\"SV\": torch.zeros((1,1))}, \n                comparator\"=\", \n                rhs\"1\", rhs_state={}, \n                label=\"eg1\")\n'''\nThe condition is f(0)=1\n'''\n\n\nBaseConditions(lhs=\"f(SV)\", lhs_state={\"SV\": torch.zeros((1,1))}, \n                comparator\"=\", \n                rhs\"f(SV)\", rhs_state={\"SV\": torch.ones((1,1))}, \n                label=\"eg2\")\n'''\nThe condition is f(0)=f(1)\n'''\n</code></pre></p>"},{"location":"api/evaluations/#eval_1","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable]):\n</code></pre> <p>Computes the loss based on the required conditions:</p> \\[\\mathcal{L}_{cond} = \\frac{1}{|U|} \\sum_{x\\in U}\\|\\mathcal{C}(v_i, x)\\|_2^2\\] <p>Details for evaluating non-equality conditions are the same as Constraint</p>"},{"location":"api/evaluations/#agentconditions","title":"AgentConditions","text":"<pre><code>class AgentConditions(agent_name: str, \n                    lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Subclass of <code>BaseConditions</code>. Defines conditions on agent with name <code>agent_name</code>.</p>"},{"location":"api/evaluations/#endogvarconditions","title":"EndogVarConditions","text":"<pre><code>class EndogVarConditions(endog_name: str, \n                        lhs: str, lhs_state: Dict[str, torch.Tensor], \n                        comparator: Comparator, \n                        rhs: str, rhs_state: Dict[str, torch.Tensor], \n                        label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Subclass of <code>BaseConditions</code>. Defines conditions on endogenous variable with name <code>endog_name</code>.</p>"},{"location":"api/evaluations/#constraint","title":"Constraint","text":"<pre><code>class Constraint(lhs: str, comparator: Comparator, rhs: str, \n            label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of a constraint (equality or inequality), and a set of variables (state, value, prices) in a model, parse the equation to a pytorch function that can be evaluated. If the constraint is an inequality, loss should be penalized whenever the inequality is not satisfied. </p>"},{"location":"api/evaluations/#eval_2","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Example:  <pre><code>Constraint(lhs=\"L\", comparator=Comparator.GEQ, rhs=\"R\", \"eg1\")\n</code></pre> This represents the inequality: \\(L\\geq R\\), it is not satisfied when \\(R-L &gt; 0\\), so the loss is formulated as:</p> \\[\\mathcal{L}_{const} = \\frac{1}{B} \\|\\text{ReLU}(R(x)-L(x))\\|_2^2\\] <p>If strict inequality is required, <pre><code>Constraint(lhs=\"L\", comparator=Comparator.LT, rhs=\"R\", \"eg1\")\n</code></pre></p> <p>This represents the inequality: \\(L&lt; R\\), it is not satisfied when \\(L-R \\geq 0\\), an additional \\(\\epsilon=10^{-8}\\) is added to the ReLU activation to ensure strictness.</p>"},{"location":"api/evaluations/#endogequation","title":"EndogEquation","text":"<pre><code>class EndogEquation(eq: str, label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of an endogenuous equation, and a set of variables (state, value, prices) in a model. parse the LHS and RHS of the equation to pytorch functions that can be evaluated. This is used to define the algebraic (partial differential) equations as loss functions.</p>"},{"location":"api/evaluations/#eval_3","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Computes the loss based on the equation:</p> \\[\\mathcal{L}_{endog} = \\frac{1}{B} \\|l(x)-r(x)\\|_2^2\\]"},{"location":"api/evaluations/#equation","title":"Equation","text":"<pre><code>class Equation(eq: str, label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of new variable definition, properly evaluate it with agent, endogenous variables, and constants. Assign new value to LHS.</p>"},{"location":"api/evaluations/#eval_4","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Compute and return the value of RHS, which will be assigned to LHS variable.</p>"},{"location":"api/evaluations/#hjbequation","title":"HJBEquation","text":"<pre><code>class HJBEquation(eq: str, label: str, latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Given a string representation of a Hamilton-Jacobi-Bellman equation, and a set of variables in a model, parse the equation to a pytorch function that can be evaluated.</p>"},{"location":"api/evaluations/#eval_5","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor]):\n</code></pre> <p>Compute the MSE with zero as min/max problem.</p>"},{"location":"api/evaluations/#system","title":"System","text":"<pre><code>class System(activation_constraints: List[Constraint], \n            label: str=None, \n            latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Represents a system to be evaluated when <code>activation_constraints</code> are all satisfied.</p>"},{"location":"api/evaluations/#add_equation","title":"add_equation","text":"<p><pre><code>def add_equation(self, eq: str, label: str=None)\n</code></pre> Add an equation to define a new variable within the system</p>"},{"location":"api/evaluations/#add_endog_equation","title":"add_endog_equation","text":"<p><pre><code>def add_endog_equation(self, eq: str, label: str=None, weight=1.0)\n</code></pre> Add an equation for loss computation within the system</p>"},{"location":"api/evaluations/#compute_constraint_mask","title":"compute_constraint_mask","text":"<p><pre><code>def compute_constraint_mask(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor])\n</code></pre> Check if the constraint is satisfied. Need to check for each individual batch element. Get a mask \\(\\mathbb{1}_{mask}\\) for loss in each batch element.</p>"},{"location":"api/evaluations/#eval_6","title":"eval","text":"<pre><code>def eval(self, available_functions: Dict[str, Callable], variables: Dict[str, torch.Tensor])\n</code></pre> <p>Compute the loss based on the system constraint. Only elements in a batch that satisfy the <code>activation_constraints</code> are used in the loss computation.</p> \\[\\mathcal{L}_{endog, i} = \\frac{1}{\\sum \\mathbb{1}_{mask}} \\langle (l-r)^2, \\mathbb{1}_{mask}\\rangle\\] \\[\\mathcal{L}_{sys} = \\sum_{i=1}^N \\lambda_i \\mathcal{L}_{endog, i}\\]"},{"location":"api/models/","title":"deep_macrofin.models","text":""},{"location":"api/models/#learnablevar","title":"LearnableVar","text":"<pre><code>class LearnableVar(name: str, state_variables: List[str], config: Dict[str, Any])\n</code></pre> <p>Base class for agents and endogenous variables. This is a subclass of <code>torch.nn</code> module.</p> <p>Parameters:  </p> <ul> <li>name: str, The name of the model.  </li> <li>state_variables: List[str], List of state variables.  </li> <li>config: Dict[str, Any], specifies number of layers/hidden units of the neural network and highest order of derivatives to take. <ul> <li>device: str, the device to run the model on (e.g., \"cpu\", \"cuda\"), default will be chosen based on whether or not GPU is available  </li> <li>hidden_units: List[int], number of units in each layer, default: [30, 30, 30, 30]  </li> <li>layer_type: str, a selection from the LayerType enum, default: LayerType.MLP  </li> <li>activation_type: str, a selection from the ActivationType enum, default: ActivationType.Tanh  </li> <li>positive: bool, apply softplus to the output to be always positive if true, default: false  </li> <li>hardcode_function: a lambda function for hardcoded forwarding function, default: None  </li> <li>derivative_order: int, an additional constraint for the number of derivatives to take, so for a function with one state variable, we can still take multiple derivatives, default: number of state variables  </li> </ul> </li> </ul>"},{"location":"api/models/#get_all_derivatives","title":"get_all_derivatives","text":"<p>Get all derivatives of the current variable, upto a specific order defined by the user with <code>derivative_order</code>. The construction of derivatives can be found in derivative_utils.</p> <pre><code>def get_all_derivatives(self):\n    '''\n    Returns a dictionary of derivative functional mapping \n    e.g. if name=\"qa\", state_variables=[\"e\", \"t\"], derivative_order=2, it will return \n    {\n        \"qa\": self.forward\n        \"qa_e\": lambda x:self.compute_derivative(x, \"e\")\n        \"qa_t\": lambda x:self.compute_derivative(x, \"t\"),\n        \"qa_ee\": lambda x:self.compute_derivative(x, \"ee\"),\n        \"qa_tt\": lambda x:self.compute_derivative(x, \"tt\"),\n        \"qa_et\": lambda x:self.compute_derivative(x, \"et\"),\n        \"qa_te\": lambda x:self.compute_derivative(x, \"te\"),\n    }\n\n    Note that the last two will be the same for C^2 functions, \n    but we keep them for completeness. \n    '''\n</code></pre>"},{"location":"api/models/#plot","title":"plot","text":"<p>Plot a specific function attached to the learnable variable over the domain.</p> <pre><code>def plot(self, target: str, domain: Dict[str, List[np.float32]]={}, ax=None):\n    '''\n    Inputs:\n        target: name for the original function, or the associated derivatives to plot\n        domain: the range of state variables to plot. \n        If state_variables=[\"x\", \"y\"] domain = {\"x\": [0,1], \"y\":[-1,1]}, it will be plotted on the region [0,1]x[-1,1].\n        If one of the variable is not provided in the domain, [0,1] will be taken as the default\n        ax: a matplotlib.Axes object to plot on, if not provided, it will be plotted on a new figure\n\n    This function is only supported for 1D or 2D state_variables.\n    '''\n</code></pre>"},{"location":"api/models/#to_dict","title":"to_dict","text":"<pre><code>def to_dict(self)\n</code></pre> <p>Save all the configurations and weights to a dictionary.</p> <pre><code>dict_to_save = {\n    \"name\": self.name,\n    \"model\": self.state_dict(),\n    \"model_config\": self.config,\n    \"system_rng\": random.getstate(),\n    \"numpy_rng\": np.random.get_state(),\n    \"torch_rng\": torch.random.get_rng_state(),\n}\n</code></pre>"},{"location":"api/models/#from_dict","title":"from_dict","text":"<pre><code>def from_dict(self, dict_to_load: Dict[str, Any])\n</code></pre> <p>Load all the configurations and weights from a dictionary.</p>"},{"location":"api/models/#agent","title":"Agent","text":"<pre><code>class Agent(name: str, state_variables: List[str], config: Dict[str, Any])\n</code></pre> <p>Subclass of <code>LearnableVar</code>. Defines agent wealth multipliers. </p>"},{"location":"api/models/#endogvar","title":"EndogVar","text":"<pre><code>class EndogVar(name: str, state_variables: List[str], config: Dict[str, Any])\n</code></pre> <p>Subclass of <code>LearnableVar</code>. Defines endogenous variables. </p>"},{"location":"api/models/#derivative_utils","title":"derivative_utils","text":""},{"location":"api/models/#get_derivs_1order","title":"get_derivs_1order","text":"<p><pre><code>def get_derivs_1order(y, x, idx):\n</code></pre> Returns the first order derivatives of \\(y\\) w.r.t. \\(x\\). Automatic differentiation used.</p> <p>Example: <pre><code>x = torch.tensor([[1.0, 2.0]]) # assuming two variables x1, x2\nx.requires_grad_(True)\ny1 = x**2\nprint(get_derivs_1order(y1, x, 0)) \n'''\nOutput: [[2.]] dy1/dx1 = 2\n'''\nprint(get_derivs_1order(y1, x, 1))\n'''\nOutput: [[4.]] dy1/dx2 = 4\n'''\n\nx = torch.tensor([[1.0, 2.0]]) # assuming two variables x1, x2\nx.requires_grad_(True)\ny2 = x[:,0:1] * x[:,1:2] \nprint(get_derivs_1order(y2, x, 0)) \n'''\nOutput: [[2.]] dy2/dx1 = 2\n'''\nprint(get_derivs_1order(y2, x, 1))\n'''\nOutput: [[1.]] dy2/dx2 = 1\n'''\n</code></pre></p>"},{"location":"api/models/#get_all_derivs","title":"get_all_derivs","text":"<pre><code>def get_all_derivs(target_var_name=\"f\", all_vars: List[str] = [\"x\", \"y\", \"z\"], derivative_order = 2) -&gt; Dict[str, Callable]:\n</code></pre> <p>Implements Algorithm 1 in the paper. Higher order derivatives are computed iteratively using dynamic programming and first order derivative.</p> <p>Example: <pre><code>derivs = get_all_derivs(target_var_name=\"qa\", all_vars= [\"e\", \"t\"], derivative_order = 2)\n'''\nderivs = {\n    \"qa_e\": lambda y, x, idx=0: get_derivs_1order(y, x, idx)\n    \"qa_t\": lambda y, x, idx=1: get_derivs_1order(y, x, idx),\n    \"qa_ee\": lambda y, x, idx=0: get_derivs_1order(y, x, idx), # here y=qa_e\n    \"qa_et\": lambda y, x, idx=1: get_derivs_1order(y, x, idx), # here y=qa_e\n    \"qa_te\": lambda y, x, idx=0: get_derivs_1order(y, x, idx), # here y=qa_t\n    \"qa_tt\": lambda y, x, idx=1: get_derivs_1order(y, x, idx), # here y=qa_t\n}\n\nAfterwards, [e,t] should be passed as x into the lambda function\n'''\n</code></pre></p>"},{"location":"api/models/#constants","title":"Constants","text":"<p>These constants are used to identify model/layer/activation types for initialization.</p> <pre><code>class LearnableModelType(str, Enum):\n    Agent=\"Agent\"\n    EndogVar=\"EndogVar\"\n\nclass LayerType(str, Enum):\n    MLP=\"MLP\"\n    KAN=\"KAN\"\n\nclass ActivationType(str, Enum):\n    ReLU=\"relu\"\n    SiLU=\"silu\"\n    Sigmoid=\"sigmoid\"\n    Tanh=\"tanh\"\n</code></pre>"},{"location":"api/pde_model/","title":"deep_macrofin.pde_model","text":"<p>This is the main interface to construct the PDE system to solve.</p>"},{"location":"api/pde_model/#pdemodel","title":"PDEModel","text":"<pre><code>class PDEModel(name: str, \n                config: Dict[str, Any] = DEFAULT_CONFIG, \n                latex_var_mapping: Dict[str, str] = {})\n</code></pre> <p>Initialize a PDEModel with the provided name and config. </p> <p>Parameters:</p> <ul> <li>name: str, name of the PDE model.</li> <li>config: Dict[str, Any], defines the training configuration. Check Constants for default values.<ul> <li>batch_size: int</li> <li>num_epochs: int</li> <li>lr: float, learning rate for optimizer</li> <li>loss_log_interval: int, the interval at which loss should be reported/recorded</li> </ul> </li> <li>latex_var_mapping: Dict[str, str], it should include all possible latex to python name conversions. Otherwise latex parsing will fail. Can be omitted if all the input equations/formula are not in latex form. For details, check <code>Formula</code>.</li> </ul>"},{"location":"api/pde_model/#set_state","title":"set_state","text":"<p><pre><code>def set_state(self, names: List[str], constraints: Dict[str, List] = {})\n</code></pre> Set the state variables (\"grid\") of the problem. By default, the constraints will be [-1, 1] (for easier sampling). Only rectangular regions are supported.</p>"},{"location":"api/pde_model/#add_param","title":"add_param","text":"<p><pre><code>def add_param(self, name: str, value: torch.Tensor)\n</code></pre> Add a single parameter (constant in the PDE system) with name and value.</p>"},{"location":"api/pde_model/#add_params","title":"add_params","text":"<p><pre><code>def add_params(self, params: Dict[str, Any])\n</code></pre> Add a dictionary of parameters (constants in the PDE system) for the system.</p>"},{"location":"api/pde_model/#add_agent","title":"add_agent","text":"<pre><code>def add_agent(self, name: str, \n            config: Dict[str, Any] = DEFAULT_LEARNABLE_VAR_CONFIG,\n            overwrite=False)\n</code></pre> <p>Add a single Agent, with relevant config of neural network representation. If called before states are set, should raise an error.</p> <p>Parameters:</p> <ul> <li>name: unique identifier of agent.</li> <li>Config: specifies number of layers/hidden units of the neural network. Check Constants for default values.</li> <li>overwrite: overwrite the previous agent with the same name, used for loading, default: False</li> </ul>"},{"location":"api/pde_model/#add_agents","title":"add_agents","text":"<p><pre><code>def add_agents(self, names: List[str], \n               configs: Dict[str, Dict[str, Any]]={})\n</code></pre> Add multiple Agents at the same time, each with different configurations.</p>"},{"location":"api/pde_model/#add_agent_condition","title":"add_agent_condition","text":"<p><pre><code>def add_agent_condition(self, name: str, \n                    lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str=None,\n                    weight: float=1.0):\n</code></pre> Add boundary/initial condition for a specific agent with associated weight</p> <p>Parameters:</p> <ul> <li>name: str, agent name, </li> <li>lhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>lhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate lhs at for the agent/endogenous variable</li> <li>comparator: Comparator</li> <li>rhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>rhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate rhs at for the agent/endogenous variable, if rhs is a constant, this can be an empty dictionary</li> <li>label: str label for the condition</li> <li>weight: float, weight in total loss computation</li> </ul>"},{"location":"api/pde_model/#add_endog","title":"add_endog","text":"<pre><code>def add_endog(self, name: str, \n            config: Dict[str, Any] = DEFAULT_LEARNABLE_VAR_CONFIG,\n            overwrite=False)\n</code></pre> <p>Add a single EndogVar, with relevant config of neural network representation. If called before states are set, should raise an error.</p> <p>Parameters:</p> <ul> <li>name: unique identifier of the endogenous variable.</li> <li>Config: specifies number of layers/hidden units of the neural network. Check Constants for default values.</li> <li>overwrite: overwrite the previous endogenous variable with the same name, used for loading, default: False</li> </ul>"},{"location":"api/pde_model/#add_endogs","title":"add_endogs","text":"<p><pre><code>def add_endogs(self, names: List[str], \n               configs: Dict[str, Dict[str, Any]]={})\n</code></pre> Add multiple EndogVars at the same time, each with different configurations.</p>"},{"location":"api/pde_model/#add_endog_condition","title":"add_endog_condition","text":"<p><pre><code>def add_endog_condition(self, name: str, \n                    lhs: str, lhs_state: Dict[str, torch.Tensor], \n                    comparator: Comparator, \n                    rhs: str, rhs_state: Dict[str, torch.Tensor], \n                    label: str=None,\n                    weight: float=1.0):\n</code></pre> Add boundary/initial condition for a specific endogenous variable with associated weight</p> <p>Parameters:</p> <ul> <li>name: str, agent name, </li> <li>lhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>lhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate lhs at for the agent/endogenous variable</li> <li>comparator: Comparator</li> <li>rhs: str, the string expression for lhs formula, latex expression not supported, should be functions of specific format agent_name(SV), or simply a constant value</li> <li>rhs_state: Dict[str, torch.Tensor], the specific value of SV to evaluate rhs at for the agent/endogenous variable, if rhs is a constant, this can be an empty dictionary</li> <li>label: str label for the condition</li> <li>weight: float, weight in total loss computation</li> </ul>"},{"location":"api/pde_model/#add_equation","title":"add_equation","text":"<pre><code>def add_equation(self, eq: str, label: str=None)\n</code></pre> <p>Add an equation to define a new variable. </p>"},{"location":"api/pde_model/#add_endog_equation","title":"add_endog_equation","text":"<pre><code>def add_endog_equation(self, eq: str, label: str=None, weight=1.0)\n</code></pre> <p>Add an endogenous equation for loss computation.</p>"},{"location":"api/pde_model/#add_constraint","title":"add_constraint","text":"<pre><code>def add_constraint(self, lhs: str, comparator: Comparator, rhs: str, label: str=None, weight=1.0)\n</code></pre> <p>Add a constraint for loss computation.</p>"},{"location":"api/pde_model/#add_hjb_equation","title":"add_hjb_equation","text":"<pre><code>def add_hjb_equation(self, eq: str, label: str=None, weight=1.0)\n</code></pre> <p>Add an HJB Equation for loss computation.</p>"},{"location":"api/pde_model/#add_system","title":"add_system","text":"<pre><code>def add_system(self, system: System, weight=1.0)\n</code></pre> <p>Add a System for loss computation.</p>"},{"location":"api/pde_model/#train_model","title":"train_model","text":"<pre><code>def train_model(self, model_dir: str=\"./\", filename: str=None, full_log=False)\n</code></pre> <p>The entire loop of training</p> <p>Parameters:</p> <ul> <li>model_dir: str, the directory to save the model. If the directory doesn't exist, it will be created automatically.</li> <li>filename: str, the filename of the model, it will be the prefix for loss table and log file.</li> <li>full_log: bool, whether or not log all individual losses in the log file.</li> </ul>"},{"location":"api/pde_model/#eval_model","title":"eval_model","text":"<pre><code>def eval_model(self, full_log=False)\n</code></pre> <p>The entire loop of evaluation</p>"},{"location":"api/pde_model/#validate_model_setup","title":"validate_model_setup","text":"<pre><code>def validate_model_setup(self, model_dir=\"./\")\n</code></pre> <p>Check that all the equations/constraints given are valid. If not, log the errors in a file, and raise an ultimate error.</p>"},{"location":"api/pde_model/#save_model","title":"save_model","text":"<pre><code>def save_model(self, model_dir: str = \"./\", filename: str=None, verbose=False)\n</code></pre> <p>Save all the agents, endogenous variables (pytorch model and configurations), and all other configurations of the PDE model.</p> <p>Parameters:</p> <ul> <li>model_dir: str, the directory to save the model</li> <li>filename: str, the filename to save the model without suffix, default: self.name </li> </ul>"},{"location":"api/pde_model/#load_model","title":"load_model","text":"<pre><code>def load_model(self, dict_to_load: Dict[str, Any])\n</code></pre> <p>Load all the agents, endogenous variables (pytorch model and configurations) from the dictionary.</p>"},{"location":"api/pde_model/#plot_vars","title":"plot_vars","text":"<pre><code>def plot_vars(self, vars_to_plot: List[str])\n</code></pre> <p>Parameters:</p> <ul> <li>vars_to_plot: variable names to plot, can be an equation defining a new variable. If Latex, need to be enclosed by $$ symbols</li> </ul> <p>Warning: This function is only supported for 1D state variables. For simple plots of 2D Agents/Endogenous Variables or their derivatives, use LearnableVar.plot.</p>"},{"location":"api/utils/","title":"deep_macrofin.utils","text":""},{"location":"api/utils/#set_seeds","title":"set_seeds","text":"<pre><code>def set_seeds(seed)\n</code></pre> <p>Set the random seeds of <code>random</code>, <code>numpy</code>, and <code>torch</code> to the provided seed. It is used by default before training loop.</p>"},{"location":"api/utils/#plot_loss_df","title":"plot_loss_df","text":"<pre><code>def plot_loss_df(fn: str=None, loss_df: pd.DataFrame=None, losses_to_plot: list=None, loss_plot_fn: str= \"./plot.jpg\")\n</code></pre> <p>Plot the provided loss df, with all losses listed in the losses_to_plot.</p> <p>Parameters:</p> <ul> <li>fn: str, the relative path to loss df csv, default: None</li> <li>loss_df: pd.DataFrame, the loaded loss df, default: None, at least one of fn and loss_df should not be None.</li> <li>losses_to_plot: List[str], the losses to plot, if None, all losses in the df will be plotted, default: None</li> <li>loss_plot_fn: str, the path to save the loss plot, default: \"./plot.jpg\"</li> </ul>"},{"location":"api/utils/#constants","title":"Constants","text":"<pre><code>class OptimizerType(str, Enum):\n    Adam = \"Adam\"\n    AdamW = \"AdamW\"\n    LBFGS = \"LBFGS\"\n\nDEFAULT_CONFIG = {\n    \"batch_size\": 100,\n    \"num_epochs\": 1000,\n    \"lr\": 1e-3,\n    \"loss_log_interval\": 100,\n    \"optimizer_type\": OptimizerType.AdamW,\n}\n\nDEFAULT_LEARNABLE_VAR_CONFIG = {\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"hidden_units\": [30, 30, 30, 30],\n    \"layer_type\": LayerType.MLP,\n    \"activation_type\": ActivationType.Tanh,\n    \"positive\": False,\n    \"derivative_order\": 2,\n}\n</code></pre>"},{"location":"examples/approx/discont/","title":"Discontinuous and Oscillating Function","text":"<p>The full solution can be found at function_approximation.ipynb.</p>"},{"location":"examples/approx/discont/#problem-setup","title":"Problem Setup","text":"\\[ y= \\begin{cases}      5 + \\sum_{k=1}^4 \\sin(kx), x&lt;0\\\\     \\cos(10x), x\\geq 0  \\end{cases} \\]"},{"location":"examples/odes/basic_ode1/","title":"Base ODE 1","text":"<p>The full solution can be found at basic_odes.ipynb.</p>"},{"location":"examples/odes/basic_ode1/#problem-setup","title":"Problem Setup","text":"\\[\\frac{dx}{dt} = 2t, x(0)=1\\] <p>The solution is \\(x(t)=t^2+1\\)</p>"},{"location":"examples/odes/basic_ode1/#implementation","title":"Implementation","text":"<ol> <li> <p>Import necessary packages <pre><code>import os\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom deep_macrofin import PDEModel\nfrom deep_macrofin import ActivationType, Comparator, EndogVar, EndogVarConditions, EndogEquation\n</code></pre></p> </li> <li> <p>Define problem. <pre><code>ode1 = PDEModel(\"ode1\") # define PDE model to solve\node1.set_state([\"t\"], {\"t\": [-2., 2.]}) # set the state variable, which defines the dimensionality of the problem\node1.add_endog(\"x\") # we use endogenous variable to represent the function we want to approximate\node1.add_endog_equation(r\"$\\frac{\\partial x}{\\partial t} = 2 * t$\", label=\"base_ode\") # endogenous equations are used to represent the ODE\node1.add_endog_condition(\"x\", \n                              \"x(SV)\", {\"SV\": torch.zeros((1, 1))},\n                              Comparator.EQ,\n                              \"1\", {},\n                              label=\"initial_condition\") # define initial condition\n</code></pre></p> </li> <li> <p>Train and evaluate <pre><code>ode1.train_model(\"./models/ode1\", \"ode1.pt\", True)\node1.eval_model(True)\n</code></pre></p> </li> <li> <p>To load a trained model <pre><code>ode1.load_model(torch.load(\"./models/ode1/ode1.pt\"))\n</code></pre></p> </li> <li> <p>Plot the solutions <pre><code>fig, ax = plt.subplots(1, 3, figsize=(16, 5))\nt = np.linspace(-2, 2)\nax[0].plot(t, t**2+1, label=\"t^2+1\")\nax[1].plot(t, 2*t, label=\"2*t\")\nax[2].plot(t, np.ones_like(t) * 2, label=\"2\")\node1.endog_vars[\"x\"].plot(\"x\", {\"t\": [-2, 2]}, ax=ax[0])\node1.endog_vars[\"x\"].plot(\"x_t\", {\"t\": [-2, 2]}, ax=ax[1])\node1.endog_vars[\"x\"].plot(\"x_tt\", {\"t\": [-2, 2]}, ax=ax[2])\nplt.subplots_adjust()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"examples/pdes/laplace/","title":"Laplace Equation Dirichlet Problem","text":"<p>The full solution can be found at basic_pdes.ipynb.</p>"},{"location":"examples/pdes/laplace/#problem-setup","title":"Problem Setup","text":"\\[\\nabla^2 T = \\frac{\\partial^2 T}{\\partial x^2} + \\frac{\\partial^2 T}{\\partial y^2} = 0, T(x,0)=T(x,\\pi)=0, T(0,y)=1\\] <p>The solution is \\(T(x,y) = \\frac{2}{\\pi} \\arctan\\frac{\\sin y}{\\sinh x}\\)</p>"},{"location":"examples/pdes/laplace/#implementation","title":"Implementation","text":""},{"location":"examples/pymacrofin/log_utility/","title":"Log Utility Problem","text":"<p>The full solution can be found at 1d_problem.ipynb.</p>"},{"location":"examples/pymacrofin/log_utility/#problem-setup","title":"Problem Setup","text":"<p>This is Proposition 4 from Brunnermeier and Sannikov 2014<sup>1</sup></p> <ol> <li> <p>Brunnermeier, Markus K. and Sannikov, Yuliy, \"A Macroeconomic Model with a Financial Sector\", SIAM Review, 104(2): 379\u2013421, 2014\u00a0\u21a9</p> </li> </ol>"}]}